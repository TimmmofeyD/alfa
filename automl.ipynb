{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad93b9aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:30:50.497415Z",
     "iopub.status.busy": "2024-11-01T18:30:50.497034Z",
     "iopub.status.idle": "2024-11-01T18:31:05.025186Z",
     "shell.execute_reply": "2024-11-01T18:31:05.024042Z"
    },
    "papermill": {
     "duration": 14.538659,
     "end_time": "2024-11-01T18:31:05.027714",
     "exception": false,
     "start_time": "2024-11-01T18:30:50.489055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flaml\r\n",
      "  Downloading FLAML-2.3.2-py3-none-any.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: NumPy>=1.17 in /opt/conda/lib/python3.10/site-packages (from flaml) (1.26.4)\r\n",
      "Downloading FLAML-2.3.2-py3-none-any.whl (313 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: flaml\r\n",
      "Successfully installed flaml-2.3.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install flaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa8a167c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-01T18:31:05.042535Z",
     "iopub.status.busy": "2024-11-01T18:31:05.042177Z",
     "iopub.status.idle": "2024-11-01T18:31:26.006606Z",
     "shell.execute_reply": "2024-11-01T18:31:26.005676Z"
    },
    "papermill": {
     "duration": 20.974394,
     "end_time": "2024-11-01T18:31:26.009033",
     "exception": false,
     "start_time": "2024-11-01T18:31:05.034639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 18:31:09,898\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-11-01 18:31:10,575\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from flaml import AutoML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Библиотеки для визуализации\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "train_data_path = '/kaggle/input/data-set/train_data.csv'\n",
    "test_data_path = '/kaggle/input/data-set/test_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6143cc1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:31:26.023832Z",
     "iopub.status.busy": "2024-11-01T18:31:26.023251Z",
     "iopub.status.idle": "2024-11-01T18:31:33.639709Z",
     "shell.execute_reply": "2024-11-01T18:31:33.638820Z"
    },
    "papermill": {
     "duration": 7.626209,
     "end_time": "2024-11-01T18:31:33.642033",
     "exception": false,
     "start_time": "2024-11-01T18:31:26.015824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_data_path)\n",
    "df_test = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdd363d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:31:33.658032Z",
     "iopub.status.busy": "2024-11-01T18:31:33.657676Z",
     "iopub.status.idle": "2024-11-01T18:31:33.952208Z",
     "shell.execute_reply": "2024-11-01T18:31:33.951299Z"
    },
    "papermill": {
     "duration": 0.306472,
     "end_time": "2024-11-01T18:31:33.956637",
     "exception": false,
     "start_time": "2024-11-01T18:31:33.650165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>smpl</th>\n",
       "      <th>id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>feature_51</th>\n",
       "      <th>feature_52</th>\n",
       "      <th>feature_53</th>\n",
       "      <th>feature_54</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>feature_64</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>feature_75</th>\n",
       "      <th>feature_76</th>\n",
       "      <th>feature_77</th>\n",
       "      <th>feature_78</th>\n",
       "      <th>feature_79</th>\n",
       "      <th>feature_80</th>\n",
       "      <th>feature_81</th>\n",
       "      <th>feature_82</th>\n",
       "      <th>feature_83</th>\n",
       "      <th>feature_84</th>\n",
       "      <th>feature_85</th>\n",
       "      <th>feature_86</th>\n",
       "      <th>feature_87</th>\n",
       "      <th>feature_88</th>\n",
       "      <th>feature_89</th>\n",
       "      <th>feature_90</th>\n",
       "      <th>feature_91</th>\n",
       "      <th>feature_92</th>\n",
       "      <th>feature_93</th>\n",
       "      <th>feature_94</th>\n",
       "      <th>feature_95</th>\n",
       "      <th>feature_96</th>\n",
       "      <th>feature_97</th>\n",
       "      <th>feature_98</th>\n",
       "      <th>feature_99</th>\n",
       "      <th>feature_100</th>\n",
       "      <th>feature_101</th>\n",
       "      <th>feature_102</th>\n",
       "      <th>feature_103</th>\n",
       "      <th>feature_104</th>\n",
       "      <th>feature_105</th>\n",
       "      <th>feature_106</th>\n",
       "      <th>feature_107</th>\n",
       "      <th>feature_108</th>\n",
       "      <th>feature_109</th>\n",
       "      <th>feature_110</th>\n",
       "      <th>feature_111</th>\n",
       "      <th>feature_112</th>\n",
       "      <th>feature_113</th>\n",
       "      <th>feature_114</th>\n",
       "      <th>feature_115</th>\n",
       "      <th>feature_116</th>\n",
       "      <th>feature_117</th>\n",
       "      <th>feature_118</th>\n",
       "      <th>feature_119</th>\n",
       "      <th>feature_120</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>feature_130</th>\n",
       "      <th>feature_131</th>\n",
       "      <th>feature_132</th>\n",
       "      <th>feature_133</th>\n",
       "      <th>feature_134</th>\n",
       "      <th>feature_135</th>\n",
       "      <th>feature_136</th>\n",
       "      <th>feature_137</th>\n",
       "      <th>feature_138</th>\n",
       "      <th>feature_139</th>\n",
       "      <th>feature_140</th>\n",
       "      <th>feature_141</th>\n",
       "      <th>feature_142</th>\n",
       "      <th>feature_143</th>\n",
       "      <th>feature_144</th>\n",
       "      <th>feature_145</th>\n",
       "      <th>feature_146</th>\n",
       "      <th>feature_147</th>\n",
       "      <th>feature_148</th>\n",
       "      <th>feature_149</th>\n",
       "      <th>feature_150</th>\n",
       "      <th>feature_151</th>\n",
       "      <th>feature_152</th>\n",
       "      <th>feature_153</th>\n",
       "      <th>feature_154</th>\n",
       "      <th>feature_155</th>\n",
       "      <th>feature_156</th>\n",
       "      <th>feature_157</th>\n",
       "      <th>feature_158</th>\n",
       "      <th>feature_159</th>\n",
       "      <th>feature_160</th>\n",
       "      <th>feature_161</th>\n",
       "      <th>feature_162</th>\n",
       "      <th>feature_163</th>\n",
       "      <th>feature_164</th>\n",
       "      <th>feature_165</th>\n",
       "      <th>feature_166</th>\n",
       "      <th>feature_167</th>\n",
       "      <th>feature_168</th>\n",
       "      <th>feature_169</th>\n",
       "      <th>feature_170</th>\n",
       "      <th>feature_171</th>\n",
       "      <th>feature_172</th>\n",
       "      <th>feature_173</th>\n",
       "      <th>feature_174</th>\n",
       "      <th>feature_175</th>\n",
       "      <th>feature_176</th>\n",
       "      <th>feature_177</th>\n",
       "      <th>feature_178</th>\n",
       "      <th>feature_179</th>\n",
       "      <th>feature_180</th>\n",
       "      <th>feature_181</th>\n",
       "      <th>feature_182</th>\n",
       "      <th>feature_183</th>\n",
       "      <th>feature_184</th>\n",
       "      <th>feature_185</th>\n",
       "      <th>feature_186</th>\n",
       "      <th>feature_187</th>\n",
       "      <th>feature_188</th>\n",
       "      <th>feature_189</th>\n",
       "      <th>feature_190</th>\n",
       "      <th>feature_191</th>\n",
       "      <th>feature_192</th>\n",
       "      <th>feature_193</th>\n",
       "      <th>feature_194</th>\n",
       "      <th>feature_195</th>\n",
       "      <th>feature_196</th>\n",
       "      <th>feature_197</th>\n",
       "      <th>feature_198</th>\n",
       "      <th>feature_199</th>\n",
       "      <th>feature_200</th>\n",
       "      <th>feature_201</th>\n",
       "      <th>feature_202</th>\n",
       "      <th>feature_203</th>\n",
       "      <th>feature_204</th>\n",
       "      <th>feature_205</th>\n",
       "      <th>feature_206</th>\n",
       "      <th>feature_207</th>\n",
       "      <th>feature_208</th>\n",
       "      <th>feature_209</th>\n",
       "      <th>feature_210</th>\n",
       "      <th>feature_211</th>\n",
       "      <th>feature_212</th>\n",
       "      <th>feature_213</th>\n",
       "      <th>feature_214</th>\n",
       "      <th>feature_215</th>\n",
       "      <th>feature_216</th>\n",
       "      <th>feature_217</th>\n",
       "      <th>feature_218</th>\n",
       "      <th>feature_219</th>\n",
       "      <th>feature_220</th>\n",
       "      <th>feature_221</th>\n",
       "      <th>feature_222</th>\n",
       "      <th>feature_223</th>\n",
       "      <th>feature_224</th>\n",
       "      <th>feature_225</th>\n",
       "      <th>feature_226</th>\n",
       "      <th>feature_227</th>\n",
       "      <th>feature_228</th>\n",
       "      <th>feature_229</th>\n",
       "      <th>feature_230</th>\n",
       "      <th>feature_231</th>\n",
       "      <th>feature_232</th>\n",
       "      <th>feature_233</th>\n",
       "      <th>feature_234</th>\n",
       "      <th>feature_235</th>\n",
       "      <th>feature_236</th>\n",
       "      <th>feature_237</th>\n",
       "      <th>feature_238</th>\n",
       "      <th>feature_239</th>\n",
       "      <th>feature_240</th>\n",
       "      <th>feature_241</th>\n",
       "      <th>feature_242</th>\n",
       "      <th>feature_243</th>\n",
       "      <th>feature_244</th>\n",
       "      <th>feature_245</th>\n",
       "      <th>feature_246</th>\n",
       "      <th>feature_247</th>\n",
       "      <th>feature_248</th>\n",
       "      <th>feature_249</th>\n",
       "      <th>feature_250</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "      <th>feature_257</th>\n",
       "      <th>feature_258</th>\n",
       "      <th>feature_259</th>\n",
       "      <th>feature_260</th>\n",
       "      <th>feature_261</th>\n",
       "      <th>feature_262</th>\n",
       "      <th>feature_263</th>\n",
       "      <th>feature_264</th>\n",
       "      <th>feature_265</th>\n",
       "      <th>feature_266</th>\n",
       "      <th>feature_267</th>\n",
       "      <th>feature_268</th>\n",
       "      <th>feature_269</th>\n",
       "      <th>feature_270</th>\n",
       "      <th>feature_271</th>\n",
       "      <th>feature_272</th>\n",
       "      <th>feature_273</th>\n",
       "      <th>feature_274</th>\n",
       "      <th>feature_275</th>\n",
       "      <th>feature_276</th>\n",
       "      <th>feature_277</th>\n",
       "      <th>feature_278</th>\n",
       "      <th>feature_279</th>\n",
       "      <th>feature_280</th>\n",
       "      <th>feature_281</th>\n",
       "      <th>feature_282</th>\n",
       "      <th>feature_283</th>\n",
       "      <th>feature_284</th>\n",
       "      <th>feature_285</th>\n",
       "      <th>feature_286</th>\n",
       "      <th>feature_287</th>\n",
       "      <th>feature_288</th>\n",
       "      <th>feature_289</th>\n",
       "      <th>feature_290</th>\n",
       "      <th>feature_291</th>\n",
       "      <th>feature_292</th>\n",
       "      <th>feature_293</th>\n",
       "      <th>feature_294</th>\n",
       "      <th>feature_295</th>\n",
       "      <th>feature_296</th>\n",
       "      <th>feature_297</th>\n",
       "      <th>feature_298</th>\n",
       "      <th>feature_299</th>\n",
       "      <th>feature_300</th>\n",
       "      <th>feature_301</th>\n",
       "      <th>feature_302</th>\n",
       "      <th>feature_303</th>\n",
       "      <th>feature_304</th>\n",
       "      <th>feature_305</th>\n",
       "      <th>feature_306</th>\n",
       "      <th>feature_307</th>\n",
       "      <th>feature_308</th>\n",
       "      <th>feature_309</th>\n",
       "      <th>feature_310</th>\n",
       "      <th>feature_311</th>\n",
       "      <th>feature_312</th>\n",
       "      <th>feature_313</th>\n",
       "      <th>feature_314</th>\n",
       "      <th>feature_315</th>\n",
       "      <th>feature_316</th>\n",
       "      <th>feature_317</th>\n",
       "      <th>feature_318</th>\n",
       "      <th>feature_319</th>\n",
       "      <th>feature_320</th>\n",
       "      <th>feature_321</th>\n",
       "      <th>feature_322</th>\n",
       "      <th>feature_323</th>\n",
       "      <th>feature_324</th>\n",
       "      <th>feature_325</th>\n",
       "      <th>feature_326</th>\n",
       "      <th>feature_327</th>\n",
       "      <th>feature_328</th>\n",
       "      <th>feature_329</th>\n",
       "      <th>feature_330</th>\n",
       "      <th>feature_331</th>\n",
       "      <th>feature_332</th>\n",
       "      <th>feature_333</th>\n",
       "      <th>feature_334</th>\n",
       "      <th>feature_335</th>\n",
       "      <th>feature_336</th>\n",
       "      <th>feature_337</th>\n",
       "      <th>feature_338</th>\n",
       "      <th>feature_339</th>\n",
       "      <th>feature_340</th>\n",
       "      <th>feature_341</th>\n",
       "      <th>feature_342</th>\n",
       "      <th>feature_343</th>\n",
       "      <th>feature_344</th>\n",
       "      <th>feature_345</th>\n",
       "      <th>feature_346</th>\n",
       "      <th>feature_347</th>\n",
       "      <th>feature_348</th>\n",
       "      <th>feature_349</th>\n",
       "      <th>feature_350</th>\n",
       "      <th>feature_351</th>\n",
       "      <th>feature_352</th>\n",
       "      <th>feature_353</th>\n",
       "      <th>feature_354</th>\n",
       "      <th>feature_355</th>\n",
       "      <th>feature_356</th>\n",
       "      <th>feature_357</th>\n",
       "      <th>feature_358</th>\n",
       "      <th>feature_359</th>\n",
       "      <th>feature_360</th>\n",
       "      <th>feature_361</th>\n",
       "      <th>feature_362</th>\n",
       "      <th>feature_363</th>\n",
       "      <th>feature_364</th>\n",
       "      <th>feature_365</th>\n",
       "      <th>feature_366</th>\n",
       "      <th>feature_367</th>\n",
       "      <th>feature_368</th>\n",
       "      <th>feature_369</th>\n",
       "      <th>feature_370</th>\n",
       "      <th>feature_371</th>\n",
       "      <th>feature_372</th>\n",
       "      <th>feature_373</th>\n",
       "      <th>feature_374</th>\n",
       "      <th>feature_375</th>\n",
       "      <th>feature_376</th>\n",
       "      <th>feature_377</th>\n",
       "      <th>feature_378</th>\n",
       "      <th>feature_379</th>\n",
       "      <th>feature_380</th>\n",
       "      <th>feature_381</th>\n",
       "      <th>feature_382</th>\n",
       "      <th>feature_383</th>\n",
       "      <th>feature_384</th>\n",
       "      <th>feature_385</th>\n",
       "      <th>feature_386</th>\n",
       "      <th>feature_387</th>\n",
       "      <th>feature_388</th>\n",
       "      <th>feature_389</th>\n",
       "      <th>feature_390</th>\n",
       "      <th>feature_391</th>\n",
       "      <th>feature_392</th>\n",
       "      <th>feature_393</th>\n",
       "      <th>feature_394</th>\n",
       "      <th>feature_395</th>\n",
       "      <th>feature_396</th>\n",
       "      <th>feature_397</th>\n",
       "      <th>feature_398</th>\n",
       "      <th>feature_399</th>\n",
       "      <th>feature_400</th>\n",
       "      <th>feature_401</th>\n",
       "      <th>feature_402</th>\n",
       "      <th>feature_403</th>\n",
       "      <th>feature_404</th>\n",
       "      <th>feature_405</th>\n",
       "      <th>feature_406</th>\n",
       "      <th>feature_407</th>\n",
       "      <th>feature_408</th>\n",
       "      <th>feature_409</th>\n",
       "      <th>feature_410</th>\n",
       "      <th>feature_411</th>\n",
       "      <th>feature_412</th>\n",
       "      <th>feature_413</th>\n",
       "      <th>feature_414</th>\n",
       "      <th>feature_415</th>\n",
       "      <th>feature_416</th>\n",
       "      <th>feature_417</th>\n",
       "      <th>feature_418</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.468142</td>\n",
       "      <td>-1.045346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384487</td>\n",
       "      <td>0.435121</td>\n",
       "      <td>-1.178548</td>\n",
       "      <td>0.124543</td>\n",
       "      <td>1.801544</td>\n",
       "      <td>0.332028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.342975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.642325</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.073636</td>\n",
       "      <td>1.832064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040974</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.319570</td>\n",
       "      <td>0.678404</td>\n",
       "      <td>2.943154</td>\n",
       "      <td>0.680208</td>\n",
       "      <td>0.514305</td>\n",
       "      <td>-0.299057</td>\n",
       "      <td>-0.298444</td>\n",
       "      <td>0.486146</td>\n",
       "      <td>-0.187802</td>\n",
       "      <td>0.780795</td>\n",
       "      <td>-0.802504</td>\n",
       "      <td>0.675421</td>\n",
       "      <td>-0.599030</td>\n",
       "      <td>2.299501</td>\n",
       "      <td>-0.612656</td>\n",
       "      <td>1.769400</td>\n",
       "      <td>-0.113554</td>\n",
       "      <td>0.349934</td>\n",
       "      <td>-0.347432</td>\n",
       "      <td>0.010639</td>\n",
       "      <td>-0.196911</td>\n",
       "      <td>1.300514</td>\n",
       "      <td>-0.080902</td>\n",
       "      <td>0.796330</td>\n",
       "      <td>-0.571195</td>\n",
       "      <td>0.750615</td>\n",
       "      <td>-1.994526</td>\n",
       "      <td>-1.202341</td>\n",
       "      <td>-1.463523</td>\n",
       "      <td>-0.460235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.041449</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>0.093989</td>\n",
       "      <td>0.788592</td>\n",
       "      <td>-0.042238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126452</td>\n",
       "      <td>0.527408</td>\n",
       "      <td>0.853002</td>\n",
       "      <td>-0.581672</td>\n",
       "      <td>0.523510</td>\n",
       "      <td>1.462681</td>\n",
       "      <td>0.499926</td>\n",
       "      <td>-0.998070</td>\n",
       "      <td>-0.902830</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660541</td>\n",
       "      <td>0.075661</td>\n",
       "      <td>0.263430</td>\n",
       "      <td>-0.220035</td>\n",
       "      <td>1.186972</td>\n",
       "      <td>0.860134</td>\n",
       "      <td>-0.152366</td>\n",
       "      <td>-0.188687</td>\n",
       "      <td>1.746777</td>\n",
       "      <td>-0.299935</td>\n",
       "      <td>-1.277905</td>\n",
       "      <td>-0.653038</td>\n",
       "      <td>-0.646313</td>\n",
       "      <td>0.218737</td>\n",
       "      <td>-0.824315</td>\n",
       "      <td>0.686127</td>\n",
       "      <td>-0.805713</td>\n",
       "      <td>0.594855</td>\n",
       "      <td>-0.299846</td>\n",
       "      <td>0.632772</td>\n",
       "      <td>-0.538958</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.777204</td>\n",
       "      <td>0.322346</td>\n",
       "      <td>0.449603</td>\n",
       "      <td>0.224158</td>\n",
       "      <td>-0.011592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313750</td>\n",
       "      <td>2.327911</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.074699</td>\n",
       "      <td>-1.222799</td>\n",
       "      <td>1.126873</td>\n",
       "      <td>-0.897274</td>\n",
       "      <td>-1.169933</td>\n",
       "      <td>1.764549</td>\n",
       "      <td>0.051998</td>\n",
       "      <td>0.059242</td>\n",
       "      <td>-1.180479</td>\n",
       "      <td>-1.443983</td>\n",
       "      <td>-1.277013</td>\n",
       "      <td>-0.134019</td>\n",
       "      <td>1.490995</td>\n",
       "      <td>-1.007765</td>\n",
       "      <td>-0.083143</td>\n",
       "      <td>-1.205676</td>\n",
       "      <td>-0.658353</td>\n",
       "      <td>-1.319770</td>\n",
       "      <td>-0.586360</td>\n",
       "      <td>0.262648</td>\n",
       "      <td>-0.191245</td>\n",
       "      <td>-0.393026</td>\n",
       "      <td>1.804129</td>\n",
       "      <td>0.049894</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.203395</td>\n",
       "      <td>-1.453285</td>\n",
       "      <td>-0.383524</td>\n",
       "      <td>1.109589</td>\n",
       "      <td>-0.240194</td>\n",
       "      <td>0.413364</td>\n",
       "      <td>-0.759104</td>\n",
       "      <td>0.206667</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.306821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.844305</td>\n",
       "      <td>-0.127400</td>\n",
       "      <td>-0.294801</td>\n",
       "      <td>0.240251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024730</td>\n",
       "      <td>-0.656020</td>\n",
       "      <td>0.337954</td>\n",
       "      <td>-0.124647</td>\n",
       "      <td>-0.246440</td>\n",
       "      <td>0.258104</td>\n",
       "      <td>0.744419</td>\n",
       "      <td>0.407665</td>\n",
       "      <td>-0.930420</td>\n",
       "      <td>-1.228201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.017206</td>\n",
       "      <td>0.585183</td>\n",
       "      <td>1.894088</td>\n",
       "      <td>0.178525</td>\n",
       "      <td>-1.397601</td>\n",
       "      <td>-0.618168</td>\n",
       "      <td>0.507300</td>\n",
       "      <td>0.069234</td>\n",
       "      <td>0.847564</td>\n",
       "      <td>0.293913</td>\n",
       "      <td>0.456653</td>\n",
       "      <td>1.405501</td>\n",
       "      <td>0.164698</td>\n",
       "      <td>1.105637</td>\n",
       "      <td>0.252699</td>\n",
       "      <td>-0.632100</td>\n",
       "      <td>-1.364036</td>\n",
       "      <td>-0.189104</td>\n",
       "      <td>-0.473863</td>\n",
       "      <td>0.166256</td>\n",
       "      <td>0.009374</td>\n",
       "      <td>-0.257498</td>\n",
       "      <td>-0.203411</td>\n",
       "      <td>-3.130904</td>\n",
       "      <td>1.050359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.029937</td>\n",
       "      <td>-0.660350</td>\n",
       "      <td>0.630274</td>\n",
       "      <td>-0.720168</td>\n",
       "      <td>1.587987</td>\n",
       "      <td>-0.903709</td>\n",
       "      <td>-0.846349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.415833</td>\n",
       "      <td>-0.592974</td>\n",
       "      <td>-0.792199</td>\n",
       "      <td>0.629578</td>\n",
       "      <td>-0.042491</td>\n",
       "      <td>-1.557814</td>\n",
       "      <td>0.293117</td>\n",
       "      <td>0.182232</td>\n",
       "      <td>0.631767</td>\n",
       "      <td>-0.275105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.814922</td>\n",
       "      <td>-1.429966</td>\n",
       "      <td>-1.967324</td>\n",
       "      <td>-0.011894</td>\n",
       "      <td>1.382303</td>\n",
       "      <td>-1.684421</td>\n",
       "      <td>0.600606</td>\n",
       "      <td>-1.678716</td>\n",
       "      <td>0.278021</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>0.887662</td>\n",
       "      <td>2.146324</td>\n",
       "      <td>-1.206914</td>\n",
       "      <td>-0.673522</td>\n",
       "      <td>-0.666780</td>\n",
       "      <td>0.137190</td>\n",
       "      <td>0.866534</td>\n",
       "      <td>-0.915680</td>\n",
       "      <td>0.896887</td>\n",
       "      <td>0.864203</td>\n",
       "      <td>-0.708157</td>\n",
       "      <td>-0.150261</td>\n",
       "      <td>-0.683593</td>\n",
       "      <td>0.090318</td>\n",
       "      <td>0.226919</td>\n",
       "      <td>0.214739</td>\n",
       "      <td>0.537111</td>\n",
       "      <td>0.599982</td>\n",
       "      <td>1.377434</td>\n",
       "      <td>0.283255</td>\n",
       "      <td>2.205156</td>\n",
       "      <td>-1.319568</td>\n",
       "      <td>-0.494630</td>\n",
       "      <td>0.564165</td>\n",
       "      <td>0.211109</td>\n",
       "      <td>0.275188</td>\n",
       "      <td>-0.561924</td>\n",
       "      <td>0.476641</td>\n",
       "      <td>0.916984</td>\n",
       "      <td>0.129031</td>\n",
       "      <td>-1.278033</td>\n",
       "      <td>-1.025281</td>\n",
       "      <td>0.289461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.568284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444746</td>\n",
       "      <td>-0.709188</td>\n",
       "      <td>0.637992</td>\n",
       "      <td>0.267719</td>\n",
       "      <td>0.011707</td>\n",
       "      <td>-0.259137</td>\n",
       "      <td>-0.823495</td>\n",
       "      <td>-0.379951</td>\n",
       "      <td>-1.143183</td>\n",
       "      <td>-0.412407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.580750</td>\n",
       "      <td>-0.044633</td>\n",
       "      <td>0.150660</td>\n",
       "      <td>0.476105</td>\n",
       "      <td>-0.676810</td>\n",
       "      <td>0.394757</td>\n",
       "      <td>0.604259</td>\n",
       "      <td>0.807586</td>\n",
       "      <td>-1.459422</td>\n",
       "      <td>-0.234030</td>\n",
       "      <td>0.821112</td>\n",
       "      <td>0.515814</td>\n",
       "      <td>0.947696</td>\n",
       "      <td>0.565654</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.511110</td>\n",
       "      <td>-0.502051</td>\n",
       "      <td>0.417777</td>\n",
       "      <td>-0.049380</td>\n",
       "      <td>0.628792</td>\n",
       "      <td>0.131042</td>\n",
       "      <td>0.889402</td>\n",
       "      <td>-0.808134</td>\n",
       "      <td>-1.017807</td>\n",
       "      <td>-1.734843</td>\n",
       "      <td>0.779995</td>\n",
       "      <td>-0.191947</td>\n",
       "      <td>-2.240253</td>\n",
       "      <td>-1.648280</td>\n",
       "      <td>0.832526</td>\n",
       "      <td>-0.863522</td>\n",
       "      <td>1.073144</td>\n",
       "      <td>0.097575</td>\n",
       "      <td>-0.315062</td>\n",
       "      <td>0.102598</td>\n",
       "      <td>0.940893</td>\n",
       "      <td>0.205870</td>\n",
       "      <td>1.392481</td>\n",
       "      <td>-1.540102</td>\n",
       "      <td>-1.399463</td>\n",
       "      <td>0.590862</td>\n",
       "      <td>0.684389</td>\n",
       "      <td>2.133173</td>\n",
       "      <td>0.733445</td>\n",
       "      <td>-0.293679</td>\n",
       "      <td>-0.308988</td>\n",
       "      <td>0.455850</td>\n",
       "      <td>-1.116855</td>\n",
       "      <td>-0.460973</td>\n",
       "      <td>-0.656475</td>\n",
       "      <td>0.297682</td>\n",
       "      <td>0.608713</td>\n",
       "      <td>1.041652</td>\n",
       "      <td>1.194602</td>\n",
       "      <td>-1.496211</td>\n",
       "      <td>0.324029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.395821</td>\n",
       "      <td>0.023010</td>\n",
       "      <td>-0.746128</td>\n",
       "      <td>-0.946506</td>\n",
       "      <td>0.089546</td>\n",
       "      <td>1.170421</td>\n",
       "      <td>-1.001922</td>\n",
       "      <td>0.783603</td>\n",
       "      <td>-1.108285</td>\n",
       "      <td>1.595645</td>\n",
       "      <td>0.058773</td>\n",
       "      <td>0.135664</td>\n",
       "      <td>0.519431</td>\n",
       "      <td>-0.777828</td>\n",
       "      <td>-1.625396</td>\n",
       "      <td>0.649841</td>\n",
       "      <td>0.603827</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.475597</td>\n",
       "      <td>-0.893660</td>\n",
       "      <td>-0.391745</td>\n",
       "      <td>1.519931</td>\n",
       "      <td>-2.042217</td>\n",
       "      <td>0.523029</td>\n",
       "      <td>0.535054</td>\n",
       "      <td>-1.253453</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.725165</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.526164</td>\n",
       "      <td>0.107756</td>\n",
       "      <td>0.314828</td>\n",
       "      <td>0.210243</td>\n",
       "      <td>1.353127</td>\n",
       "      <td>-0.417028</td>\n",
       "      <td>-0.670278</td>\n",
       "      <td>-0.205779</td>\n",
       "      <td>0.778304</td>\n",
       "      <td>-0.273153</td>\n",
       "      <td>1.047151</td>\n",
       "      <td>1.080877</td>\n",
       "      <td>-0.974841</td>\n",
       "      <td>-0.216138</td>\n",
       "      <td>-0.952752</td>\n",
       "      <td>-0.495662</td>\n",
       "      <td>-1.058111</td>\n",
       "      <td>0.894230</td>\n",
       "      <td>-0.591453</td>\n",
       "      <td>-1.216669</td>\n",
       "      <td>0.233834</td>\n",
       "      <td>-0.287891</td>\n",
       "      <td>-0.348543</td>\n",
       "      <td>0.775076</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>-0.128613</td>\n",
       "      <td>-0.021720</td>\n",
       "      <td>0.269496</td>\n",
       "      <td>0.628305</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.937712</td>\n",
       "      <td>1.164680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.322748</td>\n",
       "      <td>0.764989</td>\n",
       "      <td>0.170051</td>\n",
       "      <td>0.356889</td>\n",
       "      <td>0.781627</td>\n",
       "      <td>-0.185540</td>\n",
       "      <td>-1.686242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.572875</td>\n",
       "      <td>-0.766449</td>\n",
       "      <td>0.621146</td>\n",
       "      <td>1.145572</td>\n",
       "      <td>-0.474260</td>\n",
       "      <td>0.498067</td>\n",
       "      <td>1.668021</td>\n",
       "      <td>-0.760603</td>\n",
       "      <td>0.798047</td>\n",
       "      <td>-0.953440</td>\n",
       "      <td>0.180746</td>\n",
       "      <td>0.121726</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.822355</td>\n",
       "      <td>0.666325</td>\n",
       "      <td>-0.758953</td>\n",
       "      <td>0.503469</td>\n",
       "      <td>0.291400</td>\n",
       "      <td>-1.645741</td>\n",
       "      <td>0.146488</td>\n",
       "      <td>-0.829671</td>\n",
       "      <td>-0.246638</td>\n",
       "      <td>1.078580</td>\n",
       "      <td>-0.108352</td>\n",
       "      <td>-0.032756</td>\n",
       "      <td>-0.361507</td>\n",
       "      <td>-1.026853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.418600</td>\n",
       "      <td>-0.929668</td>\n",
       "      <td>1.284014</td>\n",
       "      <td>0.731842</td>\n",
       "      <td>0.801786</td>\n",
       "      <td>-0.728297</td>\n",
       "      <td>-0.412095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.760983</td>\n",
       "      <td>0.515132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.673905</td>\n",
       "      <td>-0.393862</td>\n",
       "      <td>-1.584207</td>\n",
       "      <td>-0.439778</td>\n",
       "      <td>0.796104</td>\n",
       "      <td>-1.331832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.958243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.539291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.103201</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.655647</td>\n",
       "      <td>0.247942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.590149</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.973363</td>\n",
       "      <td>-0.875104</td>\n",
       "      <td>2.176944</td>\n",
       "      <td>-0.727265</td>\n",
       "      <td>-0.925263</td>\n",
       "      <td>-0.345907</td>\n",
       "      <td>-1.403761</td>\n",
       "      <td>-0.534659</td>\n",
       "      <td>-0.773095</td>\n",
       "      <td>-0.867220</td>\n",
       "      <td>0.085268</td>\n",
       "      <td>1.326112</td>\n",
       "      <td>0.903044</td>\n",
       "      <td>-0.037805</td>\n",
       "      <td>-0.570683</td>\n",
       "      <td>1.098100</td>\n",
       "      <td>-0.650058</td>\n",
       "      <td>0.838633</td>\n",
       "      <td>0.818650</td>\n",
       "      <td>0.420707</td>\n",
       "      <td>-2.095442</td>\n",
       "      <td>-0.590573</td>\n",
       "      <td>-0.280188</td>\n",
       "      <td>1.177920</td>\n",
       "      <td>-1.739458</td>\n",
       "      <td>0.625996</td>\n",
       "      <td>-0.327274</td>\n",
       "      <td>0.138043</td>\n",
       "      <td>-1.642201</td>\n",
       "      <td>0.646289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.180714</td>\n",
       "      <td>-0.052744</td>\n",
       "      <td>-1.033324</td>\n",
       "      <td>0.837395</td>\n",
       "      <td>0.452283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.252064</td>\n",
       "      <td>-0.488690</td>\n",
       "      <td>0.768790</td>\n",
       "      <td>0.176442</td>\n",
       "      <td>-0.621753</td>\n",
       "      <td>0.406738</td>\n",
       "      <td>-0.033521</td>\n",
       "      <td>-1.128229</td>\n",
       "      <td>-0.556230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.264228</td>\n",
       "      <td>-0.134580</td>\n",
       "      <td>-1.321945</td>\n",
       "      <td>-0.233987</td>\n",
       "      <td>-0.367274</td>\n",
       "      <td>0.192242</td>\n",
       "      <td>0.099444</td>\n",
       "      <td>-2.434816</td>\n",
       "      <td>0.221371</td>\n",
       "      <td>-1.670063</td>\n",
       "      <td>-1.173539</td>\n",
       "      <td>-0.529720</td>\n",
       "      <td>-0.530069</td>\n",
       "      <td>0.295520</td>\n",
       "      <td>0.153126</td>\n",
       "      <td>-1.731802</td>\n",
       "      <td>-0.201868</td>\n",
       "      <td>-2.710627</td>\n",
       "      <td>0.933431</td>\n",
       "      <td>0.173819</td>\n",
       "      <td>-0.409696</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.124777</td>\n",
       "      <td>0.352603</td>\n",
       "      <td>-0.149513</td>\n",
       "      <td>0.541060</td>\n",
       "      <td>-1.845491</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.019815</td>\n",
       "      <td>-0.101222</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.073929</td>\n",
       "      <td>-1.288809</td>\n",
       "      <td>-0.169407</td>\n",
       "      <td>1.221088</td>\n",
       "      <td>-1.022738</td>\n",
       "      <td>-0.089219</td>\n",
       "      <td>1.170025</td>\n",
       "      <td>0.381867</td>\n",
       "      <td>-1.294279</td>\n",
       "      <td>-2.803755</td>\n",
       "      <td>0.704241</td>\n",
       "      <td>-0.484860</td>\n",
       "      <td>-0.080280</td>\n",
       "      <td>0.385668</td>\n",
       "      <td>-0.230779</td>\n",
       "      <td>-0.222709</td>\n",
       "      <td>-0.610192</td>\n",
       "      <td>1.078268</td>\n",
       "      <td>1.649578</td>\n",
       "      <td>0.885831</td>\n",
       "      <td>-1.243703</td>\n",
       "      <td>0.819158</td>\n",
       "      <td>0.364572</td>\n",
       "      <td>-0.066308</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.602314</td>\n",
       "      <td>-0.286886</td>\n",
       "      <td>0.179758</td>\n",
       "      <td>-1.088347</td>\n",
       "      <td>-0.480089</td>\n",
       "      <td>0.902837</td>\n",
       "      <td>-1.130827</td>\n",
       "      <td>-1.169932</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-1.487134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.116846</td>\n",
       "      <td>-0.058836</td>\n",
       "      <td>-0.672462</td>\n",
       "      <td>-0.740467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.603325</td>\n",
       "      <td>0.656015</td>\n",
       "      <td>0.333940</td>\n",
       "      <td>0.016165</td>\n",
       "      <td>-1.268323</td>\n",
       "      <td>-0.962288</td>\n",
       "      <td>0.913720</td>\n",
       "      <td>-0.616531</td>\n",
       "      <td>0.601266</td>\n",
       "      <td>-1.894769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996772</td>\n",
       "      <td>0.007350</td>\n",
       "      <td>-1.572205</td>\n",
       "      <td>0.088652</td>\n",
       "      <td>-1.798130</td>\n",
       "      <td>0.656918</td>\n",
       "      <td>0.529805</td>\n",
       "      <td>0.713996</td>\n",
       "      <td>-1.083242</td>\n",
       "      <td>-0.051258</td>\n",
       "      <td>-0.043511</td>\n",
       "      <td>0.567080</td>\n",
       "      <td>-2.165406</td>\n",
       "      <td>-0.468783</td>\n",
       "      <td>0.753014</td>\n",
       "      <td>1.191944</td>\n",
       "      <td>-0.774412</td>\n",
       "      <td>-1.826900</td>\n",
       "      <td>0.460043</td>\n",
       "      <td>0.451270</td>\n",
       "      <td>-1.049715</td>\n",
       "      <td>-1.039448</td>\n",
       "      <td>-0.214271</td>\n",
       "      <td>-1.494830</td>\n",
       "      <td>0.138920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.365203</td>\n",
       "      <td>0.231139</td>\n",
       "      <td>0.212887</td>\n",
       "      <td>-0.867707</td>\n",
       "      <td>-0.845028</td>\n",
       "      <td>-1.738828</td>\n",
       "      <td>-2.432545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066601</td>\n",
       "      <td>0.522979</td>\n",
       "      <td>0.565327</td>\n",
       "      <td>0.433472</td>\n",
       "      <td>0.689002</td>\n",
       "      <td>0.617479</td>\n",
       "      <td>0.304762</td>\n",
       "      <td>-0.771716</td>\n",
       "      <td>1.119108</td>\n",
       "      <td>0.509343</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.411803</td>\n",
       "      <td>-0.452566</td>\n",
       "      <td>1.194920</td>\n",
       "      <td>-1.891247</td>\n",
       "      <td>1.525119</td>\n",
       "      <td>0.210434</td>\n",
       "      <td>0.988621</td>\n",
       "      <td>-1.119946</td>\n",
       "      <td>0.191252</td>\n",
       "      <td>0.078334</td>\n",
       "      <td>-0.907423</td>\n",
       "      <td>-0.821667</td>\n",
       "      <td>-0.519094</td>\n",
       "      <td>-0.034778</td>\n",
       "      <td>0.052776</td>\n",
       "      <td>-1.029502</td>\n",
       "      <td>0.100321</td>\n",
       "      <td>1.675569</td>\n",
       "      <td>0.748420</td>\n",
       "      <td>-1.689240</td>\n",
       "      <td>1.461673</td>\n",
       "      <td>1.019182</td>\n",
       "      <td>0.013909</td>\n",
       "      <td>-1.441045</td>\n",
       "      <td>-0.253227</td>\n",
       "      <td>0.702967</td>\n",
       "      <td>0.632428</td>\n",
       "      <td>0.960560</td>\n",
       "      <td>-0.019770</td>\n",
       "      <td>0.915882</td>\n",
       "      <td>-0.111479</td>\n",
       "      <td>0.069311</td>\n",
       "      <td>-1.328630</td>\n",
       "      <td>-0.274612</td>\n",
       "      <td>-2.130145</td>\n",
       "      <td>-1.694103</td>\n",
       "      <td>-1.271975</td>\n",
       "      <td>0.796754</td>\n",
       "      <td>1.768707</td>\n",
       "      <td>-0.222346</td>\n",
       "      <td>-0.763386</td>\n",
       "      <td>-0.035096</td>\n",
       "      <td>0.009873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.422882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.053811</td>\n",
       "      <td>1.587036</td>\n",
       "      <td>1.181980</td>\n",
       "      <td>-0.986534</td>\n",
       "      <td>-1.156395</td>\n",
       "      <td>0.123150</td>\n",
       "      <td>-1.012012</td>\n",
       "      <td>-0.533884</td>\n",
       "      <td>0.321240</td>\n",
       "      <td>-1.716837</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.204142</td>\n",
       "      <td>-1.171752</td>\n",
       "      <td>-1.117391</td>\n",
       "      <td>1.092238</td>\n",
       "      <td>0.955735</td>\n",
       "      <td>-0.072982</td>\n",
       "      <td>-0.078929</td>\n",
       "      <td>0.060325</td>\n",
       "      <td>0.333696</td>\n",
       "      <td>0.481277</td>\n",
       "      <td>0.611947</td>\n",
       "      <td>-1.195943</td>\n",
       "      <td>-0.969317</td>\n",
       "      <td>0.629833</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.533139</td>\n",
       "      <td>0.611051</td>\n",
       "      <td>1.188689</td>\n",
       "      <td>0.658617</td>\n",
       "      <td>0.813165</td>\n",
       "      <td>-0.157380</td>\n",
       "      <td>-0.413281</td>\n",
       "      <td>0.231968</td>\n",
       "      <td>1.098548</td>\n",
       "      <td>0.290310</td>\n",
       "      <td>0.878726</td>\n",
       "      <td>1.440409</td>\n",
       "      <td>0.021213</td>\n",
       "      <td>0.025495</td>\n",
       "      <td>-0.014004</td>\n",
       "      <td>0.278497</td>\n",
       "      <td>-0.358644</td>\n",
       "      <td>1.045162</td>\n",
       "      <td>-0.471805</td>\n",
       "      <td>0.570654</td>\n",
       "      <td>1.306575</td>\n",
       "      <td>-0.986882</td>\n",
       "      <td>-0.563621</td>\n",
       "      <td>-1.653276</td>\n",
       "      <td>-0.553522</td>\n",
       "      <td>0.445039</td>\n",
       "      <td>-0.195654</td>\n",
       "      <td>2.699615</td>\n",
       "      <td>-1.436694</td>\n",
       "      <td>1.303265</td>\n",
       "      <td>1.596970</td>\n",
       "      <td>0.773270</td>\n",
       "      <td>0.163572</td>\n",
       "      <td>0.156204</td>\n",
       "      <td>-0.443805</td>\n",
       "      <td>-0.440397</td>\n",
       "      <td>-0.399911</td>\n",
       "      <td>1.042692</td>\n",
       "      <td>-0.109034</td>\n",
       "      <td>1.995238</td>\n",
       "      <td>-0.444583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135495</td>\n",
       "      <td>-1.378791</td>\n",
       "      <td>-0.061418</td>\n",
       "      <td>1.656025</td>\n",
       "      <td>0.739492</td>\n",
       "      <td>1.452635</td>\n",
       "      <td>0.279078</td>\n",
       "      <td>0.722986</td>\n",
       "      <td>-0.136428</td>\n",
       "      <td>0.523149</td>\n",
       "      <td>-0.845223</td>\n",
       "      <td>-1.158081</td>\n",
       "      <td>0.406282</td>\n",
       "      <td>-1.034597</td>\n",
       "      <td>-0.669878</td>\n",
       "      <td>-0.606321</td>\n",
       "      <td>0.879827</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.273021</td>\n",
       "      <td>-1.088210</td>\n",
       "      <td>-0.574266</td>\n",
       "      <td>-0.619914</td>\n",
       "      <td>0.741997</td>\n",
       "      <td>0.836686</td>\n",
       "      <td>0.076717</td>\n",
       "      <td>-1.598974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.261833</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.244250</td>\n",
       "      <td>1.701270</td>\n",
       "      <td>0.364853</td>\n",
       "      <td>0.040682</td>\n",
       "      <td>0.103482</td>\n",
       "      <td>1.296307</td>\n",
       "      <td>0.606847</td>\n",
       "      <td>-0.368300</td>\n",
       "      <td>-0.320346</td>\n",
       "      <td>-1.010868</td>\n",
       "      <td>1.557501</td>\n",
       "      <td>-0.158664</td>\n",
       "      <td>-0.444257</td>\n",
       "      <td>0.048692</td>\n",
       "      <td>-0.463989</td>\n",
       "      <td>0.038802</td>\n",
       "      <td>-0.284713</td>\n",
       "      <td>-1.549435</td>\n",
       "      <td>0.240735</td>\n",
       "      <td>-0.407025</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>-0.304289</td>\n",
       "      <td>1.521847</td>\n",
       "      <td>-0.453642</td>\n",
       "      <td>0.312443</td>\n",
       "      <td>-1.128488</td>\n",
       "      <td>2.227771</td>\n",
       "      <td>0.250663</td>\n",
       "      <td>1.676488</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.666902</td>\n",
       "      <td>-0.423972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.665466</td>\n",
       "      <td>1.342203</td>\n",
       "      <td>-0.385267</td>\n",
       "      <td>0.467601</td>\n",
       "      <td>0.260087</td>\n",
       "      <td>-0.722288</td>\n",
       "      <td>-0.426859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.125879</td>\n",
       "      <td>0.068354</td>\n",
       "      <td>0.537780</td>\n",
       "      <td>0.128863</td>\n",
       "      <td>-0.185917</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>-0.612005</td>\n",
       "      <td>-0.543899</td>\n",
       "      <td>0.580927</td>\n",
       "      <td>-1.110941</td>\n",
       "      <td>-0.042332</td>\n",
       "      <td>-0.187147</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.380817</td>\n",
       "      <td>1.096997</td>\n",
       "      <td>-1.333537</td>\n",
       "      <td>0.398855</td>\n",
       "      <td>0.634238</td>\n",
       "      <td>-0.268676</td>\n",
       "      <td>0.650312</td>\n",
       "      <td>-1.853633</td>\n",
       "      <td>-0.129410</td>\n",
       "      <td>0.723021</td>\n",
       "      <td>-0.784384</td>\n",
       "      <td>-0.122721</td>\n",
       "      <td>-0.546275</td>\n",
       "      <td>-1.489542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.622007</td>\n",
       "      <td>-0.473156</td>\n",
       "      <td>0.780020</td>\n",
       "      <td>0.648577</td>\n",
       "      <td>0.646100</td>\n",
       "      <td>-0.789362</td>\n",
       "      <td>0.083349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>1.658855</td>\n",
       "      <td>0.915052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.581082</td>\n",
       "      <td>0.477199</td>\n",
       "      <td>-0.622226</td>\n",
       "      <td>0.390642</td>\n",
       "      <td>0.753299</td>\n",
       "      <td>0.400727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.637646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.213259</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.333812</td>\n",
       "      <td>-0.074908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602250</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.335774</td>\n",
       "      <td>1.782889</td>\n",
       "      <td>0.756021</td>\n",
       "      <td>0.280975</td>\n",
       "      <td>-0.317603</td>\n",
       "      <td>0.370240</td>\n",
       "      <td>-0.397848</td>\n",
       "      <td>0.752353</td>\n",
       "      <td>0.093410</td>\n",
       "      <td>0.763672</td>\n",
       "      <td>-0.981743</td>\n",
       "      <td>-0.012486</td>\n",
       "      <td>-0.238419</td>\n",
       "      <td>-0.326872</td>\n",
       "      <td>-1.767364</td>\n",
       "      <td>1.308259</td>\n",
       "      <td>-1.347245</td>\n",
       "      <td>-0.772304</td>\n",
       "      <td>2.009681</td>\n",
       "      <td>-1.810025</td>\n",
       "      <td>0.498785</td>\n",
       "      <td>0.140077</td>\n",
       "      <td>1.813894</td>\n",
       "      <td>-0.149701</td>\n",
       "      <td>-0.243074</td>\n",
       "      <td>-0.946646</td>\n",
       "      <td>-1.632846</td>\n",
       "      <td>0.367061</td>\n",
       "      <td>-1.388741</td>\n",
       "      <td>-0.284864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.443954</td>\n",
       "      <td>-1.493657</td>\n",
       "      <td>1.129079</td>\n",
       "      <td>0.176012</td>\n",
       "      <td>2.365263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.723328</td>\n",
       "      <td>1.020088</td>\n",
       "      <td>0.399541</td>\n",
       "      <td>1.015374</td>\n",
       "      <td>0.432726</td>\n",
       "      <td>0.493677</td>\n",
       "      <td>0.097214</td>\n",
       "      <td>-0.303480</td>\n",
       "      <td>0.026299</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.462163</td>\n",
       "      <td>1.694762</td>\n",
       "      <td>-0.104124</td>\n",
       "      <td>0.153415</td>\n",
       "      <td>-0.187552</td>\n",
       "      <td>0.588539</td>\n",
       "      <td>-2.042665</td>\n",
       "      <td>-0.829246</td>\n",
       "      <td>0.138201</td>\n",
       "      <td>1.059464</td>\n",
       "      <td>-0.122988</td>\n",
       "      <td>-0.608095</td>\n",
       "      <td>-0.604866</td>\n",
       "      <td>-1.122324</td>\n",
       "      <td>0.887297</td>\n",
       "      <td>-1.513382</td>\n",
       "      <td>0.289026</td>\n",
       "      <td>1.395137</td>\n",
       "      <td>0.856310</td>\n",
       "      <td>-0.778699</td>\n",
       "      <td>-0.939723</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.193845</td>\n",
       "      <td>-0.249936</td>\n",
       "      <td>-1.196983</td>\n",
       "      <td>0.028618</td>\n",
       "      <td>-0.026915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.110695</td>\n",
       "      <td>-0.708828</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.429845</td>\n",
       "      <td>0.224793</td>\n",
       "      <td>-1.353030</td>\n",
       "      <td>0.188018</td>\n",
       "      <td>0.822470</td>\n",
       "      <td>1.690886</td>\n",
       "      <td>0.641219</td>\n",
       "      <td>0.719090</td>\n",
       "      <td>1.471495</td>\n",
       "      <td>-0.109378</td>\n",
       "      <td>0.943723</td>\n",
       "      <td>-0.373415</td>\n",
       "      <td>0.559970</td>\n",
       "      <td>0.243101</td>\n",
       "      <td>-0.069229</td>\n",
       "      <td>-0.156425</td>\n",
       "      <td>-0.483252</td>\n",
       "      <td>1.277412</td>\n",
       "      <td>0.636291</td>\n",
       "      <td>-0.175425</td>\n",
       "      <td>1.888265</td>\n",
       "      <td>0.757866</td>\n",
       "      <td>-0.702022</td>\n",
       "      <td>0.102439</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.468759</td>\n",
       "      <td>0.585031</td>\n",
       "      <td>1.229884</td>\n",
       "      <td>-1.283153</td>\n",
       "      <td>0.464297</td>\n",
       "      <td>0.137619</td>\n",
       "      <td>-0.308007</td>\n",
       "      <td>0.167366</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.976838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.163929</td>\n",
       "      <td>-0.857007</td>\n",
       "      <td>-0.122315</td>\n",
       "      <td>-0.046688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.886777</td>\n",
       "      <td>0.296281</td>\n",
       "      <td>-0.505381</td>\n",
       "      <td>1.615476</td>\n",
       "      <td>1.238792</td>\n",
       "      <td>-0.073948</td>\n",
       "      <td>-0.542446</td>\n",
       "      <td>0.048058</td>\n",
       "      <td>-0.224241</td>\n",
       "      <td>-0.201844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208764</td>\n",
       "      <td>-0.789506</td>\n",
       "      <td>0.371502</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>1.388004</td>\n",
       "      <td>0.312974</td>\n",
       "      <td>-0.721760</td>\n",
       "      <td>0.390571</td>\n",
       "      <td>-1.127536</td>\n",
       "      <td>0.476640</td>\n",
       "      <td>-1.363957</td>\n",
       "      <td>1.527885</td>\n",
       "      <td>0.342472</td>\n",
       "      <td>0.778398</td>\n",
       "      <td>1.115198</td>\n",
       "      <td>0.372435</td>\n",
       "      <td>0.457172</td>\n",
       "      <td>-0.189774</td>\n",
       "      <td>0.597187</td>\n",
       "      <td>0.536737</td>\n",
       "      <td>0.799887</td>\n",
       "      <td>-0.244411</td>\n",
       "      <td>-2.241794</td>\n",
       "      <td>1.505090</td>\n",
       "      <td>-0.236390</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.809148</td>\n",
       "      <td>1.423503</td>\n",
       "      <td>-0.026988</td>\n",
       "      <td>-1.855442</td>\n",
       "      <td>-1.053101</td>\n",
       "      <td>1.801683</td>\n",
       "      <td>-1.561516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.145060</td>\n",
       "      <td>-0.592703</td>\n",
       "      <td>0.584384</td>\n",
       "      <td>-1.362151</td>\n",
       "      <td>-1.271011</td>\n",
       "      <td>0.457746</td>\n",
       "      <td>-0.545932</td>\n",
       "      <td>0.043374</td>\n",
       "      <td>1.261645</td>\n",
       "      <td>-0.046620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.490928</td>\n",
       "      <td>1.063507</td>\n",
       "      <td>0.025955</td>\n",
       "      <td>-1.477186</td>\n",
       "      <td>-0.103165</td>\n",
       "      <td>-1.260223</td>\n",
       "      <td>-0.302939</td>\n",
       "      <td>0.328441</td>\n",
       "      <td>-1.051807</td>\n",
       "      <td>-0.676958</td>\n",
       "      <td>0.160137</td>\n",
       "      <td>0.725534</td>\n",
       "      <td>0.853314</td>\n",
       "      <td>-0.079099</td>\n",
       "      <td>0.339358</td>\n",
       "      <td>-0.844095</td>\n",
       "      <td>-0.416158</td>\n",
       "      <td>0.147965</td>\n",
       "      <td>-0.652813</td>\n",
       "      <td>0.146464</td>\n",
       "      <td>0.182509</td>\n",
       "      <td>0.353547</td>\n",
       "      <td>-0.568627</td>\n",
       "      <td>0.316609</td>\n",
       "      <td>-0.408552</td>\n",
       "      <td>-1.172877</td>\n",
       "      <td>-2.797013</td>\n",
       "      <td>0.570071</td>\n",
       "      <td>1.237126</td>\n",
       "      <td>-0.394783</td>\n",
       "      <td>0.057857</td>\n",
       "      <td>0.951690</td>\n",
       "      <td>0.910740</td>\n",
       "      <td>0.084905</td>\n",
       "      <td>-0.894216</td>\n",
       "      <td>0.091887</td>\n",
       "      <td>-0.629352</td>\n",
       "      <td>0.083650</td>\n",
       "      <td>0.560426</td>\n",
       "      <td>1.155306</td>\n",
       "      <td>0.780005</td>\n",
       "      <td>-1.269435</td>\n",
       "      <td>-0.410563</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.134407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.516262</td>\n",
       "      <td>-0.355502</td>\n",
       "      <td>1.328436</td>\n",
       "      <td>0.823952</td>\n",
       "      <td>-0.962213</td>\n",
       "      <td>0.142541</td>\n",
       "      <td>1.090114</td>\n",
       "      <td>-0.204129</td>\n",
       "      <td>1.321770</td>\n",
       "      <td>-0.217190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.961567</td>\n",
       "      <td>-1.059627</td>\n",
       "      <td>0.078978</td>\n",
       "      <td>-0.610295</td>\n",
       "      <td>1.411798</td>\n",
       "      <td>0.975517</td>\n",
       "      <td>-0.086469</td>\n",
       "      <td>-0.439670</td>\n",
       "      <td>-0.625058</td>\n",
       "      <td>0.534661</td>\n",
       "      <td>0.994917</td>\n",
       "      <td>1.911488</td>\n",
       "      <td>0.952608</td>\n",
       "      <td>-0.811737</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.088905</td>\n",
       "      <td>2.420187</td>\n",
       "      <td>0.603403</td>\n",
       "      <td>-0.409400</td>\n",
       "      <td>-1.033804</td>\n",
       "      <td>-0.497707</td>\n",
       "      <td>0.540116</td>\n",
       "      <td>0.454637</td>\n",
       "      <td>-1.555673</td>\n",
       "      <td>-0.057831</td>\n",
       "      <td>0.003188</td>\n",
       "      <td>1.418235</td>\n",
       "      <td>-0.716175</td>\n",
       "      <td>-0.044501</td>\n",
       "      <td>-0.754588</td>\n",
       "      <td>-0.083395</td>\n",
       "      <td>-1.596949</td>\n",
       "      <td>-0.308550</td>\n",
       "      <td>-1.301425</td>\n",
       "      <td>-0.352957</td>\n",
       "      <td>-0.340272</td>\n",
       "      <td>-0.252075</td>\n",
       "      <td>-0.209940</td>\n",
       "      <td>-0.677107</td>\n",
       "      <td>1.927504</td>\n",
       "      <td>0.918098</td>\n",
       "      <td>-0.536446</td>\n",
       "      <td>1.185014</td>\n",
       "      <td>-0.202943</td>\n",
       "      <td>-0.513509</td>\n",
       "      <td>1.004317</td>\n",
       "      <td>-1.958133</td>\n",
       "      <td>-2.373041</td>\n",
       "      <td>-0.310544</td>\n",
       "      <td>-0.612823</td>\n",
       "      <td>-1.657067</td>\n",
       "      <td>-1.051108</td>\n",
       "      <td>-0.895319</td>\n",
       "      <td>-1.447215</td>\n",
       "      <td>-1.127566</td>\n",
       "      <td>0.156201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.665356</td>\n",
       "      <td>1.605058</td>\n",
       "      <td>-0.256115</td>\n",
       "      <td>-0.051711</td>\n",
       "      <td>-0.754819</td>\n",
       "      <td>1.208956</td>\n",
       "      <td>-0.031901</td>\n",
       "      <td>0.325796</td>\n",
       "      <td>-0.466307</td>\n",
       "      <td>0.142175</td>\n",
       "      <td>0.723891</td>\n",
       "      <td>-0.424735</td>\n",
       "      <td>1.046035</td>\n",
       "      <td>0.858532</td>\n",
       "      <td>-0.873258</td>\n",
       "      <td>-0.665298</td>\n",
       "      <td>-0.584091</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.592800</td>\n",
       "      <td>-1.676850</td>\n",
       "      <td>-0.461425</td>\n",
       "      <td>-1.159953</td>\n",
       "      <td>-0.580410</td>\n",
       "      <td>-1.415054</td>\n",
       "      <td>-0.627873</td>\n",
       "      <td>0.361897</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.774056</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.138190</td>\n",
       "      <td>-1.648837</td>\n",
       "      <td>0.424413</td>\n",
       "      <td>-0.954391</td>\n",
       "      <td>-1.012680</td>\n",
       "      <td>0.353126</td>\n",
       "      <td>-1.399300</td>\n",
       "      <td>0.458409</td>\n",
       "      <td>-0.556601</td>\n",
       "      <td>-0.315339</td>\n",
       "      <td>-1.776895</td>\n",
       "      <td>-0.913342</td>\n",
       "      <td>1.007887</td>\n",
       "      <td>-1.124069</td>\n",
       "      <td>1.613377</td>\n",
       "      <td>0.365286</td>\n",
       "      <td>0.369305</td>\n",
       "      <td>1.112181</td>\n",
       "      <td>-2.941156</td>\n",
       "      <td>-0.313106</td>\n",
       "      <td>-1.044244</td>\n",
       "      <td>-0.005333</td>\n",
       "      <td>-0.792533</td>\n",
       "      <td>0.512796</td>\n",
       "      <td>-0.435759</td>\n",
       "      <td>-1.167133</td>\n",
       "      <td>1.488831</td>\n",
       "      <td>1.831155</td>\n",
       "      <td>-0.076525</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.198818</td>\n",
       "      <td>-0.520668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.509424</td>\n",
       "      <td>0.335270</td>\n",
       "      <td>-0.252650</td>\n",
       "      <td>-1.162685</td>\n",
       "      <td>1.506409</td>\n",
       "      <td>-1.273188</td>\n",
       "      <td>-0.205559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.363512</td>\n",
       "      <td>1.864165</td>\n",
       "      <td>-0.357862</td>\n",
       "      <td>-0.881552</td>\n",
       "      <td>0.597269</td>\n",
       "      <td>0.504736</td>\n",
       "      <td>1.306143</td>\n",
       "      <td>0.894581</td>\n",
       "      <td>-0.485608</td>\n",
       "      <td>0.151862</td>\n",
       "      <td>0.204297</td>\n",
       "      <td>0.343261</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.656024</td>\n",
       "      <td>0.338950</td>\n",
       "      <td>0.350464</td>\n",
       "      <td>-0.080056</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>0.579273</td>\n",
       "      <td>1.189545</td>\n",
       "      <td>1.188373</td>\n",
       "      <td>-1.506164</td>\n",
       "      <td>-0.465339</td>\n",
       "      <td>-0.784669</td>\n",
       "      <td>0.675667</td>\n",
       "      <td>-0.485999</td>\n",
       "      <td>0.586012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361481</td>\n",
       "      <td>-0.364566</td>\n",
       "      <td>-1.318596</td>\n",
       "      <td>-0.385155</td>\n",
       "      <td>0.140133</td>\n",
       "      <td>0.123245</td>\n",
       "      <td>-0.670030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.638854</td>\n",
       "      <td>0.314099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>1.102342</td>\n",
       "      <td>-0.807371</td>\n",
       "      <td>0.329158</td>\n",
       "      <td>0.484305</td>\n",
       "      <td>0.412519</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.715940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.859640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.304999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.541586</td>\n",
       "      <td>-0.458376</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.046954</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.212354</td>\n",
       "      <td>-0.557272</td>\n",
       "      <td>0.586641</td>\n",
       "      <td>0.946142</td>\n",
       "      <td>2.434117</td>\n",
       "      <td>0.746217</td>\n",
       "      <td>-0.134858</td>\n",
       "      <td>0.667817</td>\n",
       "      <td>-1.311813</td>\n",
       "      <td>-0.987538</td>\n",
       "      <td>0.090251</td>\n",
       "      <td>-0.170246</td>\n",
       "      <td>0.988806</td>\n",
       "      <td>-0.343382</td>\n",
       "      <td>0.327664</td>\n",
       "      <td>-0.693446</td>\n",
       "      <td>-1.197079</td>\n",
       "      <td>1.599838</td>\n",
       "      <td>1.526965</td>\n",
       "      <td>-0.183530</td>\n",
       "      <td>-0.575217</td>\n",
       "      <td>-0.721305</td>\n",
       "      <td>0.921539</td>\n",
       "      <td>0.463246</td>\n",
       "      <td>0.980302</td>\n",
       "      <td>0.260606</td>\n",
       "      <td>-0.303126</td>\n",
       "      <td>-0.241599</td>\n",
       "      <td>-1.439445</td>\n",
       "      <td>0.506683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.728534</td>\n",
       "      <td>1.333196</td>\n",
       "      <td>2.212643</td>\n",
       "      <td>0.989996</td>\n",
       "      <td>1.200458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.777314</td>\n",
       "      <td>1.356488</td>\n",
       "      <td>1.334583</td>\n",
       "      <td>0.507161</td>\n",
       "      <td>-0.734510</td>\n",
       "      <td>1.530486</td>\n",
       "      <td>1.015725</td>\n",
       "      <td>-0.248070</td>\n",
       "      <td>-0.797367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.647778</td>\n",
       "      <td>1.082519</td>\n",
       "      <td>0.240294</td>\n",
       "      <td>0.797235</td>\n",
       "      <td>-1.394951</td>\n",
       "      <td>1.847186</td>\n",
       "      <td>-0.880586</td>\n",
       "      <td>0.933163</td>\n",
       "      <td>0.397004</td>\n",
       "      <td>0.140066</td>\n",
       "      <td>0.148379</td>\n",
       "      <td>-0.269661</td>\n",
       "      <td>-0.617402</td>\n",
       "      <td>-0.222095</td>\n",
       "      <td>-0.440414</td>\n",
       "      <td>-1.017426</td>\n",
       "      <td>-0.127428</td>\n",
       "      <td>0.316201</td>\n",
       "      <td>-0.126269</td>\n",
       "      <td>-0.709465</td>\n",
       "      <td>-0.691001</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.172375</td>\n",
       "      <td>0.167786</td>\n",
       "      <td>-0.245611</td>\n",
       "      <td>0.948606</td>\n",
       "      <td>-1.359652</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.539031</td>\n",
       "      <td>-0.877148</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.429774</td>\n",
       "      <td>-1.205323</td>\n",
       "      <td>1.306246</td>\n",
       "      <td>2.245903</td>\n",
       "      <td>-0.072128</td>\n",
       "      <td>0.434162</td>\n",
       "      <td>-0.005514</td>\n",
       "      <td>0.631682</td>\n",
       "      <td>-0.683774</td>\n",
       "      <td>-0.538104</td>\n",
       "      <td>-0.644193</td>\n",
       "      <td>-0.587877</td>\n",
       "      <td>-0.273172</td>\n",
       "      <td>-0.082105</td>\n",
       "      <td>-0.102266</td>\n",
       "      <td>0.743286</td>\n",
       "      <td>-1.602893</td>\n",
       "      <td>2.705511</td>\n",
       "      <td>-0.850264</td>\n",
       "      <td>0.837179</td>\n",
       "      <td>-0.408891</td>\n",
       "      <td>1.623775</td>\n",
       "      <td>-0.056327</td>\n",
       "      <td>-0.592473</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.393227</td>\n",
       "      <td>-0.659929</td>\n",
       "      <td>0.096845</td>\n",
       "      <td>0.560023</td>\n",
       "      <td>0.483311</td>\n",
       "      <td>-0.688010</td>\n",
       "      <td>-0.311281</td>\n",
       "      <td>-0.332794</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.132559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.960162</td>\n",
       "      <td>-2.026977</td>\n",
       "      <td>-0.457321</td>\n",
       "      <td>1.496611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.883562</td>\n",
       "      <td>-0.115753</td>\n",
       "      <td>-1.076133</td>\n",
       "      <td>-0.465735</td>\n",
       "      <td>0.278139</td>\n",
       "      <td>-1.850325</td>\n",
       "      <td>0.522737</td>\n",
       "      <td>1.577154</td>\n",
       "      <td>-0.057310</td>\n",
       "      <td>1.311651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.610428</td>\n",
       "      <td>-0.253664</td>\n",
       "      <td>1.526026</td>\n",
       "      <td>-1.107451</td>\n",
       "      <td>0.213915</td>\n",
       "      <td>0.274537</td>\n",
       "      <td>-0.311793</td>\n",
       "      <td>0.108339</td>\n",
       "      <td>-1.133135</td>\n",
       "      <td>-0.246972</td>\n",
       "      <td>-0.277572</td>\n",
       "      <td>-2.619473</td>\n",
       "      <td>-0.315480</td>\n",
       "      <td>1.682810</td>\n",
       "      <td>0.944929</td>\n",
       "      <td>0.828605</td>\n",
       "      <td>0.416779</td>\n",
       "      <td>0.174765</td>\n",
       "      <td>-1.371408</td>\n",
       "      <td>0.637738</td>\n",
       "      <td>0.454484</td>\n",
       "      <td>-0.628541</td>\n",
       "      <td>-1.661455</td>\n",
       "      <td>-0.769516</td>\n",
       "      <td>-0.204544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.228278</td>\n",
       "      <td>-0.489225</td>\n",
       "      <td>-0.125213</td>\n",
       "      <td>-0.654200</td>\n",
       "      <td>-0.724949</td>\n",
       "      <td>-0.477704</td>\n",
       "      <td>0.969274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.169830</td>\n",
       "      <td>0.776918</td>\n",
       "      <td>0.649593</td>\n",
       "      <td>0.205862</td>\n",
       "      <td>0.567788</td>\n",
       "      <td>0.785037</td>\n",
       "      <td>1.627670</td>\n",
       "      <td>-1.823888</td>\n",
       "      <td>0.916430</td>\n",
       "      <td>-0.752902</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.284271</td>\n",
       "      <td>1.370576</td>\n",
       "      <td>0.180590</td>\n",
       "      <td>0.186486</td>\n",
       "      <td>-1.648581</td>\n",
       "      <td>-0.202836</td>\n",
       "      <td>-1.615217</td>\n",
       "      <td>-2.433494</td>\n",
       "      <td>-0.254696</td>\n",
       "      <td>0.534700</td>\n",
       "      <td>-2.323304</td>\n",
       "      <td>1.423436</td>\n",
       "      <td>1.511624</td>\n",
       "      <td>-0.116746</td>\n",
       "      <td>0.199191</td>\n",
       "      <td>-1.157929</td>\n",
       "      <td>-0.857270</td>\n",
       "      <td>0.311955</td>\n",
       "      <td>2.251281</td>\n",
       "      <td>1.976119</td>\n",
       "      <td>1.678888</td>\n",
       "      <td>2.060178</td>\n",
       "      <td>0.408986</td>\n",
       "      <td>-0.209740</td>\n",
       "      <td>-1.261743</td>\n",
       "      <td>-1.053186</td>\n",
       "      <td>0.111571</td>\n",
       "      <td>0.880554</td>\n",
       "      <td>0.771447</td>\n",
       "      <td>0.653848</td>\n",
       "      <td>-1.259758</td>\n",
       "      <td>1.379058</td>\n",
       "      <td>1.544118</td>\n",
       "      <td>-1.309901</td>\n",
       "      <td>-1.336027</td>\n",
       "      <td>1.512860</td>\n",
       "      <td>-1.545349</td>\n",
       "      <td>-0.503860</td>\n",
       "      <td>0.752157</td>\n",
       "      <td>1.234394</td>\n",
       "      <td>1.747207</td>\n",
       "      <td>0.154937</td>\n",
       "      <td>-0.336034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.240721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.792359</td>\n",
       "      <td>-0.725082</td>\n",
       "      <td>0.976723</td>\n",
       "      <td>0.156261</td>\n",
       "      <td>-0.539084</td>\n",
       "      <td>-0.587726</td>\n",
       "      <td>-0.367335</td>\n",
       "      <td>-0.766549</td>\n",
       "      <td>0.421441</td>\n",
       "      <td>-0.407718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.449698</td>\n",
       "      <td>-0.295655</td>\n",
       "      <td>-0.794883</td>\n",
       "      <td>-1.025301</td>\n",
       "      <td>-0.213784</td>\n",
       "      <td>-0.367387</td>\n",
       "      <td>-1.056893</td>\n",
       "      <td>0.765811</td>\n",
       "      <td>1.223718</td>\n",
       "      <td>-0.122904</td>\n",
       "      <td>-1.772855</td>\n",
       "      <td>1.261414</td>\n",
       "      <td>1.529433</td>\n",
       "      <td>-0.254015</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.789094</td>\n",
       "      <td>0.956844</td>\n",
       "      <td>0.216232</td>\n",
       "      <td>0.032982</td>\n",
       "      <td>-0.186666</td>\n",
       "      <td>-0.365005</td>\n",
       "      <td>1.492591</td>\n",
       "      <td>-0.108873</td>\n",
       "      <td>1.286348</td>\n",
       "      <td>-1.481087</td>\n",
       "      <td>0.313496</td>\n",
       "      <td>-0.316448</td>\n",
       "      <td>0.280948</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>-0.633695</td>\n",
       "      <td>-0.688688</td>\n",
       "      <td>0.287630</td>\n",
       "      <td>-0.843110</td>\n",
       "      <td>0.439304</td>\n",
       "      <td>-0.593944</td>\n",
       "      <td>0.321680</td>\n",
       "      <td>-0.418930</td>\n",
       "      <td>-1.988406</td>\n",
       "      <td>-0.982914</td>\n",
       "      <td>-0.567896</td>\n",
       "      <td>2.563451</td>\n",
       "      <td>-0.848526</td>\n",
       "      <td>0.630245</td>\n",
       "      <td>-1.493071</td>\n",
       "      <td>-1.043248</td>\n",
       "      <td>0.742885</td>\n",
       "      <td>-0.634762</td>\n",
       "      <td>-0.467598</td>\n",
       "      <td>-0.618751</td>\n",
       "      <td>0.155680</td>\n",
       "      <td>-1.313546</td>\n",
       "      <td>-0.366342</td>\n",
       "      <td>0.084393</td>\n",
       "      <td>-0.833920</td>\n",
       "      <td>0.844044</td>\n",
       "      <td>1.136212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.784385</td>\n",
       "      <td>-1.881721</td>\n",
       "      <td>-1.626815</td>\n",
       "      <td>0.712773</td>\n",
       "      <td>0.526955</td>\n",
       "      <td>-0.895785</td>\n",
       "      <td>1.052966</td>\n",
       "      <td>-0.187201</td>\n",
       "      <td>-0.012515</td>\n",
       "      <td>0.194762</td>\n",
       "      <td>0.537676</td>\n",
       "      <td>-2.494095</td>\n",
       "      <td>-0.771251</td>\n",
       "      <td>-1.169059</td>\n",
       "      <td>0.515979</td>\n",
       "      <td>-0.050918</td>\n",
       "      <td>-1.865379</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.032108</td>\n",
       "      <td>-0.066884</td>\n",
       "      <td>-0.370438</td>\n",
       "      <td>-0.157417</td>\n",
       "      <td>1.832491</td>\n",
       "      <td>-1.158138</td>\n",
       "      <td>-2.652128</td>\n",
       "      <td>-0.212855</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.453908</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.772282</td>\n",
       "      <td>0.420642</td>\n",
       "      <td>0.600302</td>\n",
       "      <td>0.046633</td>\n",
       "      <td>0.279207</td>\n",
       "      <td>-0.230717</td>\n",
       "      <td>0.548372</td>\n",
       "      <td>0.220223</td>\n",
       "      <td>1.184463</td>\n",
       "      <td>-0.371669</td>\n",
       "      <td>-0.576668</td>\n",
       "      <td>1.232531</td>\n",
       "      <td>2.176430</td>\n",
       "      <td>0.429022</td>\n",
       "      <td>-1.381603</td>\n",
       "      <td>-0.043974</td>\n",
       "      <td>-1.453867</td>\n",
       "      <td>-0.804753</td>\n",
       "      <td>1.498339</td>\n",
       "      <td>-0.518434</td>\n",
       "      <td>-1.499389</td>\n",
       "      <td>1.286111</td>\n",
       "      <td>1.112461</td>\n",
       "      <td>1.633326</td>\n",
       "      <td>1.271142</td>\n",
       "      <td>-1.772782</td>\n",
       "      <td>0.026696</td>\n",
       "      <td>0.859070</td>\n",
       "      <td>-0.606828</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.340881</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.387263</td>\n",
       "      <td>1.151252</td>\n",
       "      <td>1.419056</td>\n",
       "      <td>0.638762</td>\n",
       "      <td>2.045954</td>\n",
       "      <td>-0.640056</td>\n",
       "      <td>-0.604970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226103</td>\n",
       "      <td>1.391619</td>\n",
       "      <td>0.223486</td>\n",
       "      <td>0.252886</td>\n",
       "      <td>-1.774845</td>\n",
       "      <td>-0.652228</td>\n",
       "      <td>-0.055324</td>\n",
       "      <td>1.296795</td>\n",
       "      <td>0.738213</td>\n",
       "      <td>-0.747756</td>\n",
       "      <td>0.493076</td>\n",
       "      <td>-0.311358</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.244335</td>\n",
       "      <td>0.080122</td>\n",
       "      <td>-1.109347</td>\n",
       "      <td>1.557400</td>\n",
       "      <td>-0.010520</td>\n",
       "      <td>-0.500114</td>\n",
       "      <td>0.778553</td>\n",
       "      <td>-0.821763</td>\n",
       "      <td>-0.575313</td>\n",
       "      <td>-0.344703</td>\n",
       "      <td>-0.810880</td>\n",
       "      <td>-0.007987</td>\n",
       "      <td>0.321985</td>\n",
       "      <td>-0.075827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.629672</td>\n",
       "      <td>0.876864</td>\n",
       "      <td>0.411271</td>\n",
       "      <td>0.433440</td>\n",
       "      <td>0.997364</td>\n",
       "      <td>2.829590</td>\n",
       "      <td>-1.275588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.091376</td>\n",
       "      <td>0.859811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.505439</td>\n",
       "      <td>1.665086</td>\n",
       "      <td>-0.912464</td>\n",
       "      <td>-0.332054</td>\n",
       "      <td>0.707705</td>\n",
       "      <td>-1.534071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.255536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.531527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.538747</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.111787</td>\n",
       "      <td>-1.242288</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.913131</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.142546</td>\n",
       "      <td>0.621554</td>\n",
       "      <td>1.462069</td>\n",
       "      <td>-0.484931</td>\n",
       "      <td>-0.409299</td>\n",
       "      <td>0.833858</td>\n",
       "      <td>-0.897861</td>\n",
       "      <td>0.706707</td>\n",
       "      <td>-0.192703</td>\n",
       "      <td>-0.275914</td>\n",
       "      <td>-1.138562</td>\n",
       "      <td>0.971159</td>\n",
       "      <td>1.022092</td>\n",
       "      <td>-1.247827</td>\n",
       "      <td>0.362171</td>\n",
       "      <td>-0.643292</td>\n",
       "      <td>-1.108630</td>\n",
       "      <td>-1.966804</td>\n",
       "      <td>-0.120650</td>\n",
       "      <td>-1.048571</td>\n",
       "      <td>-0.255689</td>\n",
       "      <td>1.576470</td>\n",
       "      <td>-1.467532</td>\n",
       "      <td>1.051194</td>\n",
       "      <td>-2.980634</td>\n",
       "      <td>-0.250763</td>\n",
       "      <td>0.728270</td>\n",
       "      <td>-1.517486</td>\n",
       "      <td>-0.551698</td>\n",
       "      <td>-0.193385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.567878</td>\n",
       "      <td>-0.044260</td>\n",
       "      <td>-0.289074</td>\n",
       "      <td>0.026594</td>\n",
       "      <td>-1.689609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.309704</td>\n",
       "      <td>0.029623</td>\n",
       "      <td>0.021266</td>\n",
       "      <td>-0.074814</td>\n",
       "      <td>-0.018985</td>\n",
       "      <td>1.374507</td>\n",
       "      <td>-0.472786</td>\n",
       "      <td>-0.759003</td>\n",
       "      <td>-0.679297</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.149641</td>\n",
       "      <td>-0.677161</td>\n",
       "      <td>-1.663008</td>\n",
       "      <td>-1.149158</td>\n",
       "      <td>-0.329109</td>\n",
       "      <td>0.696680</td>\n",
       "      <td>0.240167</td>\n",
       "      <td>-0.427502</td>\n",
       "      <td>0.996009</td>\n",
       "      <td>1.135606</td>\n",
       "      <td>-0.129185</td>\n",
       "      <td>-0.608928</td>\n",
       "      <td>-0.589943</td>\n",
       "      <td>-1.590784</td>\n",
       "      <td>-0.132762</td>\n",
       "      <td>-0.109887</td>\n",
       "      <td>0.318993</td>\n",
       "      <td>0.147366</td>\n",
       "      <td>-1.896457</td>\n",
       "      <td>-1.712820</td>\n",
       "      <td>-0.569489</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.715887</td>\n",
       "      <td>-1.660973</td>\n",
       "      <td>-1.227575</td>\n",
       "      <td>1.293376</td>\n",
       "      <td>-0.130892</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.213802</td>\n",
       "      <td>-0.675036</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.197136</td>\n",
       "      <td>0.413594</td>\n",
       "      <td>-0.340696</td>\n",
       "      <td>1.633696</td>\n",
       "      <td>-0.048362</td>\n",
       "      <td>0.335949</td>\n",
       "      <td>0.743858</td>\n",
       "      <td>1.357425</td>\n",
       "      <td>-0.350971</td>\n",
       "      <td>1.022887</td>\n",
       "      <td>0.137418</td>\n",
       "      <td>-0.049798</td>\n",
       "      <td>-1.893938</td>\n",
       "      <td>0.203193</td>\n",
       "      <td>-0.032187</td>\n",
       "      <td>-0.183012</td>\n",
       "      <td>-2.204307</td>\n",
       "      <td>-0.787016</td>\n",
       "      <td>-0.122264</td>\n",
       "      <td>-0.643352</td>\n",
       "      <td>1.243026</td>\n",
       "      <td>-0.754825</td>\n",
       "      <td>-1.298621</td>\n",
       "      <td>-0.636491</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.509722</td>\n",
       "      <td>-0.761199</td>\n",
       "      <td>0.842186</td>\n",
       "      <td>0.356877</td>\n",
       "      <td>-0.530667</td>\n",
       "      <td>-0.063085</td>\n",
       "      <td>0.494949</td>\n",
       "      <td>-0.589909</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.829457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.124452</td>\n",
       "      <td>0.968566</td>\n",
       "      <td>-0.039368</td>\n",
       "      <td>0.560734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.870426</td>\n",
       "      <td>1.305617</td>\n",
       "      <td>-0.679976</td>\n",
       "      <td>-0.810384</td>\n",
       "      <td>-0.116147</td>\n",
       "      <td>-0.200228</td>\n",
       "      <td>0.425795</td>\n",
       "      <td>-0.116398</td>\n",
       "      <td>-0.350091</td>\n",
       "      <td>0.285263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.977235</td>\n",
       "      <td>0.070951</td>\n",
       "      <td>-0.883801</td>\n",
       "      <td>-0.048479</td>\n",
       "      <td>-0.831456</td>\n",
       "      <td>0.191986</td>\n",
       "      <td>-0.190855</td>\n",
       "      <td>1.087573</td>\n",
       "      <td>-1.087789</td>\n",
       "      <td>-0.286521</td>\n",
       "      <td>-1.141951</td>\n",
       "      <td>1.175956</td>\n",
       "      <td>0.702744</td>\n",
       "      <td>-0.521506</td>\n",
       "      <td>1.038250</td>\n",
       "      <td>0.859847</td>\n",
       "      <td>-1.076477</td>\n",
       "      <td>0.492671</td>\n",
       "      <td>-0.475863</td>\n",
       "      <td>0.700593</td>\n",
       "      <td>0.052458</td>\n",
       "      <td>-1.249765</td>\n",
       "      <td>-2.061279</td>\n",
       "      <td>-0.327044</td>\n",
       "      <td>-0.608455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.811862</td>\n",
       "      <td>0.085749</td>\n",
       "      <td>0.926526</td>\n",
       "      <td>-0.640060</td>\n",
       "      <td>-0.718608</td>\n",
       "      <td>1.140483</td>\n",
       "      <td>0.149081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.493844</td>\n",
       "      <td>-0.792096</td>\n",
       "      <td>-0.029806</td>\n",
       "      <td>-0.282305</td>\n",
       "      <td>-0.115326</td>\n",
       "      <td>0.215455</td>\n",
       "      <td>0.103860</td>\n",
       "      <td>-0.504598</td>\n",
       "      <td>0.580469</td>\n",
       "      <td>-1.688337</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.127945</td>\n",
       "      <td>0.294004</td>\n",
       "      <td>-0.746238</td>\n",
       "      <td>0.440146</td>\n",
       "      <td>-0.636260</td>\n",
       "      <td>0.290518</td>\n",
       "      <td>-0.704385</td>\n",
       "      <td>-0.450725</td>\n",
       "      <td>-3.093100</td>\n",
       "      <td>-0.287180</td>\n",
       "      <td>-0.057649</td>\n",
       "      <td>-1.638705</td>\n",
       "      <td>0.101617</td>\n",
       "      <td>-0.239199</td>\n",
       "      <td>-0.187759</td>\n",
       "      <td>0.150193</td>\n",
       "      <td>-0.919172</td>\n",
       "      <td>0.786008</td>\n",
       "      <td>-0.297549</td>\n",
       "      <td>-0.554850</td>\n",
       "      <td>-0.648266</td>\n",
       "      <td>-0.200767</td>\n",
       "      <td>1.525016</td>\n",
       "      <td>-0.446496</td>\n",
       "      <td>-0.604503</td>\n",
       "      <td>0.979317</td>\n",
       "      <td>-0.677610</td>\n",
       "      <td>-0.329548</td>\n",
       "      <td>1.138691</td>\n",
       "      <td>0.042309</td>\n",
       "      <td>-0.600066</td>\n",
       "      <td>-2.375618</td>\n",
       "      <td>-1.601539</td>\n",
       "      <td>-1.757651</td>\n",
       "      <td>0.809631</td>\n",
       "      <td>0.089887</td>\n",
       "      <td>-1.348452</td>\n",
       "      <td>0.191064</td>\n",
       "      <td>-0.122656</td>\n",
       "      <td>0.024868</td>\n",
       "      <td>0.438857</td>\n",
       "      <td>0.866927</td>\n",
       "      <td>-0.360168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.950951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.219464</td>\n",
       "      <td>0.740766</td>\n",
       "      <td>0.578892</td>\n",
       "      <td>1.220291</td>\n",
       "      <td>-0.353242</td>\n",
       "      <td>0.681880</td>\n",
       "      <td>1.188646</td>\n",
       "      <td>-0.031654</td>\n",
       "      <td>0.606029</td>\n",
       "      <td>-0.708277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.336234</td>\n",
       "      <td>-0.223922</td>\n",
       "      <td>-1.151432</td>\n",
       "      <td>-1.159671</td>\n",
       "      <td>-0.121942</td>\n",
       "      <td>0.764458</td>\n",
       "      <td>-1.558847</td>\n",
       "      <td>-0.691117</td>\n",
       "      <td>-1.114062</td>\n",
       "      <td>-1.096332</td>\n",
       "      <td>-1.166924</td>\n",
       "      <td>-0.190920</td>\n",
       "      <td>1.617828</td>\n",
       "      <td>-0.154894</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.685085</td>\n",
       "      <td>-0.259366</td>\n",
       "      <td>1.761386</td>\n",
       "      <td>0.279956</td>\n",
       "      <td>0.305717</td>\n",
       "      <td>-0.545458</td>\n",
       "      <td>-0.472276</td>\n",
       "      <td>-0.964219</td>\n",
       "      <td>-0.144359</td>\n",
       "      <td>-1.157257</td>\n",
       "      <td>0.193033</td>\n",
       "      <td>-0.512118</td>\n",
       "      <td>-0.495367</td>\n",
       "      <td>0.333013</td>\n",
       "      <td>-0.900259</td>\n",
       "      <td>-0.366629</td>\n",
       "      <td>-0.087985</td>\n",
       "      <td>0.155583</td>\n",
       "      <td>0.533653</td>\n",
       "      <td>-0.713744</td>\n",
       "      <td>-0.698580</td>\n",
       "      <td>-0.417718</td>\n",
       "      <td>0.221396</td>\n",
       "      <td>-0.458552</td>\n",
       "      <td>-0.229346</td>\n",
       "      <td>0.275596</td>\n",
       "      <td>-1.555731</td>\n",
       "      <td>0.560419</td>\n",
       "      <td>-0.163745</td>\n",
       "      <td>-2.320376</td>\n",
       "      <td>1.546314</td>\n",
       "      <td>-1.162547</td>\n",
       "      <td>0.338974</td>\n",
       "      <td>0.082602</td>\n",
       "      <td>-2.248687</td>\n",
       "      <td>0.690273</td>\n",
       "      <td>-0.490256</td>\n",
       "      <td>-1.269430</td>\n",
       "      <td>-0.213861</td>\n",
       "      <td>-0.092103</td>\n",
       "      <td>-0.962888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.182431</td>\n",
       "      <td>-0.088560</td>\n",
       "      <td>0.497103</td>\n",
       "      <td>0.055780</td>\n",
       "      <td>1.111765</td>\n",
       "      <td>-0.470018</td>\n",
       "      <td>-0.815203</td>\n",
       "      <td>-2.134086</td>\n",
       "      <td>0.112046</td>\n",
       "      <td>1.179066</td>\n",
       "      <td>0.149388</td>\n",
       "      <td>-0.592423</td>\n",
       "      <td>-0.931485</td>\n",
       "      <td>0.071299</td>\n",
       "      <td>-1.226630</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>-0.262121</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.050555</td>\n",
       "      <td>-0.299566</td>\n",
       "      <td>-1.539228</td>\n",
       "      <td>0.293038</td>\n",
       "      <td>1.659428</td>\n",
       "      <td>-0.334744</td>\n",
       "      <td>-1.186351</td>\n",
       "      <td>-1.496472</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.398077</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.195579</td>\n",
       "      <td>0.628814</td>\n",
       "      <td>0.694344</td>\n",
       "      <td>0.440065</td>\n",
       "      <td>-0.579346</td>\n",
       "      <td>-0.360595</td>\n",
       "      <td>-0.753811</td>\n",
       "      <td>0.265414</td>\n",
       "      <td>0.763227</td>\n",
       "      <td>-0.040894</td>\n",
       "      <td>0.231637</td>\n",
       "      <td>-0.415793</td>\n",
       "      <td>-0.609442</td>\n",
       "      <td>-0.467309</td>\n",
       "      <td>1.639633</td>\n",
       "      <td>-0.574299</td>\n",
       "      <td>-0.793408</td>\n",
       "      <td>3.043426</td>\n",
       "      <td>-1.766276</td>\n",
       "      <td>-1.974434</td>\n",
       "      <td>-0.406092</td>\n",
       "      <td>0.251213</td>\n",
       "      <td>-0.969566</td>\n",
       "      <td>-0.526810</td>\n",
       "      <td>-0.224624</td>\n",
       "      <td>-0.295939</td>\n",
       "      <td>-0.825563</td>\n",
       "      <td>0.061085</td>\n",
       "      <td>-1.205872</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.018431</td>\n",
       "      <td>0.635342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.123141</td>\n",
       "      <td>-0.196775</td>\n",
       "      <td>0.513455</td>\n",
       "      <td>0.378646</td>\n",
       "      <td>-0.197650</td>\n",
       "      <td>-0.024398</td>\n",
       "      <td>0.052106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.611166</td>\n",
       "      <td>-1.361757</td>\n",
       "      <td>-0.871222</td>\n",
       "      <td>-0.640538</td>\n",
       "      <td>-1.095264</td>\n",
       "      <td>1.048687</td>\n",
       "      <td>-0.795837</td>\n",
       "      <td>-0.123556</td>\n",
       "      <td>0.110427</td>\n",
       "      <td>-1.161328</td>\n",
       "      <td>1.770563</td>\n",
       "      <td>-0.480965</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.707384</td>\n",
       "      <td>1.766121</td>\n",
       "      <td>-0.140636</td>\n",
       "      <td>0.889687</td>\n",
       "      <td>-0.925039</td>\n",
       "      <td>0.049110</td>\n",
       "      <td>-0.470497</td>\n",
       "      <td>-0.590048</td>\n",
       "      <td>-0.666548</td>\n",
       "      <td>-0.478903</td>\n",
       "      <td>-1.696438</td>\n",
       "      <td>0.932974</td>\n",
       "      <td>0.828886</td>\n",
       "      <td>0.140387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.624304</td>\n",
       "      <td>-2.197691</td>\n",
       "      <td>-1.479267</td>\n",
       "      <td>-0.465917</td>\n",
       "      <td>-0.014757</td>\n",
       "      <td>-0.320434</td>\n",
       "      <td>-0.511896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target   smpl  id  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0       0  train   0   0.468142  -1.045346        0.0   0.384487   0.435121   \n",
       "1       0  train   1  -0.760983   0.515132        0.0  -1.673905  -0.393862   \n",
       "2       0  train   2   1.658855   0.915052        0.0  -0.581082   0.477199   \n",
       "3       0  train   3  -0.638854   0.314099        0.0   0.000919   1.102342   \n",
       "4       0  train   4  -1.091376   0.859811        0.0  -0.505439   1.665086   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
       "0  -1.178548   0.124543   1.801544   0.332028         0.0    0.386473   \n",
       "1  -1.584207  -0.439778   0.796104  -1.331832         0.0   -0.958243   \n",
       "2  -0.622226   0.390642   0.753299   0.400727         0.0   -0.637646   \n",
       "3  -0.807371   0.329158   0.484305   0.412519         1.0   -0.715940   \n",
       "4  -0.912464  -0.332054   0.707705  -1.534071         0.0   -0.255536   \n",
       "\n",
       "   feature_12  feature_13  feature_14  feature_15  feature_16  feature_17  \\\n",
       "0         0.0   -0.342975         0.0         0.0    0.642325         4.0   \n",
       "1         0.0   -1.539291         0.0         0.0   -0.103201         4.0   \n",
       "2         1.0    0.033663         0.0         0.0   -1.213259         4.0   \n",
       "3         0.0    1.859640         0.0         1.0   -0.304999         4.0   \n",
       "4         0.0    1.531527         0.0         0.0   -0.538747         4.0   \n",
       "\n",
       "   feature_18  feature_19  feature_20  feature_21  feature_22  feature_23  \\\n",
       "0    2.073636    1.832064         0.0         0.0    0.040974         4.0   \n",
       "1    0.655647    0.247942         1.0         0.0   -0.590149         4.0   \n",
       "2   -0.333812   -0.074908         1.0         0.0    0.602250         4.0   \n",
       "3    0.541586   -0.458376         1.0         0.0   -0.046954         4.0   \n",
       "4    1.111787   -1.242288         1.0         0.0    0.913131         4.0   \n",
       "\n",
       "   feature_24  feature_25  feature_26  feature_27  feature_28  feature_29  \\\n",
       "0    0.319570    0.678404    2.943154    0.680208    0.514305   -0.299057   \n",
       "1    0.973363   -0.875104    2.176944   -0.727265   -0.925263   -0.345907   \n",
       "2    0.335774    1.782889    0.756021    0.280975   -0.317603    0.370240   \n",
       "3    0.212354   -0.557272    0.586641    0.946142    2.434117    0.746217   \n",
       "4    0.142546    0.621554    1.462069   -0.484931   -0.409299    0.833858   \n",
       "\n",
       "   feature_30  feature_31  feature_32  feature_33  feature_34  feature_35  \\\n",
       "0   -0.298444    0.486146   -0.187802    0.780795   -0.802504    0.675421   \n",
       "1   -1.403761   -0.534659   -0.773095   -0.867220    0.085268    1.326112   \n",
       "2   -0.397848    0.752353    0.093410    0.763672   -0.981743   -0.012486   \n",
       "3   -0.134858    0.667817   -1.311813   -0.987538    0.090251   -0.170246   \n",
       "4   -0.897861    0.706707   -0.192703   -0.275914   -1.138562    0.971159   \n",
       "\n",
       "   feature_36  feature_37  feature_38  feature_39  feature_40  feature_41  \\\n",
       "0   -0.599030    2.299501   -0.612656    1.769400   -0.113554    0.349934   \n",
       "1    0.903044   -0.037805   -0.570683    1.098100   -0.650058    0.838633   \n",
       "2   -0.238419   -0.326872   -1.767364    1.308259   -1.347245   -0.772304   \n",
       "3    0.988806   -0.343382    0.327664   -0.693446   -1.197079    1.599838   \n",
       "4    1.022092   -1.247827    0.362171   -0.643292   -1.108630   -1.966804   \n",
       "\n",
       "   feature_42  feature_43  feature_44  feature_45  feature_46  feature_47  \\\n",
       "0   -0.347432    0.010639   -0.196911    1.300514   -0.080902    0.796330   \n",
       "1    0.818650    0.420707   -2.095442   -0.590573   -0.280188    1.177920   \n",
       "2    2.009681   -1.810025    0.498785    0.140077    1.813894   -0.149701   \n",
       "3    1.526965   -0.183530   -0.575217   -0.721305    0.921539    0.463246   \n",
       "4   -0.120650   -1.048571   -0.255689    1.576470   -1.467532    1.051194   \n",
       "\n",
       "   feature_48  feature_49  feature_50  feature_51  feature_52  feature_53  \\\n",
       "0   -0.571195    0.750615   -1.994526   -1.202341   -1.463523   -0.460235   \n",
       "1   -1.739458    0.625996   -0.327274    0.138043   -1.642201    0.646289   \n",
       "2   -0.243074   -0.946646   -1.632846    0.367061   -1.388741   -0.284864   \n",
       "3    0.980302    0.260606   -0.303126   -0.241599   -1.439445    0.506683   \n",
       "4   -2.980634   -0.250763    0.728270   -1.517486   -0.551698   -0.193385   \n",
       "\n",
       "   feature_54  feature_55  feature_56  feature_57  feature_58  feature_59  \\\n",
       "0         0.0   -0.041449    0.383248    0.093989    0.788592   -0.042238   \n",
       "1         0.0   -0.180714   -0.052744   -1.033324    0.837395    0.452283   \n",
       "2         0.0    0.443954   -1.493657    1.129079    0.176012    2.365263   \n",
       "3         0.0   -0.728534    1.333196    2.212643    0.989996    1.200458   \n",
       "4         0.0    0.567878   -0.044260   -0.289074    0.026594   -1.689609   \n",
       "\n",
       "   feature_60  feature_61  feature_62  feature_63  feature_64  feature_65  \\\n",
       "0         0.0    0.126452    0.527408    0.853002   -0.581672    0.523510   \n",
       "1         1.0    0.252064   -0.488690    0.768790    0.176442   -0.621753   \n",
       "2         0.0    0.723328    1.020088    0.399541    1.015374    0.432726   \n",
       "3         0.0    2.777314    1.356488    1.334583    0.507161   -0.734510   \n",
       "4         0.0    1.309704    0.029623    0.021266   -0.074814   -0.018985   \n",
       "\n",
       "   feature_66  feature_67  feature_68  feature_69  feature_70  feature_71  \\\n",
       "0    1.462681    0.499926   -0.998070   -0.902830         0.0    0.660541   \n",
       "1    0.406738   -0.033521   -1.128229   -0.556230         0.0   -1.264228   \n",
       "2    0.493677    0.097214   -0.303480    0.026299         1.0    1.462163   \n",
       "3    1.530486    1.015725   -0.248070   -0.797367         0.0    0.647778   \n",
       "4    1.374507   -0.472786   -0.759003   -0.679297         1.0   -0.149641   \n",
       "\n",
       "   feature_72  feature_73  feature_74  feature_75  feature_76  feature_77  \\\n",
       "0    0.075661    0.263430   -0.220035    1.186972    0.860134   -0.152366   \n",
       "1   -0.134580   -1.321945   -0.233987   -0.367274    0.192242    0.099444   \n",
       "2    1.694762   -0.104124    0.153415   -0.187552    0.588539   -2.042665   \n",
       "3    1.082519    0.240294    0.797235   -1.394951    1.847186   -0.880586   \n",
       "4   -0.677161   -1.663008   -1.149158   -0.329109    0.696680    0.240167   \n",
       "\n",
       "   feature_78  feature_79  feature_80  feature_81  feature_82  feature_83  \\\n",
       "0   -0.188687    1.746777   -0.299935   -1.277905   -0.653038   -0.646313   \n",
       "1   -2.434816    0.221371   -1.670063   -1.173539   -0.529720   -0.530069   \n",
       "2   -0.829246    0.138201    1.059464   -0.122988   -0.608095   -0.604866   \n",
       "3    0.933163    0.397004    0.140066    0.148379   -0.269661   -0.617402   \n",
       "4   -0.427502    0.996009    1.135606   -0.129185   -0.608928   -0.589943   \n",
       "\n",
       "   feature_84  feature_85  feature_86  feature_87  feature_88  feature_89  \\\n",
       "0    0.218737   -0.824315    0.686127   -0.805713    0.594855   -0.299846   \n",
       "1    0.295520    0.153126   -1.731802   -0.201868   -2.710627    0.933431   \n",
       "2   -1.122324    0.887297   -1.513382    0.289026    1.395137    0.856310   \n",
       "3   -0.222095   -0.440414   -1.017426   -0.127428    0.316201   -0.126269   \n",
       "4   -1.590784   -0.132762   -0.109887    0.318993    0.147366   -1.896457   \n",
       "\n",
       "   feature_90  feature_91  feature_92  feature_93  feature_94  feature_95  \\\n",
       "0    0.632772   -0.538958         7.0    0.777204    0.322346    0.449603   \n",
       "1    0.173819   -0.409696         7.0   -2.124777    0.352603   -0.149513   \n",
       "2   -0.778699   -0.939723         7.0   -0.193845   -0.249936   -1.196983   \n",
       "3   -0.709465   -0.691001         7.0    1.172375    0.167786   -0.245611   \n",
       "4   -1.712820   -0.569489         7.0    1.715887   -1.660973   -1.227575   \n",
       "\n",
       "   feature_96  feature_97  feature_98  feature_99  feature_100  feature_101  \\\n",
       "0    0.224158   -0.011592         0.0    0.313750     2.327911          3.0   \n",
       "1    0.541060   -1.845491         1.0    1.019815    -0.101222          3.0   \n",
       "2    0.028618   -0.026915         0.0    1.110695    -0.708828          3.0   \n",
       "3    0.948606   -1.359652         1.0   -0.539031    -0.877148          3.0   \n",
       "4    1.293376   -0.130892         1.0   -1.213802    -0.675036          3.0   \n",
       "\n",
       "   feature_102  feature_103  feature_104  feature_105  feature_106  \\\n",
       "0     0.074699    -1.222799     1.126873    -0.897274    -1.169933   \n",
       "1    -1.073929    -1.288809    -0.169407     1.221088    -1.022738   \n",
       "2    -0.429845     0.224793    -1.353030     0.188018     0.822470   \n",
       "3    -1.429774    -1.205323     1.306246     2.245903    -0.072128   \n",
       "4    -0.197136     0.413594    -0.340696     1.633696    -0.048362   \n",
       "\n",
       "   feature_107  feature_108  feature_109  feature_110  feature_111  \\\n",
       "0     1.764549     0.051998     0.059242    -1.180479    -1.443983   \n",
       "1    -0.089219     1.170025     0.381867    -1.294279    -2.803755   \n",
       "2     1.690886     0.641219     0.719090     1.471495    -0.109378   \n",
       "3     0.434162    -0.005514     0.631682    -0.683774    -0.538104   \n",
       "4     0.335949     0.743858     1.357425    -0.350971     1.022887   \n",
       "\n",
       "   feature_112  feature_113  feature_114  feature_115  feature_116  \\\n",
       "0    -1.277013    -0.134019     1.490995    -1.007765    -0.083143   \n",
       "1     0.704241    -0.484860    -0.080280     0.385668    -0.230779   \n",
       "2     0.943723    -0.373415     0.559970     0.243101    -0.069229   \n",
       "3    -0.644193    -0.587877    -0.273172    -0.082105    -0.102266   \n",
       "4     0.137418    -0.049798    -1.893938     0.203193    -0.032187   \n",
       "\n",
       "   feature_117  feature_118  feature_119  feature_120  feature_121  \\\n",
       "0    -1.205676    -0.658353    -1.319770    -0.586360     0.262648   \n",
       "1    -0.222709    -0.610192     1.078268     1.649578     0.885831   \n",
       "2    -0.156425    -0.483252     1.277412     0.636291    -0.175425   \n",
       "3     0.743286    -1.602893     2.705511    -0.850264     0.837179   \n",
       "4    -0.183012    -2.204307    -0.787016    -0.122264    -0.643352   \n",
       "\n",
       "   feature_122  feature_123  feature_124  feature_125  feature_126  \\\n",
       "0    -0.191245    -0.393026     1.804129     0.049894          4.0   \n",
       "1    -1.243703     0.819158     0.364572    -0.066308          4.0   \n",
       "2     1.888265     0.757866    -0.702022     0.102439          4.0   \n",
       "3    -0.408891     1.623775    -0.056327    -0.592473          4.0   \n",
       "4     1.243026    -0.754825    -1.298621    -0.636491          4.0   \n",
       "\n",
       "   feature_127  feature_128  feature_129  feature_130  feature_131  \\\n",
       "0     0.203395    -1.453285    -0.383524     1.109589    -0.240194   \n",
       "1    -1.602314    -0.286886     0.179758    -1.088347    -0.480089   \n",
       "2     0.468759     0.585031     1.229884    -1.283153     0.464297   \n",
       "3    -0.393227    -0.659929     0.096845     0.560023     0.483311   \n",
       "4    -0.509722    -0.761199     0.842186     0.356877    -0.530667   \n",
       "\n",
       "   feature_132  feature_133  feature_134  feature_135  feature_136  \\\n",
       "0     0.413364    -0.759104     0.206667          8.0     0.306821   \n",
       "1     0.902837    -1.130827    -1.169932          8.0    -1.487134   \n",
       "2     0.137619    -0.308007     0.167366          8.0    -0.976838   \n",
       "3    -0.688010    -0.311281    -0.332794          8.0     0.132559   \n",
       "4    -0.063085     0.494949    -0.589909          8.0    -0.829457   \n",
       "\n",
       "   feature_137  feature_138  feature_139  feature_140  feature_141  \\\n",
       "0          0.0    -0.844305    -0.127400    -0.294801     0.240251   \n",
       "1          0.0     0.116846    -0.058836    -0.672462    -0.740467   \n",
       "2          0.0    -0.163929    -0.857007    -0.122315    -0.046688   \n",
       "3          0.0    -0.960162    -2.026977    -0.457321     1.496611   \n",
       "4          0.0    -0.124452     0.968566    -0.039368     0.560734   \n",
       "\n",
       "   feature_142  feature_143  feature_144  feature_145  feature_146  \\\n",
       "0          0.0     0.024730    -0.656020     0.337954    -0.124647   \n",
       "1          0.0    -0.603325     0.656015     0.333940     0.016165   \n",
       "2          0.0     0.886777     0.296281    -0.505381     1.615476   \n",
       "3          0.0    -0.883562    -0.115753    -1.076133    -0.465735   \n",
       "4          1.0    -1.870426     1.305617    -0.679976    -0.810384   \n",
       "\n",
       "   feature_147  feature_148  feature_149  feature_150  feature_151  \\\n",
       "0    -0.246440     0.258104     0.744419     0.407665    -0.930420   \n",
       "1    -1.268323    -0.962288     0.913720    -0.616531     0.601266   \n",
       "2     1.238792    -0.073948    -0.542446     0.048058    -0.224241   \n",
       "3     0.278139    -1.850325     0.522737     1.577154    -0.057310   \n",
       "4    -0.116147    -0.200228     0.425795    -0.116398    -0.350091   \n",
       "\n",
       "   feature_152  feature_153  feature_154  feature_155  feature_156  \\\n",
       "0    -1.228201          0.0    -0.017206     0.585183     1.894088   \n",
       "1    -1.894769          0.0     0.996772     0.007350    -1.572205   \n",
       "2    -0.201844          0.0     0.208764    -0.789506     0.371502   \n",
       "3     1.311651          0.0     0.610428    -0.253664     1.526026   \n",
       "4     0.285263          0.0     0.977235     0.070951    -0.883801   \n",
       "\n",
       "   feature_157  feature_158  feature_159  feature_160  feature_161  \\\n",
       "0     0.178525    -1.397601    -0.618168     0.507300     0.069234   \n",
       "1     0.088652    -1.798130     0.656918     0.529805     0.713996   \n",
       "2    -0.000097     1.388004     0.312974    -0.721760     0.390571   \n",
       "3    -1.107451     0.213915     0.274537    -0.311793     0.108339   \n",
       "4    -0.048479    -0.831456     0.191986    -0.190855     1.087573   \n",
       "\n",
       "   feature_162  feature_163  feature_164  feature_165  feature_166  \\\n",
       "0     0.847564     0.293913     0.456653     1.405501     0.164698   \n",
       "1    -1.083242    -0.051258    -0.043511     0.567080    -2.165406   \n",
       "2    -1.127536     0.476640    -1.363957     1.527885     0.342472   \n",
       "3    -1.133135    -0.246972    -0.277572    -2.619473    -0.315480   \n",
       "4    -1.087789    -0.286521    -1.141951     1.175956     0.702744   \n",
       "\n",
       "   feature_167  feature_168  feature_169  feature_170  feature_171  \\\n",
       "0     1.105637     0.252699    -0.632100    -1.364036    -0.189104   \n",
       "1    -0.468783     0.753014     1.191944    -0.774412    -1.826900   \n",
       "2     0.778398     1.115198     0.372435     0.457172    -0.189774   \n",
       "3     1.682810     0.944929     0.828605     0.416779     0.174765   \n",
       "4    -0.521506     1.038250     0.859847    -1.076477     0.492671   \n",
       "\n",
       "   feature_172  feature_173  feature_174  feature_175  feature_176  \\\n",
       "0    -0.473863     0.166256     0.009374    -0.257498    -0.203411   \n",
       "1     0.460043     0.451270    -1.049715    -1.039448    -0.214271   \n",
       "2     0.597187     0.536737     0.799887    -0.244411    -2.241794   \n",
       "3    -1.371408     0.637738     0.454484    -0.628541    -1.661455   \n",
       "4    -0.475863     0.700593     0.052458    -1.249765    -2.061279   \n",
       "\n",
       "   feature_177  feature_178  feature_179  feature_180  feature_181  \\\n",
       "0    -3.130904     1.050359          0.0    -0.029937    -0.660350   \n",
       "1    -1.494830     0.138920          0.0     1.365203     0.231139   \n",
       "2     1.505090    -0.236390          1.0    -0.809148     1.423503   \n",
       "3    -0.769516    -0.204544          0.0     1.228278    -0.489225   \n",
       "4    -0.327044    -0.608455          0.0     0.811862     0.085749   \n",
       "\n",
       "   feature_182  feature_183  feature_184  feature_185  feature_186  \\\n",
       "0     0.630274    -0.720168     1.587987    -0.903709    -0.846349   \n",
       "1     0.212887    -0.867707    -0.845028    -1.738828    -2.432545   \n",
       "2    -0.026988    -1.855442    -1.053101     1.801683    -1.561516   \n",
       "3    -0.125213    -0.654200    -0.724949    -0.477704     0.969274   \n",
       "4     0.926526    -0.640060    -0.718608     1.140483     0.149081   \n",
       "\n",
       "   feature_187  feature_188  feature_189  feature_190  feature_191  \\\n",
       "0          0.0    -1.415833    -0.592974    -0.792199     0.629578   \n",
       "1          1.0     0.066601     0.522979     0.565327     0.433472   \n",
       "2          0.0     1.145060    -0.592703     0.584384    -1.362151   \n",
       "3          0.0     0.169830     0.776918     0.649593     0.205862   \n",
       "4          0.0    -0.493844    -0.792096    -0.029806    -0.282305   \n",
       "\n",
       "   feature_192  feature_193  feature_194  feature_195  feature_196  \\\n",
       "0    -0.042491    -1.557814     0.293117     0.182232     0.631767   \n",
       "1     0.689002     0.617479     0.304762    -0.771716     1.119108   \n",
       "2    -1.271011     0.457746    -0.545932     0.043374     1.261645   \n",
       "3     0.567788     0.785037     1.627670    -1.823888     0.916430   \n",
       "4    -0.115326     0.215455     0.103860    -0.504598     0.580469   \n",
       "\n",
       "   feature_197  feature_198  feature_199  feature_200  feature_201  \\\n",
       "0    -0.275105          0.0     1.814922    -1.429966    -1.967324   \n",
       "1     0.509343          1.0    -0.411803    -0.452566     1.194920   \n",
       "2    -0.046620          0.0    -0.490928     1.063507     0.025955   \n",
       "3    -0.752902          1.0    -0.284271     1.370576     0.180590   \n",
       "4    -1.688337          1.0     1.127945     0.294004    -0.746238   \n",
       "\n",
       "   feature_202  feature_203  feature_204  feature_205  feature_206  \\\n",
       "0    -0.011894     1.382303    -1.684421     0.600606    -1.678716   \n",
       "1    -1.891247     1.525119     0.210434     0.988621    -1.119946   \n",
       "2    -1.477186    -0.103165    -1.260223    -0.302939     0.328441   \n",
       "3     0.186486    -1.648581    -0.202836    -1.615217    -2.433494   \n",
       "4     0.440146    -0.636260     0.290518    -0.704385    -0.450725   \n",
       "\n",
       "   feature_207  feature_208  feature_209  feature_210  feature_211  \\\n",
       "0     0.278021     5.199338     0.887662     2.146324    -1.206914   \n",
       "1     0.191252     0.078334    -0.907423    -0.821667    -0.519094   \n",
       "2    -1.051807    -0.676958     0.160137     0.725534     0.853314   \n",
       "3    -0.254696     0.534700    -2.323304     1.423436     1.511624   \n",
       "4    -3.093100    -0.287180    -0.057649    -1.638705     0.101617   \n",
       "\n",
       "   feature_212  feature_213  feature_214  feature_215  feature_216  \\\n",
       "0    -0.673522    -0.666780     0.137190     0.866534    -0.915680   \n",
       "1    -0.034778     0.052776    -1.029502     0.100321     1.675569   \n",
       "2    -0.079099     0.339358    -0.844095    -0.416158     0.147965   \n",
       "3    -0.116746     0.199191    -1.157929    -0.857270     0.311955   \n",
       "4    -0.239199    -0.187759     0.150193    -0.919172     0.786008   \n",
       "\n",
       "   feature_217  feature_218  feature_219  feature_220  feature_221  \\\n",
       "0     0.896887     0.864203    -0.708157    -0.150261    -0.683593   \n",
       "1     0.748420    -1.689240     1.461673     1.019182     0.013909   \n",
       "2    -0.652813     0.146464     0.182509     0.353547    -0.568627   \n",
       "3     2.251281     1.976119     1.678888     2.060178     0.408986   \n",
       "4    -0.297549    -0.554850    -0.648266    -0.200767     1.525016   \n",
       "\n",
       "   feature_222  feature_223  feature_224  feature_225  feature_226  \\\n",
       "0     0.090318     0.226919     0.214739     0.537111     0.599982   \n",
       "1    -1.441045    -0.253227     0.702967     0.632428     0.960560   \n",
       "2     0.316609    -0.408552    -1.172877    -2.797013     0.570071   \n",
       "3    -0.209740    -1.261743    -1.053186     0.111571     0.880554   \n",
       "4    -0.446496    -0.604503     0.979317    -0.677610    -0.329548   \n",
       "\n",
       "   feature_227  feature_228  feature_229  feature_230  feature_231  \\\n",
       "0     1.377434     0.283255     2.205156    -1.319568    -0.494630   \n",
       "1    -0.019770     0.915882    -0.111479     0.069311    -1.328630   \n",
       "2     1.237126    -0.394783     0.057857     0.951690     0.910740   \n",
       "3     0.771447     0.653848    -1.259758     1.379058     1.544118   \n",
       "4     1.138691     0.042309    -0.600066    -2.375618    -1.601539   \n",
       "\n",
       "   feature_232  feature_233  feature_234  feature_235  feature_236  \\\n",
       "0     0.564165     0.211109     0.275188    -0.561924     0.476641   \n",
       "1    -0.274612    -2.130145    -1.694103    -1.271975     0.796754   \n",
       "2     0.084905    -0.894216     0.091887    -0.629352     0.083650   \n",
       "3    -1.309901    -1.336027     1.512860    -1.545349    -0.503860   \n",
       "4    -1.757651     0.809631     0.089887    -1.348452     0.191064   \n",
       "\n",
       "   feature_237  feature_238  feature_239  feature_240  feature_241  \\\n",
       "0     0.916984     0.129031    -1.278033    -1.025281     0.289461   \n",
       "1     1.768707    -0.222346    -0.763386    -0.035096     0.009873   \n",
       "2     0.560426     1.155306     0.780005    -1.269435    -0.410563   \n",
       "3     0.752157     1.234394     1.747207     0.154937    -0.336034   \n",
       "4    -0.122656     0.024868     0.438857     0.866927    -0.360168   \n",
       "\n",
       "   feature_242  feature_243  feature_244  feature_245  feature_246  \\\n",
       "0          0.0    -0.568284          0.0     0.444746    -0.709188   \n",
       "1          0.0    -0.422882          0.0     2.053811     1.587036   \n",
       "2          1.0     0.134407          0.0    -0.516262    -0.355502   \n",
       "3          0.0    -0.240721          0.0    -0.792359    -0.725082   \n",
       "4          0.0    -0.950951          0.0    -1.219464     0.740766   \n",
       "\n",
       "   feature_247  feature_248  feature_249  feature_250  feature_251  \\\n",
       "0     0.637992     0.267719     0.011707    -0.259137    -0.823495   \n",
       "1     1.181980    -0.986534    -1.156395     0.123150    -1.012012   \n",
       "2     1.328436     0.823952    -0.962213     0.142541     1.090114   \n",
       "3     0.976723     0.156261    -0.539084    -0.587726    -0.367335   \n",
       "4     0.578892     1.220291    -0.353242     0.681880     1.188646   \n",
       "\n",
       "   feature_252  feature_253  feature_254  feature_255  feature_256  \\\n",
       "0    -0.379951    -1.143183    -0.412407          0.0    -0.580750   \n",
       "1    -0.533884     0.321240    -1.716837          1.0    -1.204142   \n",
       "2    -0.204129     1.321770    -0.217190          0.0     0.961567   \n",
       "3    -0.766549     0.421441    -0.407718          0.0     0.449698   \n",
       "4    -0.031654     0.606029    -0.708277          0.0    -0.336234   \n",
       "\n",
       "   feature_257  feature_258  feature_259  feature_260  feature_261  \\\n",
       "0    -0.044633     0.150660     0.476105    -0.676810     0.394757   \n",
       "1    -1.171752    -1.117391     1.092238     0.955735    -0.072982   \n",
       "2    -1.059627     0.078978    -0.610295     1.411798     0.975517   \n",
       "3    -0.295655    -0.794883    -1.025301    -0.213784    -0.367387   \n",
       "4    -0.223922    -1.151432    -1.159671    -0.121942     0.764458   \n",
       "\n",
       "   feature_262  feature_263  feature_264  feature_265  feature_266  \\\n",
       "0     0.604259     0.807586    -1.459422    -0.234030     0.821112   \n",
       "1    -0.078929     0.060325     0.333696     0.481277     0.611947   \n",
       "2    -0.086469    -0.439670    -0.625058     0.534661     0.994917   \n",
       "3    -1.056893     0.765811     1.223718    -0.122904    -1.772855   \n",
       "4    -1.558847    -0.691117    -1.114062    -1.096332    -1.166924   \n",
       "\n",
       "   feature_267  feature_268  feature_269  feature_270  feature_271  \\\n",
       "0     0.515814     0.947696     0.565654          2.0     1.511110   \n",
       "1    -1.195943    -0.969317     0.629833          2.0    -0.533139   \n",
       "2     1.911488     0.952608    -0.811737          2.0    -1.088905   \n",
       "3     1.261414     1.529433    -0.254015          2.0    -1.789094   \n",
       "4    -0.190920     1.617828    -0.154894          2.0    -0.685085   \n",
       "\n",
       "   feature_272  feature_273  feature_274  feature_275  feature_276  \\\n",
       "0    -0.502051     0.417777    -0.049380     0.628792     0.131042   \n",
       "1     0.611051     1.188689     0.658617     0.813165    -0.157380   \n",
       "2     2.420187     0.603403    -0.409400    -1.033804    -0.497707   \n",
       "3     0.956844     0.216232     0.032982    -0.186666    -0.365005   \n",
       "4    -0.259366     1.761386     0.279956     0.305717    -0.545458   \n",
       "\n",
       "   feature_277  feature_278  feature_279  feature_280  feature_281  \\\n",
       "0     0.889402    -0.808134    -1.017807    -1.734843     0.779995   \n",
       "1    -0.413281     0.231968     1.098548     0.290310     0.878726   \n",
       "2     0.540116     0.454637    -1.555673    -0.057831     0.003188   \n",
       "3     1.492591    -0.108873     1.286348    -1.481087     0.313496   \n",
       "4    -0.472276    -0.964219    -0.144359    -1.157257     0.193033   \n",
       "\n",
       "   feature_282  feature_283  feature_284  feature_285  feature_286  \\\n",
       "0    -0.191947    -2.240253    -1.648280     0.832526    -0.863522   \n",
       "1     1.440409     0.021213     0.025495    -0.014004     0.278497   \n",
       "2     1.418235    -0.716175    -0.044501    -0.754588    -0.083395   \n",
       "3    -0.316448     0.280948     0.020332    -0.633695    -0.688688   \n",
       "4    -0.512118    -0.495367     0.333013    -0.900259    -0.366629   \n",
       "\n",
       "   feature_287  feature_288  feature_289  feature_290  feature_291  \\\n",
       "0     1.073144     0.097575    -0.315062     0.102598     0.940893   \n",
       "1    -0.358644     1.045162    -0.471805     0.570654     1.306575   \n",
       "2    -1.596949    -0.308550    -1.301425    -0.352957    -0.340272   \n",
       "3     0.287630    -0.843110     0.439304    -0.593944     0.321680   \n",
       "4    -0.087985     0.155583     0.533653    -0.713744    -0.698580   \n",
       "\n",
       "   feature_292  feature_293  feature_294  feature_295  feature_296  \\\n",
       "0     0.205870     1.392481    -1.540102    -1.399463     0.590862   \n",
       "1    -0.986882    -0.563621    -1.653276    -0.553522     0.445039   \n",
       "2    -0.252075    -0.209940    -0.677107     1.927504     0.918098   \n",
       "3    -0.418930    -1.988406    -0.982914    -0.567896     2.563451   \n",
       "4    -0.417718     0.221396    -0.458552    -0.229346     0.275596   \n",
       "\n",
       "   feature_297  feature_298  feature_299  feature_300  feature_301  \\\n",
       "0     0.684389     2.133173     0.733445    -0.293679    -0.308988   \n",
       "1    -0.195654     2.699615    -1.436694     1.303265     1.596970   \n",
       "2    -0.536446     1.185014    -0.202943    -0.513509     1.004317   \n",
       "3    -0.848526     0.630245    -1.493071    -1.043248     0.742885   \n",
       "4    -1.555731     0.560419    -0.163745    -2.320376     1.546314   \n",
       "\n",
       "   feature_302  feature_303  feature_304  feature_305  feature_306  \\\n",
       "0     0.455850    -1.116855    -0.460973    -0.656475     0.297682   \n",
       "1     0.773270     0.163572     0.156204    -0.443805    -0.440397   \n",
       "2    -1.958133    -2.373041    -0.310544    -0.612823    -1.657067   \n",
       "3    -0.634762    -0.467598    -0.618751     0.155680    -1.313546   \n",
       "4    -1.162547     0.338974     0.082602    -2.248687     0.690273   \n",
       "\n",
       "   feature_307  feature_308  feature_309  feature_310  feature_311  \\\n",
       "0     0.608713     1.041652     1.194602    -1.496211     0.324029   \n",
       "1    -0.399911     1.042692    -0.109034     1.995238    -0.444583   \n",
       "2    -1.051108    -0.895319    -1.447215    -1.127566     0.156201   \n",
       "3    -0.366342     0.084393    -0.833920     0.844044     1.136212   \n",
       "4    -0.490256    -1.269430    -0.213861    -0.092103    -0.962888   \n",
       "\n",
       "   feature_312  feature_313  feature_314  feature_315  feature_316  \\\n",
       "0          0.0     0.395821     0.023010    -0.746128    -0.946506   \n",
       "1          0.0     0.135495    -1.378791    -0.061418     1.656025   \n",
       "2          0.0    -1.665356     1.605058    -0.256115    -0.051711   \n",
       "3          0.0     0.784385    -1.881721    -1.626815     0.712773   \n",
       "4          0.0    -0.182431    -0.088560     0.497103     0.055780   \n",
       "\n",
       "   feature_317  feature_318  feature_319  feature_320  feature_321  \\\n",
       "0     0.089546     1.170421    -1.001922     0.783603    -1.108285   \n",
       "1     0.739492     1.452635     0.279078     0.722986    -0.136428   \n",
       "2    -0.754819     1.208956    -0.031901     0.325796    -0.466307   \n",
       "3     0.526955    -0.895785     1.052966    -0.187201    -0.012515   \n",
       "4     1.111765    -0.470018    -0.815203    -2.134086     0.112046   \n",
       "\n",
       "   feature_322  feature_323  feature_324  feature_325  feature_326  \\\n",
       "0     1.595645     0.058773     0.135664     0.519431    -0.777828   \n",
       "1     0.523149    -0.845223    -1.158081     0.406282    -1.034597   \n",
       "2     0.142175     0.723891    -0.424735     1.046035     0.858532   \n",
       "3     0.194762     0.537676    -2.494095    -0.771251    -1.169059   \n",
       "4     1.179066     0.149388    -0.592423    -0.931485     0.071299   \n",
       "\n",
       "   feature_327  feature_328  feature_329  feature_330  feature_331  \\\n",
       "0    -1.625396     0.649841     0.603827          3.0     0.475597   \n",
       "1    -0.669878    -0.606321     0.879827          3.0    -1.273021   \n",
       "2    -0.873258    -0.665298    -0.584091          3.0     0.592800   \n",
       "3     0.515979    -0.050918    -1.865379          3.0    -0.032108   \n",
       "4    -1.226630     0.006482    -0.262121          3.0    -0.050555   \n",
       "\n",
       "   feature_332  feature_333  feature_334  feature_335  feature_336  \\\n",
       "0    -0.893660    -0.391745     1.519931    -2.042217     0.523029   \n",
       "1    -1.088210    -0.574266    -0.619914     0.741997     0.836686   \n",
       "2    -1.676850    -0.461425    -1.159953    -0.580410    -1.415054   \n",
       "3    -0.066884    -0.370438    -0.157417     1.832491    -1.158138   \n",
       "4    -0.299566    -1.539228     0.293038     1.659428    -0.334744   \n",
       "\n",
       "   feature_337  feature_338  feature_339  feature_340  feature_341  \\\n",
       "0     0.535054    -1.253453          6.0     0.725165          4.0   \n",
       "1     0.076717    -1.598974          1.0    -0.261833          4.0   \n",
       "2    -0.627873     0.361897          2.0    -0.774056          4.0   \n",
       "3    -2.652128    -0.212855          3.0    -0.453908          4.0   \n",
       "4    -1.186351    -1.496472          1.0    -1.398077          4.0   \n",
       "\n",
       "   feature_342  feature_343  feature_344  feature_345  feature_346  \\\n",
       "0          0.0     0.526164     0.107756     0.314828     0.210243   \n",
       "1          0.0    -1.244250     1.701270     0.364853     0.040682   \n",
       "2          1.0    -1.138190    -1.648837     0.424413    -0.954391   \n",
       "3          1.0     1.772282     0.420642     0.600302     0.046633   \n",
       "4          0.0     1.195579     0.628814     0.694344     0.440065   \n",
       "\n",
       "   feature_347  feature_348  feature_349  feature_350  feature_351  \\\n",
       "0     1.353127    -0.417028    -0.670278    -0.205779     0.778304   \n",
       "1     0.103482     1.296307     0.606847    -0.368300    -0.320346   \n",
       "2    -1.012680     0.353126    -1.399300     0.458409    -0.556601   \n",
       "3     0.279207    -0.230717     0.548372     0.220223     1.184463   \n",
       "4    -0.579346    -0.360595    -0.753811     0.265414     0.763227   \n",
       "\n",
       "   feature_352  feature_353  feature_354  feature_355  feature_356  \\\n",
       "0    -0.273153     1.047151     1.080877    -0.974841    -0.216138   \n",
       "1    -1.010868     1.557501    -0.158664    -0.444257     0.048692   \n",
       "2    -0.315339    -1.776895    -0.913342     1.007887    -1.124069   \n",
       "3    -0.371669    -0.576668     1.232531     2.176430     0.429022   \n",
       "4    -0.040894     0.231637    -0.415793    -0.609442    -0.467309   \n",
       "\n",
       "   feature_357  feature_358  feature_359  feature_360  feature_361  \\\n",
       "0    -0.952752    -0.495662    -1.058111     0.894230    -0.591453   \n",
       "1    -0.463989     0.038802    -0.284713    -1.549435     0.240735   \n",
       "2     1.613377     0.365286     0.369305     1.112181    -2.941156   \n",
       "3    -1.381603    -0.043974    -1.453867    -0.804753     1.498339   \n",
       "4     1.639633    -0.574299    -0.793408     3.043426    -1.766276   \n",
       "\n",
       "   feature_362  feature_363  feature_364  feature_365  feature_366  \\\n",
       "0    -1.216669     0.233834    -0.287891    -0.348543     0.775076   \n",
       "1    -0.407025     0.020774    -0.304289     1.521847    -0.453642   \n",
       "2    -0.313106    -1.044244    -0.005333    -0.792533     0.512796   \n",
       "3    -0.518434    -1.499389     1.286111     1.112461     1.633326   \n",
       "4    -1.974434    -0.406092     0.251213    -0.969566    -0.526810   \n",
       "\n",
       "   feature_367  feature_368  feature_369  feature_370  feature_371  \\\n",
       "0     0.004447    -0.128613    -0.021720     0.269496     0.628305   \n",
       "1     0.312443    -1.128488     2.227771     0.250663     1.676488   \n",
       "2    -0.435759    -1.167133     1.488831     1.831155    -0.076525   \n",
       "3     1.271142    -1.772782     0.026696     0.859070    -0.606828   \n",
       "4    -0.224624    -0.295939    -0.825563     0.061085    -1.205872   \n",
       "\n",
       "   feature_372  feature_373  feature_374  feature_375  feature_376  \\\n",
       "0          4.0    -0.937712     1.164680          0.0    -0.322748   \n",
       "1          4.0     0.666902    -0.423972          0.0    -0.665466   \n",
       "2          4.0    -0.198818    -0.520668          0.0    -0.509424   \n",
       "3          4.0     0.340881    -0.036385          0.0    -0.387263   \n",
       "4          4.0    -1.018431     0.635342          0.0    -0.123141   \n",
       "\n",
       "   feature_377  feature_378  feature_379  feature_380  feature_381  \\\n",
       "0     0.764989     0.170051     0.356889     0.781627    -0.185540   \n",
       "1     1.342203    -0.385267     0.467601     0.260087    -0.722288   \n",
       "2     0.335270    -0.252650    -1.162685     1.506409    -1.273188   \n",
       "3     1.151252     1.419056     0.638762     2.045954    -0.640056   \n",
       "4    -0.196775     0.513455     0.378646    -0.197650    -0.024398   \n",
       "\n",
       "   feature_382  feature_383  feature_384  feature_385  feature_386  \\\n",
       "0    -1.686242          0.0    -0.572875    -0.766449     0.621146   \n",
       "1    -0.426859          0.0    -2.125879     0.068354     0.537780   \n",
       "2    -0.205559          0.0    -0.363512     1.864165    -0.357862   \n",
       "3    -0.604970          0.0     0.226103     1.391619     0.223486   \n",
       "4     0.052106          0.0    -0.611166    -1.361757    -0.871222   \n",
       "\n",
       "   feature_387  feature_388  feature_389  feature_390  feature_391  \\\n",
       "0     1.145572    -0.474260     0.498067     1.668021    -0.760603   \n",
       "1     0.128863    -0.185917     0.016297    -0.612005    -0.543899   \n",
       "2    -0.881552     0.597269     0.504736     1.306143     0.894581   \n",
       "3     0.252886    -1.774845    -0.652228    -0.055324     1.296795   \n",
       "4    -0.640538    -1.095264     1.048687    -0.795837    -0.123556   \n",
       "\n",
       "   feature_392  feature_393  feature_394  feature_395  feature_396  \\\n",
       "0     0.798047    -0.953440     0.180746     0.121726          3.0   \n",
       "1     0.580927    -1.110941    -0.042332    -0.187147          3.0   \n",
       "2    -0.485608     0.151862     0.204297     0.343261          3.0   \n",
       "3     0.738213    -0.747756     0.493076    -0.311358          3.0   \n",
       "4     0.110427    -1.161328     1.770563    -0.480965          3.0   \n",
       "\n",
       "   feature_397  feature_398  feature_399  feature_400  feature_401  \\\n",
       "0     0.822355     0.666325    -0.758953     0.503469     0.291400   \n",
       "1    -1.380817     1.096997    -1.333537     0.398855     0.634238   \n",
       "2    -0.656024     0.338950     0.350464    -0.080056     0.216867   \n",
       "3    -0.244335     0.080122    -1.109347     1.557400    -0.010520   \n",
       "4    -0.707384     1.766121    -0.140636     0.889687    -0.925039   \n",
       "\n",
       "   feature_402  feature_403  feature_404  feature_405  feature_406  \\\n",
       "0    -1.645741     0.146488    -0.829671    -0.246638     1.078580   \n",
       "1    -0.268676     0.650312    -1.853633    -0.129410     0.723021   \n",
       "2     0.579273     1.189545     1.188373    -1.506164    -0.465339   \n",
       "3    -0.500114     0.778553    -0.821763    -0.575313    -0.344703   \n",
       "4     0.049110    -0.470497    -0.590048    -0.666548    -0.478903   \n",
       "\n",
       "   feature_407  feature_408  feature_409  feature_410  feature_411  \\\n",
       "0    -0.108352    -0.032756    -0.361507    -1.026853          0.0   \n",
       "1    -0.784384    -0.122721    -0.546275    -1.489542          0.0   \n",
       "2    -0.784669     0.675667    -0.485999     0.586012          0.0   \n",
       "3    -0.810880    -0.007987     0.321985    -0.075827          0.0   \n",
       "4    -1.696438     0.932974     0.828886     0.140387          0.0   \n",
       "\n",
       "   feature_412  feature_413  feature_414  feature_415  feature_416  \\\n",
       "0     1.418600    -0.929668     1.284014     0.731842     0.801786   \n",
       "1    -0.622007    -0.473156     0.780020     0.648577     0.646100   \n",
       "2     0.361481    -0.364566    -1.318596    -0.385155     0.140133   \n",
       "3    -1.629672     0.876864     0.411271     0.433440     0.997364   \n",
       "4    -0.624304    -2.197691    -1.479267    -0.465917    -0.014757   \n",
       "\n",
       "   feature_417  feature_418  \n",
       "0    -0.728297    -0.412095  \n",
       "1    -0.789362     0.083349  \n",
       "2     0.123245    -0.670030  \n",
       "3     2.829590    -1.275588  \n",
       "4    -0.320434    -0.511896  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e5ae93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:31:33.978772Z",
     "iopub.status.busy": "2024-11-01T18:31:33.978413Z",
     "iopub.status.idle": "2024-11-01T18:31:34.029047Z",
     "shell.execute_reply": "2024-11-01T18:31:34.028006Z"
    },
    "papermill": {
     "duration": 0.063603,
     "end_time": "2024-11-01T18:31:34.031104",
     "exception": false,
     "start_time": "2024-11-01T18:31:33.967501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38731 entries, 0 to 38730\n",
      "Columns: 421 entries, target to feature_418\n",
      "dtypes: float64(418), int64(2), object(1)\n",
      "memory usage: 124.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a73e17b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:31:34.052417Z",
     "iopub.status.busy": "2024-11-01T18:31:34.052081Z",
     "iopub.status.idle": "2024-11-01T18:31:35.601120Z",
     "shell.execute_reply": "2024-11-01T18:31:35.600206Z"
    },
    "papermill": {
     "duration": 1.566074,
     "end_time": "2024-11-01T18:31:35.607503",
     "exception": false,
     "start_time": "2024-11-01T18:31:34.041429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "      <th>feature_31</th>\n",
       "      <th>feature_32</th>\n",
       "      <th>feature_33</th>\n",
       "      <th>feature_34</th>\n",
       "      <th>feature_35</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "      <th>feature_50</th>\n",
       "      <th>feature_51</th>\n",
       "      <th>feature_52</th>\n",
       "      <th>feature_53</th>\n",
       "      <th>feature_54</th>\n",
       "      <th>feature_55</th>\n",
       "      <th>feature_56</th>\n",
       "      <th>feature_57</th>\n",
       "      <th>feature_58</th>\n",
       "      <th>feature_59</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_61</th>\n",
       "      <th>feature_62</th>\n",
       "      <th>feature_63</th>\n",
       "      <th>feature_64</th>\n",
       "      <th>feature_65</th>\n",
       "      <th>feature_66</th>\n",
       "      <th>feature_67</th>\n",
       "      <th>feature_68</th>\n",
       "      <th>feature_69</th>\n",
       "      <th>feature_70</th>\n",
       "      <th>feature_71</th>\n",
       "      <th>feature_72</th>\n",
       "      <th>feature_73</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>feature_75</th>\n",
       "      <th>feature_76</th>\n",
       "      <th>feature_77</th>\n",
       "      <th>feature_78</th>\n",
       "      <th>feature_79</th>\n",
       "      <th>feature_80</th>\n",
       "      <th>feature_81</th>\n",
       "      <th>feature_82</th>\n",
       "      <th>feature_83</th>\n",
       "      <th>feature_84</th>\n",
       "      <th>feature_85</th>\n",
       "      <th>feature_86</th>\n",
       "      <th>feature_87</th>\n",
       "      <th>feature_88</th>\n",
       "      <th>feature_89</th>\n",
       "      <th>feature_90</th>\n",
       "      <th>feature_91</th>\n",
       "      <th>feature_92</th>\n",
       "      <th>feature_93</th>\n",
       "      <th>feature_94</th>\n",
       "      <th>feature_95</th>\n",
       "      <th>feature_96</th>\n",
       "      <th>feature_97</th>\n",
       "      <th>feature_98</th>\n",
       "      <th>feature_99</th>\n",
       "      <th>feature_100</th>\n",
       "      <th>feature_101</th>\n",
       "      <th>feature_102</th>\n",
       "      <th>feature_103</th>\n",
       "      <th>feature_104</th>\n",
       "      <th>feature_105</th>\n",
       "      <th>feature_106</th>\n",
       "      <th>feature_107</th>\n",
       "      <th>feature_108</th>\n",
       "      <th>feature_109</th>\n",
       "      <th>feature_110</th>\n",
       "      <th>feature_111</th>\n",
       "      <th>feature_112</th>\n",
       "      <th>feature_113</th>\n",
       "      <th>feature_114</th>\n",
       "      <th>feature_115</th>\n",
       "      <th>feature_116</th>\n",
       "      <th>feature_117</th>\n",
       "      <th>feature_118</th>\n",
       "      <th>feature_119</th>\n",
       "      <th>feature_120</th>\n",
       "      <th>feature_121</th>\n",
       "      <th>feature_122</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_124</th>\n",
       "      <th>feature_125</th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "      <th>feature_128</th>\n",
       "      <th>feature_129</th>\n",
       "      <th>feature_130</th>\n",
       "      <th>feature_131</th>\n",
       "      <th>feature_132</th>\n",
       "      <th>feature_133</th>\n",
       "      <th>feature_134</th>\n",
       "      <th>feature_135</th>\n",
       "      <th>feature_136</th>\n",
       "      <th>feature_137</th>\n",
       "      <th>feature_138</th>\n",
       "      <th>feature_139</th>\n",
       "      <th>feature_140</th>\n",
       "      <th>feature_141</th>\n",
       "      <th>feature_142</th>\n",
       "      <th>feature_143</th>\n",
       "      <th>feature_144</th>\n",
       "      <th>feature_145</th>\n",
       "      <th>feature_146</th>\n",
       "      <th>feature_147</th>\n",
       "      <th>feature_148</th>\n",
       "      <th>feature_149</th>\n",
       "      <th>feature_150</th>\n",
       "      <th>feature_151</th>\n",
       "      <th>feature_152</th>\n",
       "      <th>feature_153</th>\n",
       "      <th>feature_154</th>\n",
       "      <th>feature_155</th>\n",
       "      <th>feature_156</th>\n",
       "      <th>feature_157</th>\n",
       "      <th>feature_158</th>\n",
       "      <th>feature_159</th>\n",
       "      <th>feature_160</th>\n",
       "      <th>feature_161</th>\n",
       "      <th>feature_162</th>\n",
       "      <th>feature_163</th>\n",
       "      <th>feature_164</th>\n",
       "      <th>feature_165</th>\n",
       "      <th>feature_166</th>\n",
       "      <th>feature_167</th>\n",
       "      <th>feature_168</th>\n",
       "      <th>feature_169</th>\n",
       "      <th>feature_170</th>\n",
       "      <th>feature_171</th>\n",
       "      <th>feature_172</th>\n",
       "      <th>feature_173</th>\n",
       "      <th>feature_174</th>\n",
       "      <th>feature_175</th>\n",
       "      <th>feature_176</th>\n",
       "      <th>feature_177</th>\n",
       "      <th>feature_178</th>\n",
       "      <th>feature_179</th>\n",
       "      <th>feature_180</th>\n",
       "      <th>feature_181</th>\n",
       "      <th>feature_182</th>\n",
       "      <th>feature_183</th>\n",
       "      <th>feature_184</th>\n",
       "      <th>feature_185</th>\n",
       "      <th>feature_186</th>\n",
       "      <th>feature_187</th>\n",
       "      <th>feature_188</th>\n",
       "      <th>feature_189</th>\n",
       "      <th>feature_190</th>\n",
       "      <th>feature_191</th>\n",
       "      <th>feature_192</th>\n",
       "      <th>feature_193</th>\n",
       "      <th>feature_194</th>\n",
       "      <th>feature_195</th>\n",
       "      <th>feature_196</th>\n",
       "      <th>feature_197</th>\n",
       "      <th>feature_198</th>\n",
       "      <th>feature_199</th>\n",
       "      <th>feature_200</th>\n",
       "      <th>feature_201</th>\n",
       "      <th>feature_202</th>\n",
       "      <th>feature_203</th>\n",
       "      <th>feature_204</th>\n",
       "      <th>feature_205</th>\n",
       "      <th>feature_206</th>\n",
       "      <th>feature_207</th>\n",
       "      <th>feature_208</th>\n",
       "      <th>feature_209</th>\n",
       "      <th>feature_210</th>\n",
       "      <th>feature_211</th>\n",
       "      <th>feature_212</th>\n",
       "      <th>feature_213</th>\n",
       "      <th>feature_214</th>\n",
       "      <th>feature_215</th>\n",
       "      <th>feature_216</th>\n",
       "      <th>feature_217</th>\n",
       "      <th>feature_218</th>\n",
       "      <th>feature_219</th>\n",
       "      <th>feature_220</th>\n",
       "      <th>feature_221</th>\n",
       "      <th>feature_222</th>\n",
       "      <th>feature_223</th>\n",
       "      <th>feature_224</th>\n",
       "      <th>feature_225</th>\n",
       "      <th>feature_226</th>\n",
       "      <th>feature_227</th>\n",
       "      <th>feature_228</th>\n",
       "      <th>feature_229</th>\n",
       "      <th>feature_230</th>\n",
       "      <th>feature_231</th>\n",
       "      <th>feature_232</th>\n",
       "      <th>feature_233</th>\n",
       "      <th>feature_234</th>\n",
       "      <th>feature_235</th>\n",
       "      <th>feature_236</th>\n",
       "      <th>feature_237</th>\n",
       "      <th>feature_238</th>\n",
       "      <th>feature_239</th>\n",
       "      <th>feature_240</th>\n",
       "      <th>feature_241</th>\n",
       "      <th>feature_242</th>\n",
       "      <th>feature_243</th>\n",
       "      <th>feature_244</th>\n",
       "      <th>feature_245</th>\n",
       "      <th>feature_246</th>\n",
       "      <th>feature_247</th>\n",
       "      <th>feature_248</th>\n",
       "      <th>feature_249</th>\n",
       "      <th>feature_250</th>\n",
       "      <th>feature_251</th>\n",
       "      <th>feature_252</th>\n",
       "      <th>feature_253</th>\n",
       "      <th>feature_254</th>\n",
       "      <th>feature_255</th>\n",
       "      <th>feature_256</th>\n",
       "      <th>feature_257</th>\n",
       "      <th>feature_258</th>\n",
       "      <th>feature_259</th>\n",
       "      <th>feature_260</th>\n",
       "      <th>feature_261</th>\n",
       "      <th>feature_262</th>\n",
       "      <th>feature_263</th>\n",
       "      <th>feature_264</th>\n",
       "      <th>feature_265</th>\n",
       "      <th>feature_266</th>\n",
       "      <th>feature_267</th>\n",
       "      <th>feature_268</th>\n",
       "      <th>feature_269</th>\n",
       "      <th>feature_270</th>\n",
       "      <th>feature_271</th>\n",
       "      <th>feature_272</th>\n",
       "      <th>feature_273</th>\n",
       "      <th>feature_274</th>\n",
       "      <th>feature_275</th>\n",
       "      <th>feature_276</th>\n",
       "      <th>feature_277</th>\n",
       "      <th>feature_278</th>\n",
       "      <th>feature_279</th>\n",
       "      <th>feature_280</th>\n",
       "      <th>feature_281</th>\n",
       "      <th>feature_282</th>\n",
       "      <th>feature_283</th>\n",
       "      <th>feature_284</th>\n",
       "      <th>feature_285</th>\n",
       "      <th>feature_286</th>\n",
       "      <th>feature_287</th>\n",
       "      <th>feature_288</th>\n",
       "      <th>feature_289</th>\n",
       "      <th>feature_290</th>\n",
       "      <th>feature_291</th>\n",
       "      <th>feature_292</th>\n",
       "      <th>feature_293</th>\n",
       "      <th>feature_294</th>\n",
       "      <th>feature_295</th>\n",
       "      <th>feature_296</th>\n",
       "      <th>feature_297</th>\n",
       "      <th>feature_298</th>\n",
       "      <th>feature_299</th>\n",
       "      <th>feature_300</th>\n",
       "      <th>feature_301</th>\n",
       "      <th>feature_302</th>\n",
       "      <th>feature_303</th>\n",
       "      <th>feature_304</th>\n",
       "      <th>feature_305</th>\n",
       "      <th>feature_306</th>\n",
       "      <th>feature_307</th>\n",
       "      <th>feature_308</th>\n",
       "      <th>feature_309</th>\n",
       "      <th>feature_310</th>\n",
       "      <th>feature_311</th>\n",
       "      <th>feature_312</th>\n",
       "      <th>feature_313</th>\n",
       "      <th>feature_314</th>\n",
       "      <th>feature_315</th>\n",
       "      <th>feature_316</th>\n",
       "      <th>feature_317</th>\n",
       "      <th>feature_318</th>\n",
       "      <th>feature_319</th>\n",
       "      <th>feature_320</th>\n",
       "      <th>feature_321</th>\n",
       "      <th>feature_322</th>\n",
       "      <th>feature_323</th>\n",
       "      <th>feature_324</th>\n",
       "      <th>feature_325</th>\n",
       "      <th>feature_326</th>\n",
       "      <th>feature_327</th>\n",
       "      <th>feature_328</th>\n",
       "      <th>feature_329</th>\n",
       "      <th>feature_330</th>\n",
       "      <th>feature_331</th>\n",
       "      <th>feature_332</th>\n",
       "      <th>feature_333</th>\n",
       "      <th>feature_334</th>\n",
       "      <th>feature_335</th>\n",
       "      <th>feature_336</th>\n",
       "      <th>feature_337</th>\n",
       "      <th>feature_338</th>\n",
       "      <th>feature_339</th>\n",
       "      <th>feature_340</th>\n",
       "      <th>feature_341</th>\n",
       "      <th>feature_342</th>\n",
       "      <th>feature_343</th>\n",
       "      <th>feature_344</th>\n",
       "      <th>feature_345</th>\n",
       "      <th>feature_346</th>\n",
       "      <th>feature_347</th>\n",
       "      <th>feature_348</th>\n",
       "      <th>feature_349</th>\n",
       "      <th>feature_350</th>\n",
       "      <th>feature_351</th>\n",
       "      <th>feature_352</th>\n",
       "      <th>feature_353</th>\n",
       "      <th>feature_354</th>\n",
       "      <th>feature_355</th>\n",
       "      <th>feature_356</th>\n",
       "      <th>feature_357</th>\n",
       "      <th>feature_358</th>\n",
       "      <th>feature_359</th>\n",
       "      <th>feature_360</th>\n",
       "      <th>feature_361</th>\n",
       "      <th>feature_362</th>\n",
       "      <th>feature_363</th>\n",
       "      <th>feature_364</th>\n",
       "      <th>feature_365</th>\n",
       "      <th>feature_366</th>\n",
       "      <th>feature_367</th>\n",
       "      <th>feature_368</th>\n",
       "      <th>feature_369</th>\n",
       "      <th>feature_370</th>\n",
       "      <th>feature_371</th>\n",
       "      <th>feature_372</th>\n",
       "      <th>feature_373</th>\n",
       "      <th>feature_374</th>\n",
       "      <th>feature_375</th>\n",
       "      <th>feature_376</th>\n",
       "      <th>feature_377</th>\n",
       "      <th>feature_378</th>\n",
       "      <th>feature_379</th>\n",
       "      <th>feature_380</th>\n",
       "      <th>feature_381</th>\n",
       "      <th>feature_382</th>\n",
       "      <th>feature_383</th>\n",
       "      <th>feature_384</th>\n",
       "      <th>feature_385</th>\n",
       "      <th>feature_386</th>\n",
       "      <th>feature_387</th>\n",
       "      <th>feature_388</th>\n",
       "      <th>feature_389</th>\n",
       "      <th>feature_390</th>\n",
       "      <th>feature_391</th>\n",
       "      <th>feature_392</th>\n",
       "      <th>feature_393</th>\n",
       "      <th>feature_394</th>\n",
       "      <th>feature_395</th>\n",
       "      <th>feature_396</th>\n",
       "      <th>feature_397</th>\n",
       "      <th>feature_398</th>\n",
       "      <th>feature_399</th>\n",
       "      <th>feature_400</th>\n",
       "      <th>feature_401</th>\n",
       "      <th>feature_402</th>\n",
       "      <th>feature_403</th>\n",
       "      <th>feature_404</th>\n",
       "      <th>feature_405</th>\n",
       "      <th>feature_406</th>\n",
       "      <th>feature_407</th>\n",
       "      <th>feature_408</th>\n",
       "      <th>feature_409</th>\n",
       "      <th>feature_410</th>\n",
       "      <th>feature_411</th>\n",
       "      <th>feature_412</th>\n",
       "      <th>feature_413</th>\n",
       "      <th>feature_414</th>\n",
       "      <th>feature_415</th>\n",
       "      <th>feature_416</th>\n",
       "      <th>feature_417</th>\n",
       "      <th>feature_418</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "      <td>38731.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.021766</td>\n",
       "      <td>19365.000000</td>\n",
       "      <td>-0.011868</td>\n",
       "      <td>-0.022720</td>\n",
       "      <td>0.240066</td>\n",
       "      <td>0.014769</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>-0.121256</td>\n",
       "      <td>-0.021874</td>\n",
       "      <td>-0.212568</td>\n",
       "      <td>-0.030707</td>\n",
       "      <td>0.088301</td>\n",
       "      <td>-0.010807</td>\n",
       "      <td>0.176861</td>\n",
       "      <td>-0.012932</td>\n",
       "      <td>0.124629</td>\n",
       "      <td>0.195528</td>\n",
       "      <td>-0.030446</td>\n",
       "      <td>3.549766</td>\n",
       "      <td>-0.037873</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>0.864759</td>\n",
       "      <td>0.145517</td>\n",
       "      <td>-0.080829</td>\n",
       "      <td>3.539645</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.007249</td>\n",
       "      <td>-0.327361</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>-0.007191</td>\n",
       "      <td>0.014676</td>\n",
       "      <td>-0.127118</td>\n",
       "      <td>-0.001103</td>\n",
       "      <td>-0.043747</td>\n",
       "      <td>-0.016969</td>\n",
       "      <td>-0.145650</td>\n",
       "      <td>-0.003836</td>\n",
       "      <td>-0.173456</td>\n",
       "      <td>-0.010436</td>\n",
       "      <td>-0.023865</td>\n",
       "      <td>-0.013785</td>\n",
       "      <td>-0.136651</td>\n",
       "      <td>-0.006490</td>\n",
       "      <td>-0.222637</td>\n",
       "      <td>0.004805</td>\n",
       "      <td>0.013576</td>\n",
       "      <td>-0.001399</td>\n",
       "      <td>-0.001891</td>\n",
       "      <td>-0.273817</td>\n",
       "      <td>0.004304</td>\n",
       "      <td>0.017616</td>\n",
       "      <td>0.003360</td>\n",
       "      <td>-0.048813</td>\n",
       "      <td>-0.017145</td>\n",
       "      <td>-0.013058</td>\n",
       "      <td>0.228990</td>\n",
       "      <td>-0.186734</td>\n",
       "      <td>-0.017106</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>-0.029327</td>\n",
       "      <td>-0.001139</td>\n",
       "      <td>0.442514</td>\n",
       "      <td>-0.024042</td>\n",
       "      <td>-0.105719</td>\n",
       "      <td>-0.021940</td>\n",
       "      <td>0.027399</td>\n",
       "      <td>-0.011325</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>-0.031736</td>\n",
       "      <td>-0.033972</td>\n",
       "      <td>-0.010006</td>\n",
       "      <td>0.421704</td>\n",
       "      <td>-0.021387</td>\n",
       "      <td>0.005190</td>\n",
       "      <td>0.016982</td>\n",
       "      <td>-0.001517</td>\n",
       "      <td>0.020074</td>\n",
       "      <td>-0.004565</td>\n",
       "      <td>-0.011091</td>\n",
       "      <td>-0.082769</td>\n",
       "      <td>0.009481</td>\n",
       "      <td>0.003194</td>\n",
       "      <td>-0.051269</td>\n",
       "      <td>-0.134871</td>\n",
       "      <td>-0.148081</td>\n",
       "      <td>-0.035728</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>-0.003726</td>\n",
       "      <td>-0.003361</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>-0.017810</td>\n",
       "      <td>-0.049131</td>\n",
       "      <td>5.446774</td>\n",
       "      <td>-0.002723</td>\n",
       "      <td>0.007040</td>\n",
       "      <td>-0.032440</td>\n",
       "      <td>-0.005845</td>\n",
       "      <td>-0.016907</td>\n",
       "      <td>0.718133</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>0.009108</td>\n",
       "      <td>2.498748</td>\n",
       "      <td>-0.013882</td>\n",
       "      <td>-0.011444</td>\n",
       "      <td>-0.037351</td>\n",
       "      <td>-0.010772</td>\n",
       "      <td>-0.022142</td>\n",
       "      <td>0.012548</td>\n",
       "      <td>0.008680</td>\n",
       "      <td>0.003365</td>\n",
       "      <td>0.019836</td>\n",
       "      <td>-0.068095</td>\n",
       "      <td>-0.004531</td>\n",
       "      <td>-0.049030</td>\n",
       "      <td>-0.024292</td>\n",
       "      <td>-0.052030</td>\n",
       "      <td>-0.015779</td>\n",
       "      <td>-0.006686</td>\n",
       "      <td>-0.390342</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>-0.030654</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>-0.024875</td>\n",
       "      <td>0.027182</td>\n",
       "      <td>-0.335467</td>\n",
       "      <td>3.548527</td>\n",
       "      <td>-0.121321</td>\n",
       "      <td>-0.004478</td>\n",
       "      <td>-0.036363</td>\n",
       "      <td>0.004969</td>\n",
       "      <td>-0.021047</td>\n",
       "      <td>-0.047094</td>\n",
       "      <td>-0.067346</td>\n",
       "      <td>-0.069472</td>\n",
       "      <td>6.594149</td>\n",
       "      <td>-0.061260</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>-0.008483</td>\n",
       "      <td>-0.094022</td>\n",
       "      <td>0.014289</td>\n",
       "      <td>0.096073</td>\n",
       "      <td>-0.011467</td>\n",
       "      <td>-0.017763</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>-0.004232</td>\n",
       "      <td>-0.010737</td>\n",
       "      <td>-0.057190</td>\n",
       "      <td>-0.005645</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>-0.038901</td>\n",
       "      <td>-0.003432</td>\n",
       "      <td>0.626604</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>-0.101662</td>\n",
       "      <td>-0.047708</td>\n",
       "      <td>-0.018177</td>\n",
       "      <td>-0.008492</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>-0.015921</td>\n",
       "      <td>-0.003541</td>\n",
       "      <td>-0.357884</td>\n",
       "      <td>-0.050297</td>\n",
       "      <td>0.017768</td>\n",
       "      <td>-0.083280</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>-0.027420</td>\n",
       "      <td>-0.010116</td>\n",
       "      <td>-0.012896</td>\n",
       "      <td>-0.022193</td>\n",
       "      <td>-0.029590</td>\n",
       "      <td>-0.023098</td>\n",
       "      <td>-0.004230</td>\n",
       "      <td>-0.002590</td>\n",
       "      <td>-0.047705</td>\n",
       "      <td>-0.137179</td>\n",
       "      <td>-0.003686</td>\n",
       "      <td>-0.027732</td>\n",
       "      <td>0.507526</td>\n",
       "      <td>-0.009889</td>\n",
       "      <td>-0.001583</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>-0.069000</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>0.060124</td>\n",
       "      <td>-0.002185</td>\n",
       "      <td>0.212904</td>\n",
       "      <td>-0.050094</td>\n",
       "      <td>-0.060483</td>\n",
       "      <td>-0.068219</td>\n",
       "      <td>0.027051</td>\n",
       "      <td>-0.015950</td>\n",
       "      <td>-0.026602</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.003725</td>\n",
       "      <td>-0.351320</td>\n",
       "      <td>-0.130240</td>\n",
       "      <td>0.557409</td>\n",
       "      <td>0.009460</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>-0.020191</td>\n",
       "      <td>-0.004877</td>\n",
       "      <td>-0.040392</td>\n",
       "      <td>-0.115125</td>\n",
       "      <td>-0.221730</td>\n",
       "      <td>-0.035877</td>\n",
       "      <td>-0.005566</td>\n",
       "      <td>-0.001480</td>\n",
       "      <td>0.012166</td>\n",
       "      <td>-0.010566</td>\n",
       "      <td>0.004593</td>\n",
       "      <td>-0.065117</td>\n",
       "      <td>-0.165825</td>\n",
       "      <td>0.016577</td>\n",
       "      <td>-0.027902</td>\n",
       "      <td>-0.004799</td>\n",
       "      <td>-0.006482</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>-0.002309</td>\n",
       "      <td>-0.010962</td>\n",
       "      <td>-0.017173</td>\n",
       "      <td>-0.095189</td>\n",
       "      <td>-0.054226</td>\n",
       "      <td>-0.031292</td>\n",
       "      <td>-0.022812</td>\n",
       "      <td>-0.035788</td>\n",
       "      <td>-0.015487</td>\n",
       "      <td>0.018312</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>-0.038534</td>\n",
       "      <td>-0.114466</td>\n",
       "      <td>-0.001762</td>\n",
       "      <td>-0.043924</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>-0.019787</td>\n",
       "      <td>0.018997</td>\n",
       "      <td>0.014188</td>\n",
       "      <td>-0.015456</td>\n",
       "      <td>-0.015277</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>0.642095</td>\n",
       "      <td>-0.027985</td>\n",
       "      <td>0.037128</td>\n",
       "      <td>-0.035251</td>\n",
       "      <td>-0.050988</td>\n",
       "      <td>-0.354791</td>\n",
       "      <td>-0.036398</td>\n",
       "      <td>-0.129943</td>\n",
       "      <td>-0.070364</td>\n",
       "      <td>-0.020606</td>\n",
       "      <td>-0.141588</td>\n",
       "      <td>-0.003403</td>\n",
       "      <td>-0.066228</td>\n",
       "      <td>0.475123</td>\n",
       "      <td>-0.066319</td>\n",
       "      <td>-0.040016</td>\n",
       "      <td>-0.048797</td>\n",
       "      <td>-0.062881</td>\n",
       "      <td>0.004915</td>\n",
       "      <td>0.006066</td>\n",
       "      <td>0.011512</td>\n",
       "      <td>-0.015177</td>\n",
       "      <td>-0.009057</td>\n",
       "      <td>-0.126460</td>\n",
       "      <td>-0.001341</td>\n",
       "      <td>-0.024550</td>\n",
       "      <td>-0.006658</td>\n",
       "      <td>-0.014437</td>\n",
       "      <td>1.507526</td>\n",
       "      <td>0.003270</td>\n",
       "      <td>0.003211</td>\n",
       "      <td>-0.083650</td>\n",
       "      <td>-0.009141</td>\n",
       "      <td>-0.003054</td>\n",
       "      <td>0.025614</td>\n",
       "      <td>-0.028128</td>\n",
       "      <td>-0.058274</td>\n",
       "      <td>-0.040961</td>\n",
       "      <td>-0.064159</td>\n",
       "      <td>-0.017375</td>\n",
       "      <td>-0.017064</td>\n",
       "      <td>-0.169013</td>\n",
       "      <td>-0.131256</td>\n",
       "      <td>-0.030752</td>\n",
       "      <td>-0.194364</td>\n",
       "      <td>-0.023399</td>\n",
       "      <td>0.004054</td>\n",
       "      <td>-0.076873</td>\n",
       "      <td>-0.011184</td>\n",
       "      <td>-0.039726</td>\n",
       "      <td>-0.037295</td>\n",
       "      <td>-0.019333</td>\n",
       "      <td>-0.009030</td>\n",
       "      <td>-0.011395</td>\n",
       "      <td>-0.038294</td>\n",
       "      <td>-0.025584</td>\n",
       "      <td>0.011510</td>\n",
       "      <td>-0.021345</td>\n",
       "      <td>-0.028281</td>\n",
       "      <td>-0.023140</td>\n",
       "      <td>-0.013921</td>\n",
       "      <td>-0.047297</td>\n",
       "      <td>-0.123179</td>\n",
       "      <td>-0.034168</td>\n",
       "      <td>-0.049474</td>\n",
       "      <td>-0.011567</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>-0.001719</td>\n",
       "      <td>-0.019460</td>\n",
       "      <td>-0.004924</td>\n",
       "      <td>0.115024</td>\n",
       "      <td>0.008211</td>\n",
       "      <td>0.009287</td>\n",
       "      <td>-0.011622</td>\n",
       "      <td>-0.017755</td>\n",
       "      <td>-0.048480</td>\n",
       "      <td>0.005836</td>\n",
       "      <td>0.014995</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>-0.004223</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.028351</td>\n",
       "      <td>0.030415</td>\n",
       "      <td>-0.035701</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>0.014264</td>\n",
       "      <td>-0.057312</td>\n",
       "      <td>0.024448</td>\n",
       "      <td>2.470579</td>\n",
       "      <td>-0.003280</td>\n",
       "      <td>-0.072037</td>\n",
       "      <td>-0.013049</td>\n",
       "      <td>0.005417</td>\n",
       "      <td>0.009747</td>\n",
       "      <td>0.018040</td>\n",
       "      <td>-0.287418</td>\n",
       "      <td>-0.152220</td>\n",
       "      <td>5.282409</td>\n",
       "      <td>-0.015855</td>\n",
       "      <td>3.505280</td>\n",
       "      <td>0.690067</td>\n",
       "      <td>-0.009318</td>\n",
       "      <td>-0.005876</td>\n",
       "      <td>-0.010761</td>\n",
       "      <td>-0.001728</td>\n",
       "      <td>-0.005082</td>\n",
       "      <td>-0.030485</td>\n",
       "      <td>-0.087999</td>\n",
       "      <td>-0.000902</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>-0.170507</td>\n",
       "      <td>0.011570</td>\n",
       "      <td>-0.034570</td>\n",
       "      <td>-0.223282</td>\n",
       "      <td>0.010082</td>\n",
       "      <td>-0.018277</td>\n",
       "      <td>-0.049603</td>\n",
       "      <td>-0.027510</td>\n",
       "      <td>0.005554</td>\n",
       "      <td>-0.028320</td>\n",
       "      <td>-0.119015</td>\n",
       "      <td>0.005350</td>\n",
       "      <td>-0.010099</td>\n",
       "      <td>-0.031220</td>\n",
       "      <td>-0.025373</td>\n",
       "      <td>0.010071</td>\n",
       "      <td>-0.013189</td>\n",
       "      <td>-0.017985</td>\n",
       "      <td>0.011084</td>\n",
       "      <td>-0.003941</td>\n",
       "      <td>3.514110</td>\n",
       "      <td>-0.029191</td>\n",
       "      <td>-0.027324</td>\n",
       "      <td>0.526762</td>\n",
       "      <td>-0.008724</td>\n",
       "      <td>0.004876</td>\n",
       "      <td>-0.008747</td>\n",
       "      <td>0.004826</td>\n",
       "      <td>0.064518</td>\n",
       "      <td>-0.100974</td>\n",
       "      <td>-0.026204</td>\n",
       "      <td>0.221683</td>\n",
       "      <td>0.029237</td>\n",
       "      <td>0.005299</td>\n",
       "      <td>-0.003504</td>\n",
       "      <td>-0.001310</td>\n",
       "      <td>-0.001884</td>\n",
       "      <td>-0.011558</td>\n",
       "      <td>-0.052937</td>\n",
       "      <td>-0.012856</td>\n",
       "      <td>0.008890</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>-0.004060</td>\n",
       "      <td>-0.034970</td>\n",
       "      <td>2.610519</td>\n",
       "      <td>-0.005816</td>\n",
       "      <td>-0.292827</td>\n",
       "      <td>-0.004642</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>-0.002064</td>\n",
       "      <td>-0.057303</td>\n",
       "      <td>-0.014882</td>\n",
       "      <td>-0.002663</td>\n",
       "      <td>-0.002989</td>\n",
       "      <td>-0.053965</td>\n",
       "      <td>-0.115081</td>\n",
       "      <td>-0.020693</td>\n",
       "      <td>0.007855</td>\n",
       "      <td>-0.289571</td>\n",
       "      <td>0.046371</td>\n",
       "      <td>-0.011372</td>\n",
       "      <td>-0.045371</td>\n",
       "      <td>-0.009627</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>-0.019660</td>\n",
       "      <td>-0.007003</td>\n",
       "      <td>-0.026602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.145919</td>\n",
       "      <td>11180.820975</td>\n",
       "      <td>0.992397</td>\n",
       "      <td>0.991483</td>\n",
       "      <td>0.427129</td>\n",
       "      <td>1.006943</td>\n",
       "      <td>1.008098</td>\n",
       "      <td>0.983779</td>\n",
       "      <td>0.979137</td>\n",
       "      <td>0.935391</td>\n",
       "      <td>0.969471</td>\n",
       "      <td>0.369716</td>\n",
       "      <td>0.990354</td>\n",
       "      <td>0.449187</td>\n",
       "      <td>0.996109</td>\n",
       "      <td>0.406550</td>\n",
       "      <td>0.396612</td>\n",
       "      <td>0.985939</td>\n",
       "      <td>0.966210</td>\n",
       "      <td>0.979483</td>\n",
       "      <td>1.002411</td>\n",
       "      <td>0.416098</td>\n",
       "      <td>0.424886</td>\n",
       "      <td>0.993560</td>\n",
       "      <td>0.972032</td>\n",
       "      <td>0.998825</td>\n",
       "      <td>1.003163</td>\n",
       "      <td>0.832599</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>1.002357</td>\n",
       "      <td>1.005803</td>\n",
       "      <td>0.947260</td>\n",
       "      <td>0.997807</td>\n",
       "      <td>0.993147</td>\n",
       "      <td>1.004046</td>\n",
       "      <td>0.975752</td>\n",
       "      <td>1.001335</td>\n",
       "      <td>0.971011</td>\n",
       "      <td>0.989517</td>\n",
       "      <td>0.983823</td>\n",
       "      <td>1.012303</td>\n",
       "      <td>0.973383</td>\n",
       "      <td>0.992355</td>\n",
       "      <td>0.919967</td>\n",
       "      <td>0.996740</td>\n",
       "      <td>1.001245</td>\n",
       "      <td>1.004859</td>\n",
       "      <td>1.000439</td>\n",
       "      <td>0.890477</td>\n",
       "      <td>1.011104</td>\n",
       "      <td>1.017203</td>\n",
       "      <td>1.007769</td>\n",
       "      <td>0.993191</td>\n",
       "      <td>0.985656</td>\n",
       "      <td>0.989085</td>\n",
       "      <td>0.482433</td>\n",
       "      <td>0.961433</td>\n",
       "      <td>0.996867</td>\n",
       "      <td>0.992175</td>\n",
       "      <td>0.976373</td>\n",
       "      <td>1.006227</td>\n",
       "      <td>0.550350</td>\n",
       "      <td>0.980346</td>\n",
       "      <td>0.980395</td>\n",
       "      <td>0.980126</td>\n",
       "      <td>1.000793</td>\n",
       "      <td>1.002165</td>\n",
       "      <td>0.983914</td>\n",
       "      <td>0.986701</td>\n",
       "      <td>1.000378</td>\n",
       "      <td>1.000891</td>\n",
       "      <td>0.547777</td>\n",
       "      <td>0.996521</td>\n",
       "      <td>1.000203</td>\n",
       "      <td>0.992206</td>\n",
       "      <td>1.001248</td>\n",
       "      <td>0.997150</td>\n",
       "      <td>1.002456</td>\n",
       "      <td>0.994680</td>\n",
       "      <td>0.968496</td>\n",
       "      <td>0.997710</td>\n",
       "      <td>0.993169</td>\n",
       "      <td>1.009186</td>\n",
       "      <td>0.894759</td>\n",
       "      <td>0.870817</td>\n",
       "      <td>0.983705</td>\n",
       "      <td>0.987302</td>\n",
       "      <td>1.000906</td>\n",
       "      <td>0.995161</td>\n",
       "      <td>1.013218</td>\n",
       "      <td>1.007517</td>\n",
       "      <td>0.987659</td>\n",
       "      <td>0.982863</td>\n",
       "      <td>2.434343</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>1.010192</td>\n",
       "      <td>0.981436</td>\n",
       "      <td>1.001578</td>\n",
       "      <td>1.041409</td>\n",
       "      <td>0.508534</td>\n",
       "      <td>0.998113</td>\n",
       "      <td>0.992752</td>\n",
       "      <td>0.852219</td>\n",
       "      <td>1.001484</td>\n",
       "      <td>0.998365</td>\n",
       "      <td>0.996220</td>\n",
       "      <td>0.994031</td>\n",
       "      <td>1.002583</td>\n",
       "      <td>1.010490</td>\n",
       "      <td>1.000947</td>\n",
       "      <td>0.996851</td>\n",
       "      <td>0.999134</td>\n",
       "      <td>0.996913</td>\n",
       "      <td>0.988429</td>\n",
       "      <td>0.991727</td>\n",
       "      <td>0.984628</td>\n",
       "      <td>0.979060</td>\n",
       "      <td>0.973703</td>\n",
       "      <td>1.005697</td>\n",
       "      <td>0.838839</td>\n",
       "      <td>0.994783</td>\n",
       "      <td>0.963788</td>\n",
       "      <td>1.003323</td>\n",
       "      <td>0.999307</td>\n",
       "      <td>1.000240</td>\n",
       "      <td>0.993393</td>\n",
       "      <td>0.802468</td>\n",
       "      <td>1.064964</td>\n",
       "      <td>0.965830</td>\n",
       "      <td>0.990939</td>\n",
       "      <td>0.969320</td>\n",
       "      <td>0.999358</td>\n",
       "      <td>0.989305</td>\n",
       "      <td>0.940471</td>\n",
       "      <td>0.926679</td>\n",
       "      <td>1.006279</td>\n",
       "      <td>2.749426</td>\n",
       "      <td>0.986271</td>\n",
       "      <td>0.469896</td>\n",
       "      <td>0.999841</td>\n",
       "      <td>1.000491</td>\n",
       "      <td>0.965318</td>\n",
       "      <td>0.994411</td>\n",
       "      <td>0.378192</td>\n",
       "      <td>1.000449</td>\n",
       "      <td>0.997637</td>\n",
       "      <td>0.988987</td>\n",
       "      <td>1.002654</td>\n",
       "      <td>1.019872</td>\n",
       "      <td>0.963904</td>\n",
       "      <td>1.005212</td>\n",
       "      <td>1.010884</td>\n",
       "      <td>0.971834</td>\n",
       "      <td>1.001832</td>\n",
       "      <td>0.708510</td>\n",
       "      <td>1.020318</td>\n",
       "      <td>0.963762</td>\n",
       "      <td>0.956425</td>\n",
       "      <td>0.988251</td>\n",
       "      <td>1.007900</td>\n",
       "      <td>1.007781</td>\n",
       "      <td>1.003698</td>\n",
       "      <td>0.993133</td>\n",
       "      <td>0.768790</td>\n",
       "      <td>0.973345</td>\n",
       "      <td>1.006343</td>\n",
       "      <td>1.010419</td>\n",
       "      <td>0.997486</td>\n",
       "      <td>0.974750</td>\n",
       "      <td>1.004116</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.969716</td>\n",
       "      <td>1.004038</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.989670</td>\n",
       "      <td>1.001077</td>\n",
       "      <td>0.993375</td>\n",
       "      <td>0.994197</td>\n",
       "      <td>0.992879</td>\n",
       "      <td>0.992047</td>\n",
       "      <td>0.553293</td>\n",
       "      <td>0.984117</td>\n",
       "      <td>1.006338</td>\n",
       "      <td>0.996376</td>\n",
       "      <td>0.987119</td>\n",
       "      <td>0.985511</td>\n",
       "      <td>1.017893</td>\n",
       "      <td>0.996154</td>\n",
       "      <td>0.473037</td>\n",
       "      <td>0.980680</td>\n",
       "      <td>0.961801</td>\n",
       "      <td>0.921777</td>\n",
       "      <td>0.995268</td>\n",
       "      <td>0.994609</td>\n",
       "      <td>0.976283</td>\n",
       "      <td>1.008024</td>\n",
       "      <td>0.998294</td>\n",
       "      <td>0.812506</td>\n",
       "      <td>0.973214</td>\n",
       "      <td>0.550358</td>\n",
       "      <td>1.001399</td>\n",
       "      <td>0.991148</td>\n",
       "      <td>1.012044</td>\n",
       "      <td>0.996296</td>\n",
       "      <td>0.987231</td>\n",
       "      <td>0.982867</td>\n",
       "      <td>0.935318</td>\n",
       "      <td>1.024348</td>\n",
       "      <td>0.981076</td>\n",
       "      <td>0.998122</td>\n",
       "      <td>0.983445</td>\n",
       "      <td>0.998103</td>\n",
       "      <td>0.987238</td>\n",
       "      <td>1.002174</td>\n",
       "      <td>0.962967</td>\n",
       "      <td>0.952312</td>\n",
       "      <td>1.018924</td>\n",
       "      <td>0.979999</td>\n",
       "      <td>1.007037</td>\n",
       "      <td>1.001236</td>\n",
       "      <td>0.996903</td>\n",
       "      <td>0.989752</td>\n",
       "      <td>0.992397</td>\n",
       "      <td>1.007126</td>\n",
       "      <td>0.942358</td>\n",
       "      <td>0.984854</td>\n",
       "      <td>0.988344</td>\n",
       "      <td>0.975125</td>\n",
       "      <td>0.976284</td>\n",
       "      <td>1.000509</td>\n",
       "      <td>1.006309</td>\n",
       "      <td>0.994555</td>\n",
       "      <td>0.961533</td>\n",
       "      <td>0.964006</td>\n",
       "      <td>0.988547</td>\n",
       "      <td>0.958328</td>\n",
       "      <td>1.008769</td>\n",
       "      <td>0.990466</td>\n",
       "      <td>0.995682</td>\n",
       "      <td>0.995782</td>\n",
       "      <td>1.008465</td>\n",
       "      <td>1.005719</td>\n",
       "      <td>1.003312</td>\n",
       "      <td>0.534788</td>\n",
       "      <td>0.981177</td>\n",
       "      <td>0.211630</td>\n",
       "      <td>0.988520</td>\n",
       "      <td>0.969509</td>\n",
       "      <td>0.815174</td>\n",
       "      <td>0.963146</td>\n",
       "      <td>0.985389</td>\n",
       "      <td>0.942090</td>\n",
       "      <td>0.991724</td>\n",
       "      <td>0.980765</td>\n",
       "      <td>0.998303</td>\n",
       "      <td>1.009155</td>\n",
       "      <td>0.552785</td>\n",
       "      <td>0.987164</td>\n",
       "      <td>0.979085</td>\n",
       "      <td>0.981986</td>\n",
       "      <td>0.950930</td>\n",
       "      <td>0.986240</td>\n",
       "      <td>0.999074</td>\n",
       "      <td>1.008231</td>\n",
       "      <td>0.973951</td>\n",
       "      <td>0.999800</td>\n",
       "      <td>0.953235</td>\n",
       "      <td>1.012329</td>\n",
       "      <td>1.000717</td>\n",
       "      <td>1.001245</td>\n",
       "      <td>1.008392</td>\n",
       "      <td>0.499950</td>\n",
       "      <td>1.001111</td>\n",
       "      <td>0.997189</td>\n",
       "      <td>0.941418</td>\n",
       "      <td>1.005858</td>\n",
       "      <td>0.995830</td>\n",
       "      <td>1.015338</td>\n",
       "      <td>0.973696</td>\n",
       "      <td>0.979158</td>\n",
       "      <td>0.978589</td>\n",
       "      <td>0.986692</td>\n",
       "      <td>0.981022</td>\n",
       "      <td>1.002441</td>\n",
       "      <td>1.009822</td>\n",
       "      <td>0.955564</td>\n",
       "      <td>0.993839</td>\n",
       "      <td>0.883720</td>\n",
       "      <td>0.974794</td>\n",
       "      <td>1.004765</td>\n",
       "      <td>0.949082</td>\n",
       "      <td>1.010704</td>\n",
       "      <td>0.981931</td>\n",
       "      <td>0.993659</td>\n",
       "      <td>0.987267</td>\n",
       "      <td>0.993729</td>\n",
       "      <td>0.992823</td>\n",
       "      <td>0.977005</td>\n",
       "      <td>0.994534</td>\n",
       "      <td>0.996293</td>\n",
       "      <td>0.987515</td>\n",
       "      <td>0.972254</td>\n",
       "      <td>1.008084</td>\n",
       "      <td>1.004228</td>\n",
       "      <td>0.996216</td>\n",
       "      <td>0.966094</td>\n",
       "      <td>0.997272</td>\n",
       "      <td>0.920429</td>\n",
       "      <td>0.962991</td>\n",
       "      <td>0.996193</td>\n",
       "      <td>0.973503</td>\n",
       "      <td>0.984185</td>\n",
       "      <td>0.996036</td>\n",
       "      <td>0.397467</td>\n",
       "      <td>1.003202</td>\n",
       "      <td>1.004332</td>\n",
       "      <td>0.994374</td>\n",
       "      <td>0.995451</td>\n",
       "      <td>0.976371</td>\n",
       "      <td>1.004723</td>\n",
       "      <td>1.007827</td>\n",
       "      <td>1.006622</td>\n",
       "      <td>0.996736</td>\n",
       "      <td>1.016241</td>\n",
       "      <td>0.997840</td>\n",
       "      <td>1.018261</td>\n",
       "      <td>0.997349</td>\n",
       "      <td>1.098387</td>\n",
       "      <td>0.991033</td>\n",
       "      <td>0.990035</td>\n",
       "      <td>0.997974</td>\n",
       "      <td>0.880626</td>\n",
       "      <td>0.982568</td>\n",
       "      <td>0.959784</td>\n",
       "      <td>1.008039</td>\n",
       "      <td>1.009115</td>\n",
       "      <td>1.004385</td>\n",
       "      <td>1.000011</td>\n",
       "      <td>0.823351</td>\n",
       "      <td>1.015030</td>\n",
       "      <td>2.824001</td>\n",
       "      <td>0.995911</td>\n",
       "      <td>1.001002</td>\n",
       "      <td>0.519677</td>\n",
       "      <td>0.988505</td>\n",
       "      <td>0.976553</td>\n",
       "      <td>0.979663</td>\n",
       "      <td>0.989250</td>\n",
       "      <td>0.984307</td>\n",
       "      <td>0.979009</td>\n",
       "      <td>0.957222</td>\n",
       "      <td>0.990357</td>\n",
       "      <td>1.008122</td>\n",
       "      <td>0.934415</td>\n",
       "      <td>0.995079</td>\n",
       "      <td>0.968707</td>\n",
       "      <td>0.933686</td>\n",
       "      <td>1.006688</td>\n",
       "      <td>0.987008</td>\n",
       "      <td>0.979828</td>\n",
       "      <td>0.999691</td>\n",
       "      <td>1.004492</td>\n",
       "      <td>0.978963</td>\n",
       "      <td>0.984289</td>\n",
       "      <td>0.990183</td>\n",
       "      <td>0.985637</td>\n",
       "      <td>0.981675</td>\n",
       "      <td>0.977610</td>\n",
       "      <td>1.005352</td>\n",
       "      <td>0.987037</td>\n",
       "      <td>0.999032</td>\n",
       "      <td>1.003898</td>\n",
       "      <td>0.994807</td>\n",
       "      <td>0.986077</td>\n",
       "      <td>0.947330</td>\n",
       "      <td>1.008335</td>\n",
       "      <td>1.232269</td>\n",
       "      <td>1.016901</td>\n",
       "      <td>0.987258</td>\n",
       "      <td>0.984155</td>\n",
       "      <td>1.003866</td>\n",
       "      <td>0.926760</td>\n",
       "      <td>0.962188</td>\n",
       "      <td>1.001388</td>\n",
       "      <td>0.478255</td>\n",
       "      <td>1.006405</td>\n",
       "      <td>1.002959</td>\n",
       "      <td>1.000561</td>\n",
       "      <td>0.980775</td>\n",
       "      <td>1.005720</td>\n",
       "      <td>0.989477</td>\n",
       "      <td>0.984590</td>\n",
       "      <td>1.013173</td>\n",
       "      <td>1.005082</td>\n",
       "      <td>0.999009</td>\n",
       "      <td>0.994796</td>\n",
       "      <td>1.009726</td>\n",
       "      <td>0.732550</td>\n",
       "      <td>0.975858</td>\n",
       "      <td>0.867980</td>\n",
       "      <td>1.011240</td>\n",
       "      <td>1.009522</td>\n",
       "      <td>0.991369</td>\n",
       "      <td>0.970087</td>\n",
       "      <td>0.999734</td>\n",
       "      <td>0.999372</td>\n",
       "      <td>0.997574</td>\n",
       "      <td>0.985986</td>\n",
       "      <td>0.951358</td>\n",
       "      <td>0.991441</td>\n",
       "      <td>0.994804</td>\n",
       "      <td>0.806576</td>\n",
       "      <td>0.250005</td>\n",
       "      <td>0.998106</td>\n",
       "      <td>0.978557</td>\n",
       "      <td>0.972842</td>\n",
       "      <td>1.006719</td>\n",
       "      <td>0.990136</td>\n",
       "      <td>1.004295</td>\n",
       "      <td>0.987498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-4.246225</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.699710</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.106805</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.895424</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.816555</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.664452</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.063283</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.347012</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.307811</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.954366</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.507918</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.554695</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.758900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.363841</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.454976</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-4.091272</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.443351</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.652193</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.944768</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.339708</td>\n",
       "      <td>-3.232688</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-3.408886</td>\n",
       "      <td>-5.199338</td>\n",
       "      <td>-5.199338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9682.500000</td>\n",
       "      <td>-0.681368</td>\n",
       "      <td>-0.693879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.667434</td>\n",
       "      <td>-0.684292</td>\n",
       "      <td>-0.777239</td>\n",
       "      <td>-0.671211</td>\n",
       "      <td>-0.833827</td>\n",
       "      <td>-0.675604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.683985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.697260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.687860</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-0.707144</td>\n",
       "      <td>-0.666308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.758604</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-0.677279</td>\n",
       "      <td>-0.681649</td>\n",
       "      <td>-0.840418</td>\n",
       "      <td>-0.665542</td>\n",
       "      <td>-0.698127</td>\n",
       "      <td>-0.658682</td>\n",
       "      <td>-0.746514</td>\n",
       "      <td>-0.674718</td>\n",
       "      <td>-0.688303</td>\n",
       "      <td>-0.701995</td>\n",
       "      <td>-0.772276</td>\n",
       "      <td>-0.683531</td>\n",
       "      <td>-0.824109</td>\n",
       "      <td>-0.674263</td>\n",
       "      <td>-0.683371</td>\n",
       "      <td>-0.682835</td>\n",
       "      <td>-0.776043</td>\n",
       "      <td>-0.671351</td>\n",
       "      <td>-0.824086</td>\n",
       "      <td>-0.666752</td>\n",
       "      <td>-0.649290</td>\n",
       "      <td>-0.685386</td>\n",
       "      <td>-0.685574</td>\n",
       "      <td>-0.852023</td>\n",
       "      <td>-0.677065</td>\n",
       "      <td>-0.668913</td>\n",
       "      <td>-0.674211</td>\n",
       "      <td>-0.716812</td>\n",
       "      <td>-0.677853</td>\n",
       "      <td>-0.680538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.829336</td>\n",
       "      <td>-0.702029</td>\n",
       "      <td>-0.677652</td>\n",
       "      <td>-0.683259</td>\n",
       "      <td>-0.682331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.673798</td>\n",
       "      <td>-0.768800</td>\n",
       "      <td>-0.663962</td>\n",
       "      <td>-0.644688</td>\n",
       "      <td>-0.670600</td>\n",
       "      <td>-0.667435</td>\n",
       "      <td>-0.695625</td>\n",
       "      <td>-0.696346</td>\n",
       "      <td>-0.679138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.696442</td>\n",
       "      <td>-0.654223</td>\n",
       "      <td>-0.648411</td>\n",
       "      <td>-0.668647</td>\n",
       "      <td>-0.639314</td>\n",
       "      <td>-0.681414</td>\n",
       "      <td>-0.677848</td>\n",
       "      <td>-0.732940</td>\n",
       "      <td>-0.648078</td>\n",
       "      <td>-0.673144</td>\n",
       "      <td>-0.734181</td>\n",
       "      <td>-0.660248</td>\n",
       "      <td>-0.657845</td>\n",
       "      <td>-0.695686</td>\n",
       "      <td>-0.665292</td>\n",
       "      <td>-0.675586</td>\n",
       "      <td>-0.679654</td>\n",
       "      <td>-0.691353</td>\n",
       "      <td>-0.663678</td>\n",
       "      <td>-0.669272</td>\n",
       "      <td>-0.704815</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-0.685645</td>\n",
       "      <td>-0.682386</td>\n",
       "      <td>-0.698412</td>\n",
       "      <td>-0.686526</td>\n",
       "      <td>-0.706113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.656090</td>\n",
       "      <td>-0.652237</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-0.686674</td>\n",
       "      <td>-0.687176</td>\n",
       "      <td>-0.709406</td>\n",
       "      <td>-0.680050</td>\n",
       "      <td>-0.695799</td>\n",
       "      <td>-0.665555</td>\n",
       "      <td>-0.664379</td>\n",
       "      <td>-0.669893</td>\n",
       "      <td>-0.651534</td>\n",
       "      <td>-0.733940</td>\n",
       "      <td>-0.666988</td>\n",
       "      <td>-0.707228</td>\n",
       "      <td>-0.687310</td>\n",
       "      <td>-0.702785</td>\n",
       "      <td>-0.675715</td>\n",
       "      <td>-0.684830</td>\n",
       "      <td>-0.964990</td>\n",
       "      <td>-0.664721</td>\n",
       "      <td>-0.683806</td>\n",
       "      <td>-0.669378</td>\n",
       "      <td>-0.682840</td>\n",
       "      <td>-0.692777</td>\n",
       "      <td>-0.664108</td>\n",
       "      <td>-0.842510</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-0.761843</td>\n",
       "      <td>-0.669571</td>\n",
       "      <td>-0.679850</td>\n",
       "      <td>-0.667447</td>\n",
       "      <td>-0.681983</td>\n",
       "      <td>-0.681745</td>\n",
       "      <td>-0.699874</td>\n",
       "      <td>-0.743448</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-0.729382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.680508</td>\n",
       "      <td>-0.680576</td>\n",
       "      <td>-0.733024</td>\n",
       "      <td>-0.665368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.680736</td>\n",
       "      <td>-0.694943</td>\n",
       "      <td>-0.665731</td>\n",
       "      <td>-0.676426</td>\n",
       "      <td>-0.709279</td>\n",
       "      <td>-0.699680</td>\n",
       "      <td>-0.686066</td>\n",
       "      <td>-0.684163</td>\n",
       "      <td>-0.687280</td>\n",
       "      <td>-0.677873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.694038</td>\n",
       "      <td>-0.737563</td>\n",
       "      <td>-0.689729</td>\n",
       "      <td>-0.695203</td>\n",
       "      <td>-0.693250</td>\n",
       "      <td>-0.666674</td>\n",
       "      <td>-0.679412</td>\n",
       "      <td>-0.671704</td>\n",
       "      <td>-0.863241</td>\n",
       "      <td>-0.695036</td>\n",
       "      <td>-0.671222</td>\n",
       "      <td>-0.754554</td>\n",
       "      <td>-0.686435</td>\n",
       "      <td>-0.673865</td>\n",
       "      <td>-0.681113</td>\n",
       "      <td>-0.682271</td>\n",
       "      <td>-0.661535</td>\n",
       "      <td>-0.706800</td>\n",
       "      <td>-0.686145</td>\n",
       "      <td>-0.680943</td>\n",
       "      <td>-0.666125</td>\n",
       "      <td>-0.718780</td>\n",
       "      <td>-0.787404</td>\n",
       "      <td>-0.668847</td>\n",
       "      <td>-0.691923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.666620</td>\n",
       "      <td>-0.682406</td>\n",
       "      <td>-0.669836</td>\n",
       "      <td>-0.736730</td>\n",
       "      <td>-0.672787</td>\n",
       "      <td>-0.634107</td>\n",
       "      <td>-0.673962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.700640</td>\n",
       "      <td>-0.690815</td>\n",
       "      <td>-0.707112</td>\n",
       "      <td>-0.644546</td>\n",
       "      <td>-0.680162</td>\n",
       "      <td>-0.699703</td>\n",
       "      <td>-0.672659</td>\n",
       "      <td>-0.674841</td>\n",
       "      <td>-0.859872</td>\n",
       "      <td>-0.765482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.651846</td>\n",
       "      <td>-0.673189</td>\n",
       "      <td>-0.671716</td>\n",
       "      <td>-0.684371</td>\n",
       "      <td>-0.668645</td>\n",
       "      <td>-0.698100</td>\n",
       "      <td>-0.740687</td>\n",
       "      <td>-0.928778</td>\n",
       "      <td>-0.702428</td>\n",
       "      <td>-0.671008</td>\n",
       "      <td>-0.665760</td>\n",
       "      <td>-0.652822</td>\n",
       "      <td>-0.675162</td>\n",
       "      <td>-0.677545</td>\n",
       "      <td>-0.711591</td>\n",
       "      <td>-0.792625</td>\n",
       "      <td>-0.675707</td>\n",
       "      <td>-0.700641</td>\n",
       "      <td>-0.683357</td>\n",
       "      <td>-0.681463</td>\n",
       "      <td>-0.654180</td>\n",
       "      <td>-0.671422</td>\n",
       "      <td>-0.688828</td>\n",
       "      <td>-0.698988</td>\n",
       "      <td>-0.712954</td>\n",
       "      <td>-0.702600</td>\n",
       "      <td>-0.694788</td>\n",
       "      <td>-0.679224</td>\n",
       "      <td>-0.702850</td>\n",
       "      <td>-0.704846</td>\n",
       "      <td>-0.655318</td>\n",
       "      <td>-0.664692</td>\n",
       "      <td>-0.689651</td>\n",
       "      <td>-0.755472</td>\n",
       "      <td>-0.672518</td>\n",
       "      <td>-0.692781</td>\n",
       "      <td>-0.677945</td>\n",
       "      <td>-0.684627</td>\n",
       "      <td>-0.668330</td>\n",
       "      <td>-0.650276</td>\n",
       "      <td>-0.687876</td>\n",
       "      <td>-0.693796</td>\n",
       "      <td>-0.678622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.692234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.704182</td>\n",
       "      <td>-0.688298</td>\n",
       "      <td>-0.870681</td>\n",
       "      <td>-0.687415</td>\n",
       "      <td>-0.788120</td>\n",
       "      <td>-0.703509</td>\n",
       "      <td>-0.695951</td>\n",
       "      <td>-0.778082</td>\n",
       "      <td>-0.665238</td>\n",
       "      <td>-0.741956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.737534</td>\n",
       "      <td>-0.700711</td>\n",
       "      <td>-0.699503</td>\n",
       "      <td>-0.702058</td>\n",
       "      <td>-0.666210</td>\n",
       "      <td>-0.668001</td>\n",
       "      <td>-0.665963</td>\n",
       "      <td>-0.686670</td>\n",
       "      <td>-0.676353</td>\n",
       "      <td>-0.764734</td>\n",
       "      <td>-0.676452</td>\n",
       "      <td>-0.705631</td>\n",
       "      <td>-0.680850</td>\n",
       "      <td>-0.678844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.684928</td>\n",
       "      <td>-0.672167</td>\n",
       "      <td>-0.725791</td>\n",
       "      <td>-0.676952</td>\n",
       "      <td>-0.687317</td>\n",
       "      <td>-0.661192</td>\n",
       "      <td>-0.669826</td>\n",
       "      <td>-0.709711</td>\n",
       "      <td>-0.684039</td>\n",
       "      <td>-0.722082</td>\n",
       "      <td>-0.677184</td>\n",
       "      <td>-0.682438</td>\n",
       "      <td>-0.847536</td>\n",
       "      <td>-0.764480</td>\n",
       "      <td>-0.706234</td>\n",
       "      <td>-0.763601</td>\n",
       "      <td>-0.674194</td>\n",
       "      <td>-0.664689</td>\n",
       "      <td>-0.715565</td>\n",
       "      <td>-0.694508</td>\n",
       "      <td>-0.692439</td>\n",
       "      <td>-0.703734</td>\n",
       "      <td>-0.678792</td>\n",
       "      <td>-0.675102</td>\n",
       "      <td>-0.673751</td>\n",
       "      <td>-0.684357</td>\n",
       "      <td>-0.684677</td>\n",
       "      <td>-0.665803</td>\n",
       "      <td>-0.691151</td>\n",
       "      <td>-0.694493</td>\n",
       "      <td>-0.694056</td>\n",
       "      <td>-0.673886</td>\n",
       "      <td>-0.705164</td>\n",
       "      <td>-0.760023</td>\n",
       "      <td>-0.693051</td>\n",
       "      <td>-0.677476</td>\n",
       "      <td>-0.664614</td>\n",
       "      <td>-0.667957</td>\n",
       "      <td>-0.642705</td>\n",
       "      <td>-0.692741</td>\n",
       "      <td>-0.683254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.677226</td>\n",
       "      <td>-0.662146</td>\n",
       "      <td>-0.685323</td>\n",
       "      <td>-0.696603</td>\n",
       "      <td>-0.697441</td>\n",
       "      <td>-0.669281</td>\n",
       "      <td>-0.680172</td>\n",
       "      <td>-0.679450</td>\n",
       "      <td>-0.666772</td>\n",
       "      <td>-0.697591</td>\n",
       "      <td>-0.640673</td>\n",
       "      <td>-0.665611</td>\n",
       "      <td>-0.703837</td>\n",
       "      <td>-0.711522</td>\n",
       "      <td>-0.652252</td>\n",
       "      <td>-0.697612</td>\n",
       "      <td>-0.660348</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.670270</td>\n",
       "      <td>-0.704174</td>\n",
       "      <td>-0.697900</td>\n",
       "      <td>-0.671823</td>\n",
       "      <td>-0.679616</td>\n",
       "      <td>-0.655450</td>\n",
       "      <td>-0.833933</td>\n",
       "      <td>-0.851046</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-0.687613</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.676489</td>\n",
       "      <td>-0.664887</td>\n",
       "      <td>-0.672611</td>\n",
       "      <td>-0.667491</td>\n",
       "      <td>-0.677543</td>\n",
       "      <td>-0.685926</td>\n",
       "      <td>-0.722432</td>\n",
       "      <td>-0.668427</td>\n",
       "      <td>-0.659671</td>\n",
       "      <td>-0.783459</td>\n",
       "      <td>-0.671197</td>\n",
       "      <td>-0.698205</td>\n",
       "      <td>-0.844036</td>\n",
       "      <td>-0.664671</td>\n",
       "      <td>-0.684689</td>\n",
       "      <td>-0.713787</td>\n",
       "      <td>-0.692382</td>\n",
       "      <td>-0.650802</td>\n",
       "      <td>-0.695107</td>\n",
       "      <td>-0.773901</td>\n",
       "      <td>-0.655925</td>\n",
       "      <td>-0.668755</td>\n",
       "      <td>-0.691752</td>\n",
       "      <td>-0.668176</td>\n",
       "      <td>-0.670563</td>\n",
       "      <td>-0.668315</td>\n",
       "      <td>-0.693579</td>\n",
       "      <td>-0.662723</td>\n",
       "      <td>-0.677450</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-0.668016</td>\n",
       "      <td>-0.715063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.684471</td>\n",
       "      <td>-0.673860</td>\n",
       "      <td>-0.674191</td>\n",
       "      <td>-0.681502</td>\n",
       "      <td>-0.571487</td>\n",
       "      <td>-0.731304</td>\n",
       "      <td>-0.705002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.648097</td>\n",
       "      <td>-0.665864</td>\n",
       "      <td>-0.671427</td>\n",
       "      <td>-0.670812</td>\n",
       "      <td>-0.668331</td>\n",
       "      <td>-0.678788</td>\n",
       "      <td>-0.707585</td>\n",
       "      <td>-0.698908</td>\n",
       "      <td>-0.656044</td>\n",
       "      <td>-0.680333</td>\n",
       "      <td>-0.687233</td>\n",
       "      <td>-0.700607</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-0.667426</td>\n",
       "      <td>-0.855066</td>\n",
       "      <td>-0.677072</td>\n",
       "      <td>-0.674688</td>\n",
       "      <td>-0.665726</td>\n",
       "      <td>-0.701116</td>\n",
       "      <td>-0.679259</td>\n",
       "      <td>-0.680280</td>\n",
       "      <td>-0.677313</td>\n",
       "      <td>-0.728601</td>\n",
       "      <td>-0.749936</td>\n",
       "      <td>-0.685081</td>\n",
       "      <td>-0.671584</td>\n",
       "      <td>-0.816589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.694098</td>\n",
       "      <td>-0.700684</td>\n",
       "      <td>-0.676811</td>\n",
       "      <td>-0.676359</td>\n",
       "      <td>-0.687918</td>\n",
       "      <td>-0.678918</td>\n",
       "      <td>-0.692241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>19365.000000</td>\n",
       "      <td>-0.014384</td>\n",
       "      <td>-0.014327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025820</td>\n",
       "      <td>0.005269</td>\n",
       "      <td>-0.166902</td>\n",
       "      <td>-0.021997</td>\n",
       "      <td>-0.207170</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.012138</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-0.022042</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.090774</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.008526</td>\n",
       "      <td>0.016427</td>\n",
       "      <td>-0.270955</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>0.014306</td>\n",
       "      <td>-0.111424</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>-0.038537</td>\n",
       "      <td>-0.007767</td>\n",
       "      <td>-0.176931</td>\n",
       "      <td>-0.002277</td>\n",
       "      <td>-0.151748</td>\n",
       "      <td>0.013356</td>\n",
       "      <td>-0.008604</td>\n",
       "      <td>-0.007453</td>\n",
       "      <td>-0.154888</td>\n",
       "      <td>-0.005520</td>\n",
       "      <td>-0.211537</td>\n",
       "      <td>0.016169</td>\n",
       "      <td>0.018603</td>\n",
       "      <td>-0.011422</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>-0.258583</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.011099</td>\n",
       "      <td>0.013609</td>\n",
       "      <td>-0.042106</td>\n",
       "      <td>-0.022408</td>\n",
       "      <td>-0.002108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.185143</td>\n",
       "      <td>-0.009547</td>\n",
       "      <td>-0.004178</td>\n",
       "      <td>-0.028918</td>\n",
       "      <td>-0.006192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.014412</td>\n",
       "      <td>-0.108404</td>\n",
       "      <td>-0.026349</td>\n",
       "      <td>0.020259</td>\n",
       "      <td>-0.010423</td>\n",
       "      <td>0.010058</td>\n",
       "      <td>-0.030827</td>\n",
       "      <td>-0.067367</td>\n",
       "      <td>-0.015607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.017485</td>\n",
       "      <td>0.006073</td>\n",
       "      <td>0.022797</td>\n",
       "      <td>-0.004590</td>\n",
       "      <td>0.020412</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>-0.001545</td>\n",
       "      <td>-0.068004</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>-0.001109</td>\n",
       "      <td>-0.078867</td>\n",
       "      <td>-0.145386</td>\n",
       "      <td>-0.143250</td>\n",
       "      <td>-0.024907</td>\n",
       "      <td>0.015646</td>\n",
       "      <td>0.007094</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>-0.005236</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>-0.006539</td>\n",
       "      <td>-0.034390</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.006652</td>\n",
       "      <td>0.022871</td>\n",
       "      <td>-0.007924</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>-0.065948</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005004</td>\n",
       "      <td>0.018982</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-0.011705</td>\n",
       "      <td>-0.019005</td>\n",
       "      <td>-0.026268</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>-0.014663</td>\n",
       "      <td>0.020830</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>0.022570</td>\n",
       "      <td>-0.057819</td>\n",
       "      <td>0.023388</td>\n",
       "      <td>-0.043273</td>\n",
       "      <td>-0.015105</td>\n",
       "      <td>-0.046913</td>\n",
       "      <td>-0.019429</td>\n",
       "      <td>-0.011355</td>\n",
       "      <td>-0.433357</td>\n",
       "      <td>0.008834</td>\n",
       "      <td>-0.019820</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>0.006878</td>\n",
       "      <td>-0.035902</td>\n",
       "      <td>0.036334</td>\n",
       "      <td>-0.252741</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-0.127674</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>-0.039692</td>\n",
       "      <td>0.005205</td>\n",
       "      <td>-0.023360</td>\n",
       "      <td>-0.026835</td>\n",
       "      <td>-0.073093</td>\n",
       "      <td>-0.075327</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-0.065016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001668</td>\n",
       "      <td>-0.002098</td>\n",
       "      <td>-0.079461</td>\n",
       "      <td>0.013777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003127</td>\n",
       "      <td>-0.015204</td>\n",
       "      <td>0.006867</td>\n",
       "      <td>-0.008749</td>\n",
       "      <td>-0.015627</td>\n",
       "      <td>-0.053359</td>\n",
       "      <td>-0.005095</td>\n",
       "      <td>-0.002006</td>\n",
       "      <td>-0.009693</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007680</td>\n",
       "      <td>-0.084080</td>\n",
       "      <td>-0.027815</td>\n",
       "      <td>-0.014082</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>-0.023392</td>\n",
       "      <td>0.003424</td>\n",
       "      <td>-0.259467</td>\n",
       "      <td>-0.042626</td>\n",
       "      <td>0.046825</td>\n",
       "      <td>-0.081586</td>\n",
       "      <td>-0.007873</td>\n",
       "      <td>-0.023128</td>\n",
       "      <td>-0.001343</td>\n",
       "      <td>-0.005731</td>\n",
       "      <td>0.008404</td>\n",
       "      <td>-0.037789</td>\n",
       "      <td>-0.018052</td>\n",
       "      <td>-0.004971</td>\n",
       "      <td>-0.020749</td>\n",
       "      <td>-0.074985</td>\n",
       "      <td>-0.183422</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>-0.018222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.019816</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.003916</td>\n",
       "      <td>-0.081696</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>0.052789</td>\n",
       "      <td>-0.005165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.028304</td>\n",
       "      <td>-0.034562</td>\n",
       "      <td>-0.081004</td>\n",
       "      <td>0.027624</td>\n",
       "      <td>-0.021278</td>\n",
       "      <td>-0.021185</td>\n",
       "      <td>0.003208</td>\n",
       "      <td>0.003287</td>\n",
       "      <td>-0.292288</td>\n",
       "      <td>-0.126179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006842</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>-0.007775</td>\n",
       "      <td>-0.031789</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>-0.050626</td>\n",
       "      <td>-0.089729</td>\n",
       "      <td>-0.259739</td>\n",
       "      <td>-0.026123</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>-0.000916</td>\n",
       "      <td>0.014279</td>\n",
       "      <td>-0.009614</td>\n",
       "      <td>0.003161</td>\n",
       "      <td>-0.053874</td>\n",
       "      <td>-0.165923</td>\n",
       "      <td>0.024050</td>\n",
       "      <td>-0.027535</td>\n",
       "      <td>-0.009086</td>\n",
       "      <td>-0.012283</td>\n",
       "      <td>0.009074</td>\n",
       "      <td>-0.001724</td>\n",
       "      <td>-0.008198</td>\n",
       "      <td>-0.008406</td>\n",
       "      <td>-0.067569</td>\n",
       "      <td>-0.045307</td>\n",
       "      <td>-0.029990</td>\n",
       "      <td>-0.023578</td>\n",
       "      <td>-0.028460</td>\n",
       "      <td>-0.026933</td>\n",
       "      <td>0.011267</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>-0.028038</td>\n",
       "      <td>-0.107425</td>\n",
       "      <td>-0.006555</td>\n",
       "      <td>-0.032586</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>-0.022614</td>\n",
       "      <td>0.016027</td>\n",
       "      <td>0.013607</td>\n",
       "      <td>-0.012243</td>\n",
       "      <td>-0.012823</td>\n",
       "      <td>0.007481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.023215</td>\n",
       "      <td>-0.035255</td>\n",
       "      <td>-0.291406</td>\n",
       "      <td>-0.016990</td>\n",
       "      <td>-0.141078</td>\n",
       "      <td>-0.052883</td>\n",
       "      <td>-0.018266</td>\n",
       "      <td>-0.150063</td>\n",
       "      <td>-0.009604</td>\n",
       "      <td>-0.090001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.073471</td>\n",
       "      <td>-0.031648</td>\n",
       "      <td>-0.036504</td>\n",
       "      <td>-0.043174</td>\n",
       "      <td>-0.004844</td>\n",
       "      <td>-0.003647</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.012980</td>\n",
       "      <td>-0.017978</td>\n",
       "      <td>-0.110067</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>-0.002493</td>\n",
       "      <td>-0.002289</td>\n",
       "      <td>-0.021363</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.018471</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>-0.046470</td>\n",
       "      <td>-0.008815</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.030858</td>\n",
       "      <td>-0.037564</td>\n",
       "      <td>-0.069100</td>\n",
       "      <td>-0.025482</td>\n",
       "      <td>-0.063273</td>\n",
       "      <td>-0.003838</td>\n",
       "      <td>-0.021937</td>\n",
       "      <td>-0.194608</td>\n",
       "      <td>-0.116781</td>\n",
       "      <td>-0.024802</td>\n",
       "      <td>-0.144874</td>\n",
       "      <td>-0.010997</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>-0.059995</td>\n",
       "      <td>-0.009451</td>\n",
       "      <td>-0.035380</td>\n",
       "      <td>-0.023512</td>\n",
       "      <td>-0.016083</td>\n",
       "      <td>-0.012573</td>\n",
       "      <td>-0.013839</td>\n",
       "      <td>-0.027744</td>\n",
       "      <td>-0.023230</td>\n",
       "      <td>0.009442</td>\n",
       "      <td>0.012055</td>\n",
       "      <td>-0.041937</td>\n",
       "      <td>-0.020363</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>-0.055656</td>\n",
       "      <td>-0.124352</td>\n",
       "      <td>-0.034034</td>\n",
       "      <td>-0.059405</td>\n",
       "      <td>0.034815</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>0.006415</td>\n",
       "      <td>-0.014600</td>\n",
       "      <td>-0.013987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005269</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>-0.008253</td>\n",
       "      <td>-0.026796</td>\n",
       "      <td>-0.037694</td>\n",
       "      <td>0.014581</td>\n",
       "      <td>0.018563</td>\n",
       "      <td>0.005215</td>\n",
       "      <td>-0.008423</td>\n",
       "      <td>-0.009215</td>\n",
       "      <td>0.032356</td>\n",
       "      <td>0.020918</td>\n",
       "      <td>-0.046228</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.024651</td>\n",
       "      <td>-0.050827</td>\n",
       "      <td>0.036272</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-0.000394</td>\n",
       "      <td>-0.058590</td>\n",
       "      <td>-0.008775</td>\n",
       "      <td>0.015762</td>\n",
       "      <td>0.005465</td>\n",
       "      <td>0.010772</td>\n",
       "      <td>-0.215250</td>\n",
       "      <td>-0.155190</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-0.008881</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007242</td>\n",
       "      <td>-0.004794</td>\n",
       "      <td>-0.006529</td>\n",
       "      <td>-0.002665</td>\n",
       "      <td>0.017862</td>\n",
       "      <td>-0.026158</td>\n",
       "      <td>-0.078754</td>\n",
       "      <td>0.003358</td>\n",
       "      <td>0.013152</td>\n",
       "      <td>-0.168200</td>\n",
       "      <td>0.010623</td>\n",
       "      <td>-0.013932</td>\n",
       "      <td>-0.195286</td>\n",
       "      <td>0.007497</td>\n",
       "      <td>-0.008879</td>\n",
       "      <td>-0.063387</td>\n",
       "      <td>-0.018019</td>\n",
       "      <td>0.006949</td>\n",
       "      <td>-0.028692</td>\n",
       "      <td>-0.162282</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>-0.002078</td>\n",
       "      <td>-0.031105</td>\n",
       "      <td>-0.026888</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>-0.006910</td>\n",
       "      <td>-0.014610</td>\n",
       "      <td>0.011852</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-0.013549</td>\n",
       "      <td>-0.008381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.019065</td>\n",
       "      <td>0.007727</td>\n",
       "      <td>-0.005162</td>\n",
       "      <td>-0.002807</td>\n",
       "      <td>0.079940</td>\n",
       "      <td>-0.093222</td>\n",
       "      <td>-0.047493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033889</td>\n",
       "      <td>0.006935</td>\n",
       "      <td>-0.020253</td>\n",
       "      <td>-0.001357</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>-0.018453</td>\n",
       "      <td>-0.054771</td>\n",
       "      <td>-0.011937</td>\n",
       "      <td>0.016183</td>\n",
       "      <td>-0.003888</td>\n",
       "      <td>-0.006015</td>\n",
       "      <td>-0.053030</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-0.006121</td>\n",
       "      <td>-0.267089</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>0.009865</td>\n",
       "      <td>-0.006369</td>\n",
       "      <td>-0.046984</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>-0.010543</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>-0.051921</td>\n",
       "      <td>-0.104127</td>\n",
       "      <td>-0.010843</td>\n",
       "      <td>0.017161</td>\n",
       "      <td>-0.216972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.016165</td>\n",
       "      <td>-0.035410</td>\n",
       "      <td>-0.000174</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>-0.018817</td>\n",
       "      <td>-0.012701</td>\n",
       "      <td>-0.030068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>29047.500000</td>\n",
       "      <td>0.658456</td>\n",
       "      <td>0.651838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.706342</td>\n",
       "      <td>0.682534</td>\n",
       "      <td>0.566233</td>\n",
       "      <td>0.637608</td>\n",
       "      <td>0.407831</td>\n",
       "      <td>0.630862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.670615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.662320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.637805</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.625285</td>\n",
       "      <td>0.694324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.580801</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.680930</td>\n",
       "      <td>0.690049</td>\n",
       "      <td>0.215828</td>\n",
       "      <td>0.672486</td>\n",
       "      <td>0.664352</td>\n",
       "      <td>0.701466</td>\n",
       "      <td>0.474503</td>\n",
       "      <td>0.659911</td>\n",
       "      <td>0.616066</td>\n",
       "      <td>0.658116</td>\n",
       "      <td>0.488541</td>\n",
       "      <td>0.668743</td>\n",
       "      <td>0.483525</td>\n",
       "      <td>0.645433</td>\n",
       "      <td>0.649353</td>\n",
       "      <td>0.664466</td>\n",
       "      <td>0.502314</td>\n",
       "      <td>0.667086</td>\n",
       "      <td>0.396409</td>\n",
       "      <td>0.682201</td>\n",
       "      <td>0.687998</td>\n",
       "      <td>0.676576</td>\n",
       "      <td>0.682372</td>\n",
       "      <td>0.312719</td>\n",
       "      <td>0.679234</td>\n",
       "      <td>0.706864</td>\n",
       "      <td>0.676778</td>\n",
       "      <td>0.637295</td>\n",
       "      <td>0.639296</td>\n",
       "      <td>0.662326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460671</td>\n",
       "      <td>0.658355</td>\n",
       "      <td>0.679361</td>\n",
       "      <td>0.623000</td>\n",
       "      <td>0.683725</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636080</td>\n",
       "      <td>0.560209</td>\n",
       "      <td>0.636567</td>\n",
       "      <td>0.700650</td>\n",
       "      <td>0.656935</td>\n",
       "      <td>0.673629</td>\n",
       "      <td>0.614965</td>\n",
       "      <td>0.617835</td>\n",
       "      <td>0.659814</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.658252</td>\n",
       "      <td>0.678299</td>\n",
       "      <td>0.683622</td>\n",
       "      <td>0.704619</td>\n",
       "      <td>0.689569</td>\n",
       "      <td>0.671516</td>\n",
       "      <td>0.649946</td>\n",
       "      <td>0.562425</td>\n",
       "      <td>0.689526</td>\n",
       "      <td>0.669840</td>\n",
       "      <td>0.602288</td>\n",
       "      <td>0.410799</td>\n",
       "      <td>0.386432</td>\n",
       "      <td>0.642403</td>\n",
       "      <td>0.668708</td>\n",
       "      <td>0.663565</td>\n",
       "      <td>0.657187</td>\n",
       "      <td>0.686378</td>\n",
       "      <td>0.679482</td>\n",
       "      <td>0.643038</td>\n",
       "      <td>0.618040</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.668608</td>\n",
       "      <td>0.703017</td>\n",
       "      <td>0.623359</td>\n",
       "      <td>0.674070</td>\n",
       "      <td>0.632564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.661227</td>\n",
       "      <td>0.668705</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.672801</td>\n",
       "      <td>0.683639</td>\n",
       "      <td>0.628867</td>\n",
       "      <td>0.655540</td>\n",
       "      <td>0.655558</td>\n",
       "      <td>0.675666</td>\n",
       "      <td>0.677406</td>\n",
       "      <td>0.675147</td>\n",
       "      <td>0.697021</td>\n",
       "      <td>0.607188</td>\n",
       "      <td>0.663034</td>\n",
       "      <td>0.605553</td>\n",
       "      <td>0.640443</td>\n",
       "      <td>0.599634</td>\n",
       "      <td>0.652679</td>\n",
       "      <td>0.674749</td>\n",
       "      <td>0.345654</td>\n",
       "      <td>0.678847</td>\n",
       "      <td>0.626354</td>\n",
       "      <td>0.691999</td>\n",
       "      <td>0.693373</td>\n",
       "      <td>0.656083</td>\n",
       "      <td>0.689854</td>\n",
       "      <td>0.268594</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.492650</td>\n",
       "      <td>0.666030</td>\n",
       "      <td>0.628626</td>\n",
       "      <td>0.685006</td>\n",
       "      <td>0.623871</td>\n",
       "      <td>0.621807</td>\n",
       "      <td>0.525681</td>\n",
       "      <td>0.595708</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.603789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.686235</td>\n",
       "      <td>0.651036</td>\n",
       "      <td>0.528690</td>\n",
       "      <td>0.695480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.653102</td>\n",
       "      <td>0.654872</td>\n",
       "      <td>0.661487</td>\n",
       "      <td>0.683811</td>\n",
       "      <td>0.678691</td>\n",
       "      <td>0.606983</td>\n",
       "      <td>0.678720</td>\n",
       "      <td>0.674997</td>\n",
       "      <td>0.630973</td>\n",
       "      <td>0.679704</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.694811</td>\n",
       "      <td>0.521614</td>\n",
       "      <td>0.608245</td>\n",
       "      <td>0.654434</td>\n",
       "      <td>0.662826</td>\n",
       "      <td>0.678928</td>\n",
       "      <td>0.657116</td>\n",
       "      <td>0.676457</td>\n",
       "      <td>0.250686</td>\n",
       "      <td>0.600243</td>\n",
       "      <td>0.723665</td>\n",
       "      <td>0.601225</td>\n",
       "      <td>0.664304</td>\n",
       "      <td>0.628198</td>\n",
       "      <td>0.674098</td>\n",
       "      <td>0.651928</td>\n",
       "      <td>0.648462</td>\n",
       "      <td>0.623204</td>\n",
       "      <td>0.646286</td>\n",
       "      <td>0.665715</td>\n",
       "      <td>0.682493</td>\n",
       "      <td>0.606861</td>\n",
       "      <td>0.458604</td>\n",
       "      <td>0.669172</td>\n",
       "      <td>0.635909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.652128</td>\n",
       "      <td>0.688771</td>\n",
       "      <td>0.673223</td>\n",
       "      <td>0.598993</td>\n",
       "      <td>0.665609</td>\n",
       "      <td>0.747343</td>\n",
       "      <td>0.654492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617916</td>\n",
       "      <td>0.591478</td>\n",
       "      <td>0.537881</td>\n",
       "      <td>0.700512</td>\n",
       "      <td>0.648235</td>\n",
       "      <td>0.644586</td>\n",
       "      <td>0.670970</td>\n",
       "      <td>0.676562</td>\n",
       "      <td>0.198561</td>\n",
       "      <td>0.477222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.682903</td>\n",
       "      <td>0.665349</td>\n",
       "      <td>0.683534</td>\n",
       "      <td>0.654502</td>\n",
       "      <td>0.676629</td>\n",
       "      <td>0.610618</td>\n",
       "      <td>0.550888</td>\n",
       "      <td>0.461788</td>\n",
       "      <td>0.624207</td>\n",
       "      <td>0.670523</td>\n",
       "      <td>0.667467</td>\n",
       "      <td>0.664712</td>\n",
       "      <td>0.654054</td>\n",
       "      <td>0.687883</td>\n",
       "      <td>0.567744</td>\n",
       "      <td>0.471451</td>\n",
       "      <td>0.715944</td>\n",
       "      <td>0.630760</td>\n",
       "      <td>0.663159</td>\n",
       "      <td>0.667628</td>\n",
       "      <td>0.676326</td>\n",
       "      <td>0.655871</td>\n",
       "      <td>0.666384</td>\n",
       "      <td>0.665252</td>\n",
       "      <td>0.569637</td>\n",
       "      <td>0.605758</td>\n",
       "      <td>0.645956</td>\n",
       "      <td>0.641528</td>\n",
       "      <td>0.643243</td>\n",
       "      <td>0.670830</td>\n",
       "      <td>0.712132</td>\n",
       "      <td>0.681980</td>\n",
       "      <td>0.628111</td>\n",
       "      <td>0.505471</td>\n",
       "      <td>0.680817</td>\n",
       "      <td>0.613243</td>\n",
       "      <td>0.668326</td>\n",
       "      <td>0.633589</td>\n",
       "      <td>0.708499</td>\n",
       "      <td>0.680368</td>\n",
       "      <td>0.666718</td>\n",
       "      <td>0.656913</td>\n",
       "      <td>0.700025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.636916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642102</td>\n",
       "      <td>0.614827</td>\n",
       "      <td>0.183542</td>\n",
       "      <td>0.630084</td>\n",
       "      <td>0.508163</td>\n",
       "      <td>0.578725</td>\n",
       "      <td>0.646214</td>\n",
       "      <td>0.504279</td>\n",
       "      <td>0.665809</td>\n",
       "      <td>0.589700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.622924</td>\n",
       "      <td>0.631730</td>\n",
       "      <td>0.619547</td>\n",
       "      <td>0.584028</td>\n",
       "      <td>0.672474</td>\n",
       "      <td>0.671524</td>\n",
       "      <td>0.692412</td>\n",
       "      <td>0.652217</td>\n",
       "      <td>0.674628</td>\n",
       "      <td>0.482589</td>\n",
       "      <td>0.680090</td>\n",
       "      <td>0.652700</td>\n",
       "      <td>0.659263</td>\n",
       "      <td>0.667734</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.674463</td>\n",
       "      <td>0.677322</td>\n",
       "      <td>0.586387</td>\n",
       "      <td>0.680060</td>\n",
       "      <td>0.663287</td>\n",
       "      <td>0.715003</td>\n",
       "      <td>0.619177</td>\n",
       "      <td>0.608503</td>\n",
       "      <td>0.618294</td>\n",
       "      <td>0.590665</td>\n",
       "      <td>0.642173</td>\n",
       "      <td>0.661831</td>\n",
       "      <td>0.494695</td>\n",
       "      <td>0.479138</td>\n",
       "      <td>0.634528</td>\n",
       "      <td>0.438733</td>\n",
       "      <td>0.644575</td>\n",
       "      <td>0.658146</td>\n",
       "      <td>0.570896</td>\n",
       "      <td>0.673738</td>\n",
       "      <td>0.644721</td>\n",
       "      <td>0.622284</td>\n",
       "      <td>0.650054</td>\n",
       "      <td>0.657240</td>\n",
       "      <td>0.649651</td>\n",
       "      <td>0.617253</td>\n",
       "      <td>0.633518</td>\n",
       "      <td>0.678736</td>\n",
       "      <td>0.640723</td>\n",
       "      <td>0.626097</td>\n",
       "      <td>0.660890</td>\n",
       "      <td>0.659276</td>\n",
       "      <td>0.596069</td>\n",
       "      <td>0.488429</td>\n",
       "      <td>0.642589</td>\n",
       "      <td>0.544192</td>\n",
       "      <td>0.654496</td>\n",
       "      <td>0.668004</td>\n",
       "      <td>0.651565</td>\n",
       "      <td>0.635511</td>\n",
       "      <td>0.665204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.695264</td>\n",
       "      <td>0.696179</td>\n",
       "      <td>0.666952</td>\n",
       "      <td>0.660854</td>\n",
       "      <td>0.605258</td>\n",
       "      <td>0.674529</td>\n",
       "      <td>0.692627</td>\n",
       "      <td>0.683897</td>\n",
       "      <td>0.668354</td>\n",
       "      <td>0.703089</td>\n",
       "      <td>0.695080</td>\n",
       "      <td>0.745182</td>\n",
       "      <td>0.634631</td>\n",
       "      <td>0.748011</td>\n",
       "      <td>0.670368</td>\n",
       "      <td>0.577994</td>\n",
       "      <td>0.701178</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.667374</td>\n",
       "      <td>0.558802</td>\n",
       "      <td>0.674363</td>\n",
       "      <td>0.679178</td>\n",
       "      <td>0.682172</td>\n",
       "      <td>0.700314</td>\n",
       "      <td>0.333345</td>\n",
       "      <td>0.526050</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.658798</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.665592</td>\n",
       "      <td>0.666499</td>\n",
       "      <td>0.655302</td>\n",
       "      <td>0.666243</td>\n",
       "      <td>0.675526</td>\n",
       "      <td>0.624573</td>\n",
       "      <td>0.529647</td>\n",
       "      <td>0.664947</td>\n",
       "      <td>0.680391</td>\n",
       "      <td>0.430322</td>\n",
       "      <td>0.675982</td>\n",
       "      <td>0.618464</td>\n",
       "      <td>0.420581</td>\n",
       "      <td>0.677061</td>\n",
       "      <td>0.666965</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.643053</td>\n",
       "      <td>0.659368</td>\n",
       "      <td>0.648527</td>\n",
       "      <td>0.541127</td>\n",
       "      <td>0.668646</td>\n",
       "      <td>0.655012</td>\n",
       "      <td>0.619187</td>\n",
       "      <td>0.625274</td>\n",
       "      <td>0.687504</td>\n",
       "      <td>0.660772</td>\n",
       "      <td>0.668024</td>\n",
       "      <td>0.695034</td>\n",
       "      <td>0.671332</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.629284</td>\n",
       "      <td>0.651715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.676816</td>\n",
       "      <td>0.674138</td>\n",
       "      <td>0.662224</td>\n",
       "      <td>0.676479</td>\n",
       "      <td>0.669706</td>\n",
       "      <td>0.521216</td>\n",
       "      <td>0.616124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710427</td>\n",
       "      <td>0.676100</td>\n",
       "      <td>0.667043</td>\n",
       "      <td>0.625860</td>\n",
       "      <td>0.662465</td>\n",
       "      <td>0.658025</td>\n",
       "      <td>0.606116</td>\n",
       "      <td>0.669995</td>\n",
       "      <td>0.675216</td>\n",
       "      <td>0.680425</td>\n",
       "      <td>0.668209</td>\n",
       "      <td>0.621743</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.653412</td>\n",
       "      <td>0.282553</td>\n",
       "      <td>0.675088</td>\n",
       "      <td>0.690446</td>\n",
       "      <td>0.666159</td>\n",
       "      <td>0.593359</td>\n",
       "      <td>0.654436</td>\n",
       "      <td>0.674147</td>\n",
       "      <td>0.659467</td>\n",
       "      <td>0.604614</td>\n",
       "      <td>0.495817</td>\n",
       "      <td>0.648343</td>\n",
       "      <td>0.681021</td>\n",
       "      <td>0.320220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.667831</td>\n",
       "      <td>0.629747</td>\n",
       "      <td>0.645590</td>\n",
       "      <td>0.694864</td>\n",
       "      <td>0.655448</td>\n",
       "      <td>0.666673</td>\n",
       "      <td>0.657315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>38730.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.722168</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.378639</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>4.380803</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.138034</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.246578</td>\n",
       "      <td>3.889856</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.516029</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.529987</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.356735</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.546892</td>\n",
       "      <td>3.082753</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.907361</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.472644</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.526900</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.151898</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>0.934364</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.396341</td>\n",
       "      <td>3.278443</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>1.432372</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.375330</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.422338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.303650</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.962323</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.885017</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.467284</td>\n",
       "      <td>0.958964</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.774054</td>\n",
       "      <td>3.407011</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.340873</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.218074</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.412414</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.554574</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.094941</td>\n",
       "      <td>3.723774</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.232846</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.237509</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.296033</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.311295</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.286598</td>\n",
       "      <td>2.749321</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.493450</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.531606</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.469164</td>\n",
       "      <td>2.796923</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.652442</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.286110</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.034810</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.327373</td>\n",
       "      <td>4.065857</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.586342</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.586193</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.597784</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.792883</td>\n",
       "      <td>2.011166</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.575721</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.807687</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.271568</td>\n",
       "      <td>4.154042</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.152379</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.987530</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.114105</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.143222</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.558523</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>1.586383</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.182370</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.496085</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.415187</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.922461</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.211506</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.215739</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.481788</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.113628</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.545055</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.426364</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.860577</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>1.395742</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>5.199338</td>\n",
       "      <td>3.585167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target            id     feature_1     feature_2     feature_3  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.021766  19365.000000     -0.011868     -0.022720      0.240066   \n",
       "std        0.145919  11180.820975      0.992397      0.991483      0.427129   \n",
       "min        0.000000      0.000000     -5.199338     -5.199338      0.000000   \n",
       "25%        0.000000   9682.500000     -0.681368     -0.693879      0.000000   \n",
       "50%        0.000000  19365.000000     -0.014384     -0.014327      0.000000   \n",
       "75%        0.000000  29047.500000      0.658456      0.651838      0.000000   \n",
       "max        1.000000  38730.000000      5.199338      5.199338      1.000000   \n",
       "\n",
       "          feature_4     feature_5     feature_6     feature_7     feature_8  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.014769      0.002543     -0.121256     -0.021874     -0.212568   \n",
       "std        1.006943      1.008098      0.983779      0.979137      0.935391   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.667434     -0.684292     -0.777239     -0.671211     -0.833827   \n",
       "50%        0.025820      0.005269     -0.166902     -0.021997     -0.207170   \n",
       "75%        0.706342      0.682534      0.566233      0.637608      0.407831   \n",
       "max        5.199338      5.199338      2.722168      5.199338      5.199338   \n",
       "\n",
       "          feature_9    feature_10    feature_11    feature_12    feature_13  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.030707      0.088301     -0.010807      0.176861     -0.012932   \n",
       "std        0.969471      0.369716      0.990354      0.449187      0.996109   \n",
       "min       -5.199338      0.000000     -5.199338      0.000000     -5.199338   \n",
       "25%       -0.675604      0.000000     -0.683985      0.000000     -0.697260   \n",
       "50%       -0.023462      0.000000     -0.009062      0.000000      0.004651   \n",
       "75%        0.630862      0.000000      0.670615      0.000000      0.662320   \n",
       "max        3.378639      2.000000      5.199338      2.000000      5.199338   \n",
       "\n",
       "         feature_14    feature_15    feature_16    feature_17    feature_18  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.124629      0.195528     -0.030446      3.549766     -0.037873   \n",
       "std        0.406550      0.396612      0.985939      0.966210      0.979483   \n",
       "min        0.000000      0.000000     -5.199338      1.000000     -5.199338   \n",
       "25%        0.000000      0.000000     -0.687860      4.000000     -0.707144   \n",
       "50%        0.000000      0.000000     -0.012138      4.000000     -0.022042   \n",
       "75%        0.000000      0.000000      0.637805      4.000000      0.625285   \n",
       "max        2.000000      1.000000      5.199338      4.000000      5.199338   \n",
       "\n",
       "         feature_19    feature_20    feature_21    feature_22    feature_23  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.003320      0.864759      0.145517     -0.080829      3.539645   \n",
       "std        1.002411      0.416098      0.424886      0.993560      0.972032   \n",
       "min       -5.199338      0.000000      0.000000     -5.199338      1.000000   \n",
       "25%       -0.666308      1.000000      0.000000     -0.758604      4.000000   \n",
       "50%        0.002220      1.000000      0.000000     -0.090774      4.000000   \n",
       "75%        0.694324      1.000000      0.000000      0.580801      4.000000   \n",
       "max        5.199338      2.000000      2.000000      5.199338      4.000000   \n",
       "\n",
       "         feature_24    feature_25    feature_26    feature_27    feature_28  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.000678      0.007249     -0.327361      0.000599     -0.007191   \n",
       "std        0.998825      1.003163      0.832599      0.999066      1.002357   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.677279     -0.681649     -0.840418     -0.665542     -0.698127   \n",
       "50%        0.008526      0.016427     -0.270955      0.000264      0.004550   \n",
       "75%        0.680930      0.690049      0.215828      0.672486      0.664352   \n",
       "max        5.199338      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "         feature_29    feature_30    feature_31    feature_32    feature_33  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.014676     -0.127118     -0.001103     -0.043747     -0.016969   \n",
       "std        1.005803      0.947260      0.997807      0.993147      1.004046   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.658682     -0.746514     -0.674718     -0.688303     -0.701995   \n",
       "50%        0.014306     -0.111424      0.004282     -0.038537     -0.007767   \n",
       "75%        0.701466      0.474503      0.659911      0.616066      0.658116   \n",
       "max        5.199338      5.199338      4.380803      5.199338      5.199338   \n",
       "\n",
       "         feature_34    feature_35    feature_36    feature_37    feature_38  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.145650     -0.003836     -0.173456     -0.010436     -0.023865   \n",
       "std        0.975752      1.001335      0.971011      0.989517      0.983823   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.772276     -0.683531     -0.824109     -0.674263     -0.683371   \n",
       "50%       -0.176931     -0.002277     -0.151748      0.013356     -0.008604   \n",
       "75%        0.488541      0.668743      0.483525      0.645433      0.649353   \n",
       "max        5.199338      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "         feature_39    feature_40    feature_41    feature_42    feature_43  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.013785     -0.136651     -0.006490     -0.222637      0.004805   \n",
       "std        1.012303      0.973383      0.992355      0.919967      0.996740   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -4.246225   \n",
       "25%       -0.682835     -0.776043     -0.671351     -0.824086     -0.666752   \n",
       "50%       -0.007453     -0.154888     -0.005520     -0.211537      0.016169   \n",
       "75%        0.664466      0.502314      0.667086      0.396409      0.682201   \n",
       "max        5.199338      5.199338      5.199338      3.138034      5.199338   \n",
       "\n",
       "         feature_44    feature_45    feature_46    feature_47    feature_48  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.013576     -0.001399     -0.001891     -0.273817      0.004304   \n",
       "std        1.001245      1.004859      1.000439      0.890477      1.011104   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.649290     -0.685386     -0.685574     -0.852023     -0.677065   \n",
       "50%        0.018603     -0.011422      0.004631     -0.258583      0.008900   \n",
       "75%        0.687998      0.676576      0.682372      0.312719      0.679234   \n",
       "max        5.199338      5.199338      3.246578      3.889856      5.199338   \n",
       "\n",
       "         feature_49    feature_50    feature_51    feature_52    feature_53  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.017616      0.003360     -0.048813     -0.017145     -0.013058   \n",
       "std        1.017203      1.007769      0.993191      0.985656      0.989085   \n",
       "min       -5.199338     -3.699710     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.668913     -0.674211     -0.716812     -0.677853     -0.680538   \n",
       "50%        0.011099      0.013609     -0.042106     -0.022408     -0.002108   \n",
       "75%        0.706864      0.676778      0.637295      0.639296      0.662326   \n",
       "max        5.199338      5.199338      2.516029      5.199338      5.199338   \n",
       "\n",
       "         feature_54    feature_55    feature_56    feature_57    feature_58  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.228990     -0.186734     -0.017106      0.001745     -0.029327   \n",
       "std        0.482433      0.961433      0.996867      0.992175      0.976373   \n",
       "min        0.000000     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%        0.000000     -0.829336     -0.702029     -0.677652     -0.683259   \n",
       "50%        0.000000     -0.185143     -0.009547     -0.004178     -0.028918   \n",
       "75%        0.000000      0.460671      0.658355      0.679361      0.623000   \n",
       "max        2.000000      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "         feature_59    feature_60    feature_61    feature_62    feature_63  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.001139      0.442514     -0.024042     -0.105719     -0.021940   \n",
       "std        1.006227      0.550350      0.980346      0.980395      0.980126   \n",
       "min       -5.199338      0.000000     -3.106805     -5.199338     -5.199338   \n",
       "25%       -0.682331      0.000000     -0.673798     -0.768800     -0.663962   \n",
       "50%       -0.006192      0.000000     -0.014412     -0.108404     -0.026349   \n",
       "75%        0.683725      1.000000      0.636080      0.560209      0.636567   \n",
       "max        3.529987      2.000000      5.199338      5.199338      5.199338   \n",
       "\n",
       "         feature_64    feature_65    feature_66    feature_67    feature_68  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.027399     -0.011325      0.004819     -0.031736     -0.033972   \n",
       "std        1.000793      1.002165      0.983914      0.986701      1.000378   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.644688     -0.670600     -0.667435     -0.695625     -0.696346   \n",
       "50%        0.020259     -0.010423      0.010058     -0.030827     -0.067367   \n",
       "75%        0.700650      0.656935      0.673629      0.614965      0.617835   \n",
       "max        5.199338      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "         feature_69    feature_70    feature_71    feature_72    feature_73  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.010006      0.421704     -0.021387      0.005190      0.016982   \n",
       "std        1.000891      0.547777      0.996521      1.000203      0.992206   \n",
       "min       -5.199338      0.000000     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.679138      0.000000     -0.696442     -0.654223     -0.648411   \n",
       "50%       -0.015607      0.000000     -0.017485      0.006073      0.022797   \n",
       "75%        0.659814      1.000000      0.658252      0.678299      0.683622   \n",
       "max        5.199338      2.000000      5.199338      5.199338      5.199338   \n",
       "\n",
       "         feature_74    feature_75    feature_76    feature_77    feature_78  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.001517      0.020074     -0.004565     -0.011091     -0.082769   \n",
       "std        1.001248      0.997150      1.002456      0.994680      0.968496   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.668647     -0.639314     -0.681414     -0.677848     -0.732940   \n",
       "50%       -0.004590      0.020412      0.001790     -0.001545     -0.068004   \n",
       "75%        0.704619      0.689569      0.671516      0.649946      0.562425   \n",
       "max        5.199338      5.199338      5.199338      3.356735      5.199338   \n",
       "\n",
       "         feature_79    feature_80    feature_81    feature_82    feature_83  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.009481      0.003194     -0.051269     -0.134871     -0.148081   \n",
       "std        0.997710      0.993169      1.009186      0.894759      0.870817   \n",
       "min       -5.199338     -3.895424     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.648078     -0.673144     -0.734181     -0.660248     -0.657845   \n",
       "50%        0.004595     -0.001109     -0.078867     -0.145386     -0.143250   \n",
       "75%        0.689526      0.669840      0.602288      0.410799      0.386432   \n",
       "max        5.199338      5.199338      5.199338      3.546892      3.082753   \n",
       "\n",
       "         feature_84    feature_85    feature_86    feature_87    feature_88  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.035728      0.005096     -0.003726     -0.003361      0.002134   \n",
       "std        0.983705      0.987302      1.000906      0.995161      1.013218   \n",
       "min       -5.199338     -5.199338     -5.199338     -3.816555     -5.199338   \n",
       "25%       -0.695686     -0.665292     -0.675586     -0.679654     -0.691353   \n",
       "50%       -0.024907      0.015646      0.007094      0.001654     -0.005236   \n",
       "75%        0.642403      0.668708      0.663565      0.657187      0.686378   \n",
       "max        5.199338      5.199338      5.199338      3.907361      5.199338   \n",
       "\n",
       "         feature_89    feature_90    feature_91    feature_92    feature_93  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.004338     -0.017810     -0.049131      5.446774     -0.002723   \n",
       "std        1.007517      0.987659      0.982863      2.434343      0.992308   \n",
       "min       -5.199338     -5.199338     -5.199338      1.000000     -5.199338   \n",
       "25%       -0.663678     -0.669272     -0.704815      3.000000     -0.685645   \n",
       "50%        0.001045     -0.006539     -0.034390      7.000000      0.006652   \n",
       "75%        0.679482      0.643038      0.618040      7.000000      0.668608   \n",
       "max        5.199338      5.199338      5.199338      7.000000      5.199338   \n",
       "\n",
       "         feature_94    feature_95    feature_96    feature_97    feature_98  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.007040     -0.032440     -0.005845     -0.016907      0.718133   \n",
       "std        1.010192      0.981436      1.001578      1.041409      0.508534   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338      0.000000   \n",
       "25%       -0.682386     -0.698412     -0.686526     -0.706113      0.000000   \n",
       "50%        0.022871     -0.007924      0.007820     -0.065948      1.000000   \n",
       "75%        0.703017      0.623359      0.674070      0.632564      1.000000   \n",
       "max        3.472644      5.199338      5.199338      5.199338      2.000000   \n",
       "\n",
       "         feature_99   feature_100   feature_101   feature_102   feature_103  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.002397      0.009108      2.498748     -0.013882     -0.011444   \n",
       "std        0.998113      0.992752      0.852219      1.001484      0.998365   \n",
       "min       -5.199338     -5.199338      1.000000     -5.199338     -5.199338   \n",
       "25%       -0.656090     -0.652237      2.000000     -0.686674     -0.687176   \n",
       "50%       -0.005004      0.018982      3.000000     -0.011705     -0.019005   \n",
       "75%        0.661227      0.668705      3.000000      0.672801      0.683639   \n",
       "max        5.199338      5.199338      3.000000      5.199338      5.199338   \n",
       "\n",
       "        feature_104   feature_105   feature_106   feature_107   feature_108  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.037351     -0.010772     -0.022142      0.012548      0.008680   \n",
       "std        0.996220      0.994031      1.002583      1.010490      1.000947   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.709406     -0.680050     -0.695799     -0.665555     -0.664379   \n",
       "50%       -0.026268      0.005873     -0.014663      0.020830      0.001485   \n",
       "75%        0.628867      0.655540      0.655558      0.675666      0.677406   \n",
       "max        5.199338      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_109   feature_110   feature_111   feature_112   feature_113  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.003365      0.019836     -0.068095     -0.004531     -0.049030   \n",
       "std        0.996851      0.999134      0.996913      0.988429      0.991727   \n",
       "min       -5.199338     -5.199338     -5.199338     -3.664452     -5.199338   \n",
       "25%       -0.669893     -0.651534     -0.733940     -0.666988     -0.707228   \n",
       "50%       -0.002566      0.022570     -0.057819      0.023388     -0.043273   \n",
       "75%        0.675147      0.697021      0.607188      0.663034      0.605553   \n",
       "max        5.199338      3.526900      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_114   feature_115   feature_116   feature_117   feature_118  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.024292     -0.052030     -0.015779     -0.006686     -0.390342   \n",
       "std        0.984628      0.979060      0.973703      1.005697      0.838839   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.687310     -0.702785     -0.675715     -0.684830     -0.964990   \n",
       "50%       -0.015105     -0.046913     -0.019429     -0.011355     -0.433357   \n",
       "75%        0.640443      0.599634      0.652679      0.674749      0.345654   \n",
       "max        5.199338      5.199338      3.151898      5.199338      0.934364   \n",
       "\n",
       "        feature_119   feature_120   feature_121   feature_122   feature_123  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.000997     -0.030654      0.000988      0.003359     -0.024875   \n",
       "std        0.994783      0.963788      1.003323      0.999307      1.000240   \n",
       "min       -5.199338     -3.063283     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.664721     -0.683806     -0.669378     -0.682840     -0.692777   \n",
       "50%        0.008834     -0.019820      0.002444      0.006878     -0.035902   \n",
       "75%        0.678847      0.626354      0.691999      0.693373      0.656083   \n",
       "max        5.199338      3.396341      3.278443      5.199338      5.199338   \n",
       "\n",
       "        feature_124   feature_125   feature_126   feature_127   feature_128  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.027182     -0.335467      3.548527     -0.121321     -0.004478   \n",
       "std        0.993393      0.802468      1.064964      0.965830      0.990939   \n",
       "min       -5.199338     -5.199338      1.000000     -5.199338     -5.199338   \n",
       "25%       -0.664108     -0.842510      4.000000     -0.761843     -0.669571   \n",
       "50%        0.036334     -0.252741      4.000000     -0.127674      0.004623   \n",
       "75%        0.689854      0.268594      4.000000      0.492650      0.666030   \n",
       "max        5.199338      1.432372      4.000000      5.199338      5.199338   \n",
       "\n",
       "        feature_129   feature_130   feature_131   feature_132   feature_133  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.036363      0.004969     -0.021047     -0.047094     -0.067346   \n",
       "std        0.969320      0.999358      0.989305      0.940471      0.926679   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -3.347012   \n",
       "25%       -0.679850     -0.667447     -0.681983     -0.681745     -0.699874   \n",
       "50%       -0.039692      0.005205     -0.023360     -0.026835     -0.073093   \n",
       "75%        0.628626      0.685006      0.623871      0.621807      0.525681   \n",
       "max        5.199338      5.199338      5.199338      3.375330      5.199338   \n",
       "\n",
       "        feature_134   feature_135   feature_136   feature_137   feature_138  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.069472      6.594149     -0.061260      0.207792      0.001187   \n",
       "std        1.006279      2.749426      0.986271      0.469896      0.999841   \n",
       "min       -5.199338      1.000000     -5.199338      0.000000     -5.199338   \n",
       "25%       -0.743448      8.000000     -0.729382      0.000000     -0.680508   \n",
       "50%       -0.075327      8.000000     -0.065016      0.000000     -0.001668   \n",
       "75%        0.595708      8.000000      0.603789      0.000000      0.686235   \n",
       "max        5.199338      9.000000      5.199338      2.000000      5.199338   \n",
       "\n",
       "        feature_139   feature_140   feature_141   feature_142   feature_143  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.008483     -0.094022      0.014289      0.096073     -0.011467   \n",
       "std        1.000491      0.965318      0.994411      0.378192      1.000449   \n",
       "min       -5.199338     -5.199338     -5.199338      0.000000     -5.199338   \n",
       "25%       -0.680576     -0.733024     -0.665368      0.000000     -0.680736   \n",
       "50%       -0.002098     -0.079461      0.013777      0.000000     -0.003127   \n",
       "75%        0.651036      0.528690      0.695480      0.000000      0.653102   \n",
       "max        5.199338      5.199338      5.199338      2.000000      5.199338   \n",
       "\n",
       "        feature_144   feature_145   feature_146   feature_147   feature_148  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.017763     -0.000097     -0.004232     -0.010737     -0.057190   \n",
       "std        0.997637      0.988987      1.002654      1.019872      0.963904   \n",
       "min       -5.199338     -3.307811     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.694943     -0.665731     -0.676426     -0.709279     -0.699680   \n",
       "50%       -0.015204      0.006867     -0.008749     -0.015627     -0.053359   \n",
       "75%        0.654872      0.661487      0.683811      0.678691      0.606983   \n",
       "max        5.199338      5.199338      5.199338      5.199338      3.422338   \n",
       "\n",
       "        feature_149   feature_150   feature_151   feature_152   feature_153  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.005645      0.000232     -0.038901     -0.003432      0.626604   \n",
       "std        1.005212      1.010884      0.971834      1.001832      0.708510   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338      0.000000   \n",
       "25%       -0.686066     -0.684163     -0.687280     -0.677873      0.000000   \n",
       "50%       -0.005095     -0.002006     -0.009693      0.011535      0.000000   \n",
       "75%        0.678720      0.674997      0.630973      0.679704      1.000000   \n",
       "max        5.199338      5.199338      5.199338      3.303650      2.000000   \n",
       "\n",
       "        feature_154   feature_155   feature_156   feature_157   feature_158  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.003440     -0.101662     -0.047708     -0.018177     -0.008492   \n",
       "std        1.020318      0.963762      0.956425      0.988251      1.007900   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.694038     -0.737563     -0.689729     -0.695203     -0.693250   \n",
       "50%        0.007680     -0.084080     -0.027815     -0.014082      0.003669   \n",
       "75%        0.694811      0.521614      0.608245      0.654434      0.662826   \n",
       "max        3.962323      5.199338      2.885017      5.199338      5.199338   \n",
       "\n",
       "        feature_159   feature_160   feature_161   feature_162   feature_163  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.001503     -0.015921     -0.003541     -0.357884     -0.050297   \n",
       "std        1.007781      1.003698      0.993133      0.768790      0.973345   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.666674     -0.679412     -0.671704     -0.863241     -0.695036   \n",
       "50%        0.001074     -0.023392      0.003424     -0.259467     -0.042626   \n",
       "75%        0.678928      0.657116      0.676457      0.250686      0.600243   \n",
       "max        5.199338      5.199338      3.467284      0.958964      5.199338   \n",
       "\n",
       "        feature_164   feature_165   feature_166   feature_167   feature_168  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.017768     -0.083280     -0.000228     -0.027420     -0.010116   \n",
       "std        1.006343      1.010419      0.997486      0.974750      1.004116   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -3.954366   \n",
       "25%       -0.671222     -0.754554     -0.686435     -0.673865     -0.681113   \n",
       "50%        0.046825     -0.081586     -0.007873     -0.023128     -0.001343   \n",
       "75%        0.723665      0.601225      0.664304      0.628198      0.674098   \n",
       "max        5.199338      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_169   feature_170   feature_171   feature_172   feature_173  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.012896     -0.022193     -0.029590     -0.023098     -0.004230   \n",
       "std        0.998500      0.969716      1.004038      0.998939      0.989670   \n",
       "min       -5.199338     -5.199338     -3.507918     -5.199338     -5.199338   \n",
       "25%       -0.682271     -0.661535     -0.706800     -0.686145     -0.680943   \n",
       "50%       -0.005731      0.008404     -0.037789     -0.018052     -0.004971   \n",
       "75%        0.651928      0.648462      0.623204      0.646286      0.665715   \n",
       "max        5.199338      5.199338      5.199338      3.774054      3.407011   \n",
       "\n",
       "        feature_174   feature_175   feature_176   feature_177   feature_178  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.002590     -0.047705     -0.137179     -0.003686     -0.027732   \n",
       "std        1.001077      0.993375      0.994197      0.992879      0.992047   \n",
       "min       -5.199338     -3.554695     -5.199338     -5.199338     -3.758900   \n",
       "25%       -0.666125     -0.718780     -0.787404     -0.668847     -0.691923   \n",
       "50%       -0.020749     -0.074985     -0.183422      0.003678     -0.018222   \n",
       "75%        0.682493      0.606861      0.458604      0.669172      0.635909   \n",
       "max        5.199338      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_179   feature_180   feature_181   feature_182   feature_183  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.507526     -0.009889     -0.001583      0.002106     -0.069000   \n",
       "std        0.553293      0.984117      1.006338      0.996376      0.987119   \n",
       "min        0.000000     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%        0.000000     -0.666620     -0.682406     -0.669836     -0.736730   \n",
       "50%        0.000000     -0.019816      0.002786      0.003916     -0.081696   \n",
       "75%        1.000000      0.652128      0.688771      0.673223      0.598993   \n",
       "max        2.000000      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_184   feature_185   feature_186   feature_187   feature_188  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.002231      0.060124     -0.002185      0.212904     -0.050094   \n",
       "std        0.985511      1.017893      0.996154      0.473037      0.980680   \n",
       "min       -5.199338     -5.199338     -5.199338      0.000000     -5.199338   \n",
       "25%       -0.672787     -0.634107     -0.673962      0.000000     -0.700640   \n",
       "50%        0.005974      0.052789     -0.005165      0.000000     -0.028304   \n",
       "75%        0.665609      0.747343      0.654492      0.000000      0.617916   \n",
       "max        5.199338      3.340873      5.199338      2.000000      5.199338   \n",
       "\n",
       "        feature_189   feature_190   feature_191   feature_192   feature_193  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.060483     -0.068219      0.027051     -0.015950     -0.026602   \n",
       "std        0.961801      0.921777      0.995268      0.994609      0.976283   \n",
       "min       -5.199338     -3.363841     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.690815     -0.707112     -0.644546     -0.680162     -0.699703   \n",
       "50%       -0.034562     -0.081004      0.027624     -0.021278     -0.021185   \n",
       "75%        0.591478      0.537881      0.700512      0.648235      0.644586   \n",
       "max        5.199338      5.199338      5.199338      3.218074      5.199338   \n",
       "\n",
       "        feature_194   feature_195   feature_196   feature_197   feature_198  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.001455      0.003725     -0.351320     -0.130240      0.557409   \n",
       "std        1.008024      0.998294      0.812506      0.973214      0.550358   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338      0.000000   \n",
       "25%       -0.672659     -0.674841     -0.859872     -0.765482      0.000000   \n",
       "50%        0.003208      0.003287     -0.292288     -0.126179      1.000000   \n",
       "75%        0.670970      0.676562      0.198561      0.477222      1.000000   \n",
       "max        5.199338      5.199338      2.412414      5.199338      2.000000   \n",
       "\n",
       "        feature_199   feature_200   feature_201   feature_202   feature_203  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.009460      0.000994      0.001052     -0.020191     -0.004877   \n",
       "std        1.001399      0.991148      1.012044      0.996296      0.987231   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.651846     -0.673189     -0.671716     -0.684371     -0.668645   \n",
       "50%        0.006842      0.008660     -0.007775     -0.031789      0.001738   \n",
       "75%        0.682903      0.665349      0.683534      0.654502      0.676629   \n",
       "max        5.199338      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_204   feature_205   feature_206   feature_207   feature_208  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.040392     -0.115125     -0.221730     -0.035877     -0.005566   \n",
       "std        0.982867      0.935318      1.024348      0.981076      0.998122   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.698100     -0.740687     -0.928778     -0.702428     -0.671008   \n",
       "50%       -0.050626     -0.089729     -0.259739     -0.026123      0.000299   \n",
       "75%        0.610618      0.550888      0.461788      0.624207      0.670523   \n",
       "max        5.199338      3.554574      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_209   feature_210   feature_211   feature_212   feature_213  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.001480      0.012166     -0.010566      0.004593     -0.065117   \n",
       "std        0.983445      0.998103      0.987238      1.002174      0.962967   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.665760     -0.652822     -0.675162     -0.677545     -0.711591   \n",
       "50%       -0.000916      0.014279     -0.009614      0.003161     -0.053874   \n",
       "75%        0.667467      0.664712      0.654054      0.687883      0.567744   \n",
       "max        5.199338      3.094941      3.723774      5.199338      5.199338   \n",
       "\n",
       "        feature_214   feature_215   feature_216   feature_217   feature_218  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.165825      0.016577     -0.027902     -0.004799     -0.006482   \n",
       "std        0.952312      1.018924      0.979999      1.007037      1.001236   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.792625     -0.675707     -0.700641     -0.683357     -0.681463   \n",
       "50%       -0.165923      0.024050     -0.027535     -0.009086     -0.012283   \n",
       "75%        0.471451      0.715944      0.630760      0.663159      0.667628   \n",
       "max        3.232846      5.199338      5.199338      5.199338      3.237509   \n",
       "\n",
       "        feature_219   feature_220   feature_221   feature_222   feature_223  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.013254     -0.002309     -0.010962     -0.017173     -0.095189   \n",
       "std        0.996903      0.989752      0.992397      1.007126      0.942358   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.654180     -0.671422     -0.688828     -0.698988     -0.712954   \n",
       "50%        0.009074     -0.001724     -0.008198     -0.008406     -0.067569   \n",
       "75%        0.676326      0.655871      0.666384      0.665252      0.569637   \n",
       "max        5.199338      5.199338      5.199338      5.199338      2.296033   \n",
       "\n",
       "        feature_224   feature_225   feature_226   feature_227   feature_228  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.054226     -0.031292     -0.022812     -0.035788     -0.015487   \n",
       "std        0.984854      0.988344      0.975125      0.976284      1.000509   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.702600     -0.694788     -0.679224     -0.702850     -0.704846   \n",
       "50%       -0.045307     -0.029990     -0.023578     -0.028460     -0.026933   \n",
       "75%        0.605758      0.645956      0.641528      0.643243      0.670830   \n",
       "max        5.199338      5.199338      5.199338      3.311295      5.199338   \n",
       "\n",
       "        feature_229   feature_230   feature_231   feature_232   feature_233  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.018312      0.003328     -0.038534     -0.114466     -0.001762   \n",
       "std        1.006309      0.994555      0.961533      0.964006      0.988547   \n",
       "min       -5.199338     -5.199338     -3.454976     -5.199338     -5.199338   \n",
       "25%       -0.655318     -0.664692     -0.689651     -0.755472     -0.672518   \n",
       "50%        0.011267      0.004450     -0.028038     -0.107425     -0.006555   \n",
       "75%        0.712132      0.681980      0.628111      0.505471      0.680817   \n",
       "max        5.199338      5.199338      3.286598      2.749321      5.199338   \n",
       "\n",
       "        feature_234   feature_235   feature_236   feature_237   feature_238  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.043924      0.000164     -0.019787      0.018997      0.014188   \n",
       "std        0.958328      1.008769      0.990466      0.995682      0.995782   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.692781     -0.677945     -0.684627     -0.668330     -0.650276   \n",
       "50%       -0.032586      0.005794     -0.022614      0.016027      0.013607   \n",
       "75%        0.613243      0.668326      0.633589      0.708499      0.680368   \n",
       "max        5.199338      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_239   feature_240   feature_241   feature_242   feature_243  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.015456     -0.015277      0.005944      0.642095     -0.027985   \n",
       "std        1.008465      1.005719      1.003312      0.534788      0.981177   \n",
       "min       -5.199338     -5.199338     -5.199338      0.000000     -5.199338   \n",
       "25%       -0.687876     -0.693796     -0.678622      0.000000     -0.692234   \n",
       "50%       -0.012243     -0.012823      0.007481      1.000000     -0.023905   \n",
       "75%        0.666718      0.656913      0.700025      1.000000      0.636916   \n",
       "max        5.199338      5.199338      3.493450      2.000000      5.199338   \n",
       "\n",
       "        feature_244   feature_245   feature_246   feature_247   feature_248  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.037128     -0.035251     -0.050988     -0.354791     -0.036398   \n",
       "std        0.211630      0.988520      0.969509      0.815174      0.963146   \n",
       "min        0.000000     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%        0.000000     -0.704182     -0.688298     -0.870681     -0.687415   \n",
       "50%        0.000000     -0.023215     -0.035255     -0.291406     -0.016990   \n",
       "75%        0.000000      0.642102      0.614827      0.183542      0.630084   \n",
       "max        2.000000      3.531606      5.199338      2.469164      2.796923   \n",
       "\n",
       "        feature_249   feature_250   feature_251   feature_252   feature_253  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.129943     -0.070364     -0.020606     -0.141588     -0.003403   \n",
       "std        0.985389      0.942090      0.991724      0.980765      0.998303   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.788120     -0.703509     -0.695951     -0.778082     -0.665238   \n",
       "50%       -0.141078     -0.052883     -0.018266     -0.150063     -0.009604   \n",
       "75%        0.508163      0.578725      0.646214      0.504279      0.665809   \n",
       "max        5.199338      2.652442      5.199338      5.199338      3.286110   \n",
       "\n",
       "        feature_254   feature_255   feature_256   feature_257   feature_258  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.066228      0.475123     -0.066319     -0.040016     -0.048797   \n",
       "std        1.009155      0.552785      0.987164      0.979085      0.981986   \n",
       "min       -5.199338      0.000000     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.741956      0.000000     -0.737534     -0.700711     -0.699503   \n",
       "50%       -0.090001      0.000000     -0.073471     -0.031648     -0.036504   \n",
       "75%        0.589700      1.000000      0.622924      0.631730      0.619547   \n",
       "max        5.199338      2.000000      3.034810      5.199338      5.199338   \n",
       "\n",
       "        feature_259   feature_260   feature_261   feature_262   feature_263  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.062881      0.004915      0.006066      0.011512     -0.015177   \n",
       "std        0.950930      0.986240      0.999074      1.008231      0.973951   \n",
       "min       -5.199338     -4.091272     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.702058     -0.666210     -0.668001     -0.665963     -0.686670   \n",
       "50%       -0.043174     -0.004844     -0.003647      0.000919      0.012980   \n",
       "75%        0.584028      0.672474      0.671524      0.692412      0.652217   \n",
       "max        5.199338      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_264   feature_265   feature_266   feature_267   feature_268  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.009057     -0.126460     -0.001341     -0.024550     -0.006658   \n",
       "std        0.999800      0.953235      1.012329      1.000717      1.001245   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.676353     -0.764734     -0.676452     -0.705631     -0.680850   \n",
       "50%       -0.017978     -0.110067      0.008700     -0.002493     -0.002289   \n",
       "75%        0.674628      0.482589      0.680090      0.652700      0.659263   \n",
       "max        5.199338      2.327373      4.065857      5.199338      5.199338   \n",
       "\n",
       "        feature_269   feature_270   feature_271   feature_272   feature_273  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.014437      1.507526      0.003270      0.003211     -0.083650   \n",
       "std        1.008392      0.499950      1.001111      0.997189      0.941418   \n",
       "min       -5.199338      1.000000     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.678844      1.000000     -0.684928     -0.672167     -0.725791   \n",
       "50%       -0.021363      2.000000      0.018471      0.001874     -0.046470   \n",
       "75%        0.667734      2.000000      0.674463      0.677322      0.586387   \n",
       "max        5.199338      2.000000      3.586342      5.199338      2.586193   \n",
       "\n",
       "        feature_274   feature_275   feature_276   feature_277   feature_278  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.009141     -0.003054      0.025614     -0.028128     -0.058274   \n",
       "std        1.005858      0.995830      1.015338      0.973696      0.979158   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.676952     -0.687317     -0.661192     -0.669826     -0.709711   \n",
       "50%       -0.008815      0.002568      0.030858     -0.037564     -0.069100   \n",
       "75%        0.680060      0.663287      0.715003      0.619177      0.608503   \n",
       "max        5.199338      5.199338      5.199338      5.199338      2.597784   \n",
       "\n",
       "        feature_279   feature_280   feature_281   feature_282   feature_283  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.040961     -0.064159     -0.017375     -0.017064     -0.169013   \n",
       "std        0.978589      0.986692      0.981022      1.002441      1.009822   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.684039     -0.722082     -0.677184     -0.682438     -0.847536   \n",
       "50%       -0.025482     -0.063273     -0.003838     -0.021937     -0.194608   \n",
       "75%        0.618294      0.590665      0.642173      0.661831      0.494695   \n",
       "max        5.199338      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_284   feature_285   feature_286   feature_287   feature_288  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.131256     -0.030752     -0.194364     -0.023399      0.004054   \n",
       "std        0.955564      0.993839      0.883720      0.974794      1.004765   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.764480     -0.706234     -0.763601     -0.674194     -0.664689   \n",
       "50%       -0.116781     -0.024802     -0.144874     -0.010997      0.004684   \n",
       "75%        0.479138      0.634528      0.438733      0.644575      0.658146   \n",
       "max        5.199338      3.792883      2.011166      5.199338      5.199338   \n",
       "\n",
       "        feature_289   feature_290   feature_291   feature_292   feature_293  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.076873     -0.011184     -0.039726     -0.037295     -0.019333   \n",
       "std        0.949082      1.010704      0.981931      0.993659      0.987267   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.715565     -0.694508     -0.692439     -0.703734     -0.678792   \n",
       "50%       -0.059995     -0.009451     -0.035380     -0.023512     -0.016083   \n",
       "75%        0.570896      0.673738      0.644721      0.622284      0.650054   \n",
       "max        2.575721      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_294   feature_295   feature_296   feature_297   feature_298  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.009030     -0.011395     -0.038294     -0.025584      0.011510   \n",
       "std        0.993729      0.992823      0.977005      0.994534      0.996293   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.675102     -0.673751     -0.684357     -0.684677     -0.665803   \n",
       "50%       -0.012573     -0.013839     -0.027744     -0.023230      0.009442   \n",
       "75%        0.657240      0.649651      0.617253      0.633518      0.678736   \n",
       "max        5.199338      3.807687      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_299   feature_300   feature_301   feature_302   feature_303  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.021345     -0.028281     -0.023140     -0.013921     -0.047297   \n",
       "std        0.987515      0.972254      1.008084      1.004228      0.996216   \n",
       "min       -5.199338     -3.443351     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.691151     -0.694493     -0.694056     -0.673886     -0.705164   \n",
       "50%        0.012055     -0.041937     -0.020363      0.002564     -0.055656   \n",
       "75%        0.640723      0.626097      0.660890      0.659276      0.596069   \n",
       "max        5.199338      5.199338      5.199338      3.271568      4.154042   \n",
       "\n",
       "        feature_304   feature_305   feature_306   feature_307   feature_308  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.123179     -0.034168     -0.049474     -0.011567      0.000201   \n",
       "std        0.966094      0.997272      0.920429      0.962991      0.996193   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.760023     -0.693051     -0.677476     -0.664614     -0.667957   \n",
       "50%       -0.124352     -0.034034     -0.059405      0.034815      0.001750   \n",
       "75%        0.488429      0.642589      0.544192      0.654496      0.668004   \n",
       "max        5.199338      3.152379      5.199338      3.987530      5.199338   \n",
       "\n",
       "        feature_309   feature_310   feature_311   feature_312   feature_313  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.001719     -0.019460     -0.004924      0.115024      0.008211   \n",
       "std        0.973503      0.984185      0.996036      0.397467      1.003202   \n",
       "min       -5.199338     -5.199338     -5.199338      0.000000     -5.199338   \n",
       "25%       -0.642705     -0.692741     -0.683254      0.000000     -0.677226   \n",
       "50%        0.006415     -0.014600     -0.013987      0.000000      0.005269   \n",
       "75%        0.651565      0.635511      0.665204      0.000000      0.695264   \n",
       "max        5.199338      5.199338      5.199338      2.000000      5.199338   \n",
       "\n",
       "        feature_314   feature_315   feature_316   feature_317   feature_318  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.009287     -0.011622     -0.017755     -0.048480      0.005836   \n",
       "std        1.004332      0.994374      0.995451      0.976371      1.004723   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.662146     -0.685323     -0.696603     -0.697441     -0.669281   \n",
       "50%        0.003827     -0.008253     -0.026796     -0.037694      0.014581   \n",
       "75%        0.696179      0.666952      0.660854      0.605258      0.674529   \n",
       "max        5.199338      5.199338      3.114105      5.199338      5.199338   \n",
       "\n",
       "        feature_319   feature_320   feature_321   feature_322   feature_323  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.014995      0.000284     -0.004223      0.003681      0.028351   \n",
       "std        1.007827      1.006622      0.996736      1.016241      0.997840   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.680172     -0.679450     -0.666772     -0.697591     -0.640673   \n",
       "50%        0.018563      0.005215     -0.008423     -0.009215      0.032356   \n",
       "75%        0.692627      0.683897      0.668354      0.703089      0.695080   \n",
       "max        5.199338      5.199338      3.143222      5.199338      5.199338   \n",
       "\n",
       "        feature_324   feature_325   feature_326   feature_327   feature_328  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.030415     -0.035701      0.016932      0.014264     -0.057312   \n",
       "std        1.018261      0.997349      1.098387      0.991033      0.990035   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.665611     -0.703837     -0.711522     -0.652252     -0.697612   \n",
       "50%        0.020918     -0.046228      0.001567      0.024651     -0.050827   \n",
       "75%        0.745182      0.634631      0.748011      0.670368      0.577994   \n",
       "max        5.199338      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_329   feature_330   feature_331   feature_332   feature_333  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.024448      2.470579     -0.003280     -0.072037     -0.013049   \n",
       "std        0.997974      0.880626      0.982568      0.959784      1.008039   \n",
       "min       -5.199338      1.000000     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.660348      1.000000     -0.670270     -0.704174     -0.697900   \n",
       "50%        0.036272      3.000000     -0.000394     -0.058590     -0.008775   \n",
       "75%        0.701178      3.000000      0.667374      0.558802      0.674363   \n",
       "max        5.199338      3.000000      5.199338      5.199338      3.558523   \n",
       "\n",
       "        feature_334   feature_335   feature_336   feature_337   feature_338  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.005417      0.009747      0.018040     -0.287418     -0.152220   \n",
       "std        1.009115      1.004385      1.000011      0.823351      1.015030   \n",
       "min       -3.652193     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.671823     -0.679616     -0.655450     -0.833933     -0.851046   \n",
       "50%        0.015762      0.005465      0.010772     -0.215250     -0.155190   \n",
       "75%        0.679178      0.682172      0.700314      0.333345      0.526050   \n",
       "max        5.199338      5.199338      5.199338      1.586383      5.199338   \n",
       "\n",
       "        feature_339   feature_340   feature_341   feature_342   feature_343  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       5.282409     -0.015855      3.505280      0.690067     -0.009318   \n",
       "std        2.824001      0.995911      1.001002      0.519677      0.988505   \n",
       "min        1.000000     -5.199338      1.000000      0.000000     -5.199338   \n",
       "25%        4.000000     -0.687613      4.000000      0.000000     -0.676489   \n",
       "50%        6.000000     -0.008881      4.000000      1.000000      0.007242   \n",
       "75%        6.000000      0.658798      4.000000      1.000000      0.665592   \n",
       "max       17.000000      5.199338      5.000000      2.000000      3.182370   \n",
       "\n",
       "        feature_344   feature_345   feature_346   feature_347   feature_348  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.005876     -0.010761     -0.001728     -0.005082     -0.030485   \n",
       "std        0.976553      0.979663      0.989250      0.984307      0.979009   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.664887     -0.672611     -0.667491     -0.677543     -0.685926   \n",
       "50%       -0.004794     -0.006529     -0.002665      0.017862     -0.026158   \n",
       "75%        0.666499      0.655302      0.666243      0.675526      0.624573   \n",
       "max        5.199338      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_349   feature_350   feature_351   feature_352   feature_353  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.087999     -0.000902      0.005475     -0.170507      0.011570   \n",
       "std        0.957222      0.990357      1.008122      0.934415      0.995079   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.722432     -0.668427     -0.659671     -0.783459     -0.671197   \n",
       "50%       -0.078754      0.003358      0.013152     -0.168200      0.010623   \n",
       "75%        0.529647      0.664947      0.680391      0.430322      0.675982   \n",
       "max        5.199338      5.199338      5.199338      3.496085      5.199338   \n",
       "\n",
       "        feature_354   feature_355   feature_356   feature_357   feature_358  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.034570     -0.223282      0.010082     -0.018277     -0.049603   \n",
       "std        0.968707      0.933686      1.006688      0.987008      0.979828   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.698205     -0.844036     -0.664671     -0.684689     -0.713787   \n",
       "50%       -0.013932     -0.195286      0.007497     -0.008879     -0.063387   \n",
       "75%        0.618464      0.420581      0.677061      0.666965      0.618182   \n",
       "max        5.199338      3.415187      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_359   feature_360   feature_361   feature_362   feature_363  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.027510      0.005554     -0.028320     -0.119015      0.005350   \n",
       "std        0.999691      1.004492      0.978963      0.984289      0.990183   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.692382     -0.650802     -0.695107     -0.773901     -0.655925   \n",
       "50%       -0.018019      0.006949     -0.028692     -0.162282      0.005570   \n",
       "75%        0.643053      0.659368      0.648527      0.541127      0.668646   \n",
       "max        3.922461      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_364   feature_365   feature_366   feature_367   feature_368  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.010099     -0.031220     -0.025373      0.010071     -0.013189   \n",
       "std        0.985637      0.981675      0.977610      1.005352      0.987037   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.668755     -0.691752     -0.668176     -0.670563     -0.668315   \n",
       "50%       -0.002078     -0.031105     -0.026888      0.000091     -0.006910   \n",
       "75%        0.655012      0.619187      0.625274      0.687504      0.660772   \n",
       "max        5.199338      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_369   feature_370   feature_371   feature_372   feature_373  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.017985      0.011084     -0.003941      3.514110     -0.029191   \n",
       "std        0.999032      1.003898      0.994807      0.986077      0.947330   \n",
       "min       -5.199338     -5.199338     -5.199338      1.000000     -5.199338   \n",
       "25%       -0.693579     -0.662723     -0.677450      4.000000     -0.668016   \n",
       "50%       -0.014610      0.011852      0.001542      4.000000     -0.013549   \n",
       "75%        0.668024      0.695034      0.671332      4.000000      0.629284   \n",
       "max        5.199338      5.199338      5.199338      5.000000      3.211506   \n",
       "\n",
       "        feature_374   feature_375   feature_376   feature_377   feature_378  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.027324      0.526762     -0.008724      0.004876     -0.008747   \n",
       "std        1.008335      1.232269      1.016901      0.987258      0.984155   \n",
       "min       -5.199338      0.000000     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.715063      0.000000     -0.684471     -0.673860     -0.674191   \n",
       "50%       -0.008381      0.000000     -0.019065      0.007727     -0.005162   \n",
       "75%        0.651715      0.000000      0.676816      0.674138      0.662224   \n",
       "max        5.199338      6.000000      5.199338      3.215739      5.199338   \n",
       "\n",
       "        feature_379   feature_380   feature_381   feature_382   feature_383  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.004826      0.064518     -0.100974     -0.026204      0.221683   \n",
       "std        1.003866      0.926760      0.962188      1.001388      0.478255   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338      0.000000   \n",
       "25%       -0.681502     -0.571487     -0.731304     -0.705002      0.000000   \n",
       "50%       -0.002807      0.079940     -0.093222     -0.047493      0.000000   \n",
       "75%        0.676479      0.669706      0.521216      0.616124      0.000000   \n",
       "max        5.199338      5.199338      5.199338      5.199338      2.000000   \n",
       "\n",
       "        feature_384   feature_385   feature_386   feature_387   feature_388  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.029237      0.005299     -0.003504     -0.001310     -0.001884   \n",
       "std        1.006405      1.002959      1.000561      0.980775      1.005720   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.648097     -0.665864     -0.671427     -0.670812     -0.668331   \n",
       "50%        0.033889      0.006935     -0.020253     -0.001357      0.002499   \n",
       "75%        0.710427      0.676100      0.667043      0.625860      0.662465   \n",
       "max        5.199338      5.199338      5.199338      5.199338      5.199338   \n",
       "\n",
       "        feature_389   feature_390   feature_391   feature_392   feature_393  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.011558     -0.052937     -0.012856      0.008890      0.004487   \n",
       "std        0.989477      0.984590      1.013173      1.005082      0.999009   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.678788     -0.707585     -0.698908     -0.656044     -0.680333   \n",
       "50%       -0.018453     -0.054771     -0.011937      0.016183     -0.003888   \n",
       "75%        0.658025      0.606116      0.669995      0.675216      0.680425   \n",
       "max        5.199338      5.199338      5.199338      3.481788      5.199338   \n",
       "\n",
       "        feature_394   feature_395   feature_396   feature_397   feature_398  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.004060     -0.034970      2.610519     -0.005816     -0.292827   \n",
       "std        0.994796      1.009726      0.732550      0.975858      0.867980   \n",
       "min       -5.199338     -5.199338      1.000000     -5.199338     -5.199338   \n",
       "25%       -0.687233     -0.700607      3.000000     -0.667426     -0.855066   \n",
       "50%       -0.006015     -0.053030      3.000000     -0.006121     -0.267089   \n",
       "75%        0.668209      0.621743      3.000000      0.653412      0.282553   \n",
       "max        5.199338      5.199338      3.000000      5.199338      3.113628   \n",
       "\n",
       "        feature_399   feature_400   feature_401   feature_402   feature_403  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.004642      0.007315     -0.002064     -0.057303     -0.014882   \n",
       "std        1.011240      1.009522      0.991369      0.970087      0.999734   \n",
       "min       -5.199338     -5.199338     -5.199338     -5.199338     -5.199338   \n",
       "25%       -0.677072     -0.674688     -0.665726     -0.701116     -0.679259   \n",
       "50%        0.004427      0.009865     -0.006369     -0.046984      0.003130   \n",
       "75%        0.675088      0.690446      0.666159      0.593359      0.654436   \n",
       "max        5.199338      5.199338      5.199338      3.545055      5.199338   \n",
       "\n",
       "        feature_404   feature_405   feature_406   feature_407   feature_408  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean      -0.002663     -0.002989     -0.053965     -0.115081     -0.020693   \n",
       "std        0.999372      0.997574      0.985986      0.951358      0.991441   \n",
       "min       -3.944768     -5.199338     -5.199338     -5.199338     -3.339708   \n",
       "25%       -0.680280     -0.677313     -0.728601     -0.749936     -0.685081   \n",
       "50%       -0.010543      0.000590     -0.051921     -0.104127     -0.010843   \n",
       "75%        0.674147      0.659467      0.604614      0.495817      0.648343   \n",
       "max        3.426364      5.199338      5.199338      5.199338      3.860577   \n",
       "\n",
       "        feature_409   feature_410   feature_411   feature_412   feature_413  \\\n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000   \n",
       "mean       0.007855     -0.289571      0.046371     -0.011372     -0.045371   \n",
       "std        0.994804      0.806576      0.250005      0.998106      0.978557   \n",
       "min       -3.232688     -5.199338      0.000000     -5.199338     -5.199338   \n",
       "25%       -0.671584     -0.816589      0.000000     -0.694098     -0.700684   \n",
       "50%        0.017161     -0.216972      0.000000     -0.016165     -0.035410   \n",
       "75%        0.681021      0.320220      0.000000      0.667831      0.629747   \n",
       "max        5.199338      1.395742      2.000000      5.199338      5.199338   \n",
       "\n",
       "        feature_414   feature_415   feature_416   feature_417   feature_418  \n",
       "count  38731.000000  38731.000000  38731.000000  38731.000000  38731.000000  \n",
       "mean      -0.009627      0.010300     -0.019660     -0.007003     -0.026602  \n",
       "std        0.972842      1.006719      0.990136      1.004295      0.987498  \n",
       "min       -5.199338     -5.199338     -3.408886     -5.199338     -5.199338  \n",
       "25%       -0.676811     -0.676359     -0.687918     -0.678918     -0.692241  \n",
       "50%       -0.000174      0.020541     -0.018817     -0.012701     -0.030068  \n",
       "75%        0.645590      0.694864      0.655448      0.666673      0.657315  \n",
       "max        5.199338      5.199338      5.199338      5.199338      3.585167  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb4f343b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:31:35.639293Z",
     "iopub.status.busy": "2024-11-01T18:31:35.638936Z",
     "iopub.status.idle": "2024-11-01T18:31:35.648976Z",
     "shell.execute_reply": "2024-11-01T18:31:35.648081Z"
    },
    "papermill": {
     "duration": 0.027725,
     "end_time": "2024-11-01T18:31:35.650957",
     "exception": false,
     "start_time": "2024-11-01T18:31:35.623232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    37888\n",
       "1      843\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8e6ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:31:35.682312Z",
     "iopub.status.busy": "2024-11-01T18:31:35.681967Z",
     "iopub.status.idle": "2024-11-01T18:31:36.534151Z",
     "shell.execute_reply": "2024-11-01T18:31:36.533217Z"
    },
    "papermill": {
     "duration": 0.87037,
     "end_time": "2024-11-01T18:31:36.536432",
     "exception": false,
     "start_time": "2024-11-01T18:31:35.666062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(target         0\n",
       " smpl           0\n",
       " id             0\n",
       " feature_1      0\n",
       " feature_2      0\n",
       "               ..\n",
       " feature_414    0\n",
       " feature_415    0\n",
       " feature_416    0\n",
       " feature_417    0\n",
       " feature_418    0\n",
       " Length: 421, dtype: int64,\n",
       " target         0\n",
       " smpl           0\n",
       " id             0\n",
       " feature_1      0\n",
       " feature_2      0\n",
       "               ..\n",
       " feature_414    0\n",
       " feature_415    0\n",
       " feature_416    0\n",
       " feature_417    0\n",
       " feature_418    0\n",
       " Length: 421, dtype: int64,\n",
       " 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum(), df_train.isna().sum(), df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4771acc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:31:36.568830Z",
     "iopub.status.busy": "2024-11-01T18:31:36.568479Z",
     "iopub.status.idle": "2024-11-01T18:31:36.612659Z",
     "shell.execute_reply": "2024-11-01T18:31:36.611823Z"
    },
    "papermill": {
     "duration": 0.063086,
     "end_time": "2024-11-01T18:31:36.615045",
     "exception": false,
     "start_time": "2024-11-01T18:31:36.551959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=['target', 'id', 'smpl'])\n",
    "y = df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280abbf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:31:36.649196Z",
     "iopub.status.busy": "2024-11-01T18:31:36.648387Z",
     "iopub.status.idle": "2024-11-01T18:31:36.751974Z",
     "shell.execute_reply": "2024-11-01T18:31:36.751188Z"
    },
    "papermill": {
     "duration": 0.122665,
     "end_time": "2024-11-01T18:31:36.754331",
     "exception": false,
     "start_time": "2024-11-01T18:31:36.631666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9b6f8cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:31:36.786863Z",
     "iopub.status.busy": "2024-11-01T18:31:36.786522Z",
     "iopub.status.idle": "2024-11-01T18:31:36.791409Z",
     "shell.execute_reply": "2024-11-01T18:31:36.790538Z"
    },
    "papermill": {
     "duration": 0.023491,
     "end_time": "2024-11-01T18:31:36.793615",
     "exception": false,
     "start_time": "2024-11-01T18:31:36.770124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27111, 418) (11620, 418) (27111,) (11620,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9ffd414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:31:36.825067Z",
     "iopub.status.busy": "2024-11-01T18:31:36.824770Z",
     "iopub.status.idle": "2024-11-01T18:31:36.829325Z",
     "shell.execute_reply": "2024-11-01T18:31:36.828456Z"
    },
    "papermill": {
     "duration": 0.022631,
     "end_time": "2024-11-01T18:31:36.831222",
     "exception": false,
     "start_time": "2024-11-01T18:31:36.808591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "automl = AutoML()\n",
    "settings = {\n",
    "    \"time_budget\": 1800,  \n",
    "    \"metric\": 'f1',  \n",
    "    \"task\": 'classification'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea9aec61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T18:31:36.862408Z",
     "iopub.status.busy": "2024-11-01T18:31:36.862069Z",
     "iopub.status.idle": "2024-11-01T19:01:39.322782Z",
     "shell.execute_reply": "2024-11-01T19:01:39.321869Z"
    },
    "papermill": {
     "duration": 1802.478764,
     "end_time": "2024-11-01T19:01:39.324911",
     "exception": false,
     "start_time": "2024-11-01T18:31:36.846147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 11-01 18:31:39] {1728} INFO - task = classification\n",
      "[flaml.automl.logger: 11-01 18:31:39] {1739} INFO - Evaluation method: holdout\n",
      "[flaml.automl.logger: 11-01 18:31:39] {1838} INFO - Minimizing error metric: 1-f1\n",
      "[flaml.automl.logger: 11-01 18:31:39] {1955} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth', 'sgd', 'catboost', 'lrl1']\n",
      "[flaml.automl.logger: 11-01 18:31:39] {2258} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:31:41] {2393} INFO - Estimated sufficient time budget=13703s. Estimated necessary time budget=338s.\n",
      "[flaml.automl.logger: 11-01 18:31:41] {2442} INFO -  at 4.2s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:31:41] {2258} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:31:42] {2442} INFO -  at 5.7s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:31:42] {2258} INFO - iteration 2, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:31:43] {2442} INFO -  at 6.4s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:31:43] {2258} INFO - iteration 3, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:31:44] {2442} INFO -  at 8.1s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:31:44] {2258} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:31:46] {2442} INFO -  at 9.4s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:31:46] {2258} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:31:48] {2442} INFO -  at 11.2s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:31:48] {2258} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:31:49] {2442} INFO -  at 12.4s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:31:49] {2258} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:31:50] {2442} INFO -  at 13.7s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:31:50] {2258} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:31:52] {2442} INFO -  at 15.2s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:31:52] {2258} INFO - iteration 9, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:31:52] {2442} INFO -  at 15.9s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:31:52] {2258} INFO - iteration 10, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:31:54] {2442} INFO -  at 17.1s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:31:54] {2258} INFO - iteration 11, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:31:55] {2442} INFO -  at 18.2s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:31:55] {2258} INFO - iteration 12, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:31:56] {2442} INFO -  at 19.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:31:56] {2258} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:31:57] {2442} INFO -  at 20.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:31:57] {2258} INFO - iteration 14, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:32:08] {2442} INFO -  at 31.6s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:08] {2258} INFO - iteration 15, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:08] {2442} INFO -  at 31.9s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:08] {2258} INFO - iteration 16, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:08] {2442} INFO -  at 32.1s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:08] {2258} INFO - iteration 17, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:09] {2442} INFO -  at 32.3s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:09] {2258} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:32:10] {2442} INFO -  at 33.8s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:10] {2258} INFO - iteration 19, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:11] {2442} INFO -  at 34.3s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:11] {2258} INFO - iteration 20, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:11] {2442} INFO -  at 34.5s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:11] {2258} INFO - iteration 21, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:11] {2442} INFO -  at 34.9s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:11] {2258} INFO - iteration 22, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:32:12] {2442} INFO -  at 36.1s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:12] {2258} INFO - iteration 23, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:32:14] {2442} INFO -  at 37.2s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:14] {2258} INFO - iteration 24, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:14] {2442} INFO -  at 37.4s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:14] {2258} INFO - iteration 25, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:32:14] {2442} INFO -  at 38.0s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:14] {2258} INFO - iteration 26, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:15] {2442} INFO -  at 38.4s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:15] {2258} INFO - iteration 27, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:15] {2442} INFO -  at 38.7s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:15] {2258} INFO - iteration 28, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:15] {2442} INFO -  at 39.0s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:15] {2258} INFO - iteration 29, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:16] {2442} INFO -  at 39.2s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:16] {2258} INFO - iteration 30, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:32:16] {2442} INFO -  at 39.8s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:16] {2258} INFO - iteration 31, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:32:17] {2442} INFO -  at 40.2s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:17] {2258} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:32:18] {2442} INFO -  at 41.8s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:18] {2258} INFO - iteration 33, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:32:19] {2442} INFO -  at 42.4s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:19] {2258} INFO - iteration 34, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:19] {2442} INFO -  at 42.6s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:19] {2258} INFO - iteration 35, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:32:20] {2442} INFO -  at 43.3s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:20] {2258} INFO - iteration 36, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:32:21] {2442} INFO -  at 44.3s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:21] {2258} INFO - iteration 37, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:32:33] {2442} INFO -  at 56.4s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:33] {2258} INFO - iteration 38, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:32:34] {2442} INFO -  at 57.7s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:34] {2258} INFO - iteration 39, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:34] {2442} INFO -  at 58.1s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:34] {2258} INFO - iteration 40, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:32:35] {2442} INFO -  at 59.1s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:35] {2258} INFO - iteration 41, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:36] {2442} INFO -  at 59.3s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:36] {2258} INFO - iteration 42, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:36] {2442} INFO -  at 59.7s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:36] {2258} INFO - iteration 43, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:32:37] {2442} INFO -  at 61.0s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:37] {2258} INFO - iteration 44, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:38] {2442} INFO -  at 61.3s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:38] {2258} INFO - iteration 45, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:32:38] {2442} INFO -  at 61.9s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:38] {2258} INFO - iteration 46, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:39] {2442} INFO -  at 62.3s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:39] {2258} INFO - iteration 47, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:32:39] {2442} INFO -  at 62.8s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:39] {2258} INFO - iteration 48, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:39] {2442} INFO -  at 63.1s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:39] {2258} INFO - iteration 49, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:32:41] {2442} INFO -  at 64.8s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:41] {2258} INFO - iteration 50, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:32:44] {2442} INFO -  at 67.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:44] {2258} INFO - iteration 51, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:44] {2442} INFO -  at 67.6s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:44] {2258} INFO - iteration 52, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:32:45] {2442} INFO -  at 68.7s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:45] {2258} INFO - iteration 53, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:32:46] {2442} INFO -  at 69.3s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:46] {2258} INFO - iteration 54, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:32:47] {2442} INFO -  at 70.4s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:47] {2258} INFO - iteration 55, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:32:48] {2442} INFO -  at 71.8s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:48] {2258} INFO - iteration 56, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:49] {2442} INFO -  at 72.3s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:49] {2258} INFO - iteration 57, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:32:49] {2442} INFO -  at 72.8s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:49] {2258} INFO - iteration 58, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:32:50] {2442} INFO -  at 73.2s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:50] {2258} INFO - iteration 59, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:32:51] {2442} INFO -  at 74.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:51] {2258} INFO - iteration 60, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:32:52] {2442} INFO -  at 75.6s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:52] {2258} INFO - iteration 61, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:52] {2442} INFO -  at 76.0s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:52] {2258} INFO - iteration 62, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:32:54] {2442} INFO -  at 77.4s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:54] {2258} INFO - iteration 63, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:54] {2442} INFO -  at 78.1s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:54] {2258} INFO - iteration 64, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:55] {2442} INFO -  at 78.6s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:55] {2258} INFO - iteration 65, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:32:56] {2442} INFO -  at 80.1s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:56] {2258} INFO - iteration 66, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:32:58] {2442} INFO -  at 81.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:58] {2258} INFO - iteration 67, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:32:58] {2442} INFO -  at 81.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:58] {2258} INFO - iteration 68, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:32:59] {2442} INFO -  at 82.8s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:32:59] {2258} INFO - iteration 69, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:33:00] {2442} INFO -  at 84.1s,\testimator lgbm's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:33:00] {2258} INFO - iteration 70, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:33:02] {2442} INFO -  at 85.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:33:02] {2258} INFO - iteration 71, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:33:02] {2442} INFO -  at 86.0s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:33:02] {2258} INFO - iteration 72, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:33:04] {2442} INFO -  at 87.1s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:33:04] {2258} INFO - iteration 73, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:33:04] {2442} INFO -  at 87.9s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:33:04] {2258} INFO - iteration 74, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:33:05] {2442} INFO -  at 89.1s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:33:05] {2258} INFO - iteration 75, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:33:10] {2442} INFO -  at 94.1s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:33:10] {2258} INFO - iteration 76, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:33:11] {2442} INFO -  at 94.8s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=1.0000\n",
      "[flaml.automl.logger: 11-01 18:33:11] {2258} INFO - iteration 77, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:33:13] {2442} INFO -  at 96.2s,\testimator lgbm's best error=0.9524,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:13] {2258} INFO - iteration 78, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:33:13] {2442} INFO -  at 96.7s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:13] {2258} INFO - iteration 79, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:33:14] {2442} INFO -  at 97.6s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:14] {2258} INFO - iteration 80, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:33:15] {2442} INFO -  at 98.7s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:15] {2258} INFO - iteration 81, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:33:16] {2442} INFO -  at 99.7s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:16] {2258} INFO - iteration 82, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:33:17] {2442} INFO -  at 100.7s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:17] {2258} INFO - iteration 83, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:33:18] {2442} INFO -  at 101.6s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:18] {2258} INFO - iteration 84, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:33:18] {2442} INFO -  at 101.9s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:18] {2258} INFO - iteration 85, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:33:19] {2442} INFO -  at 102.5s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:19] {2258} INFO - iteration 86, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:33:19] {2442} INFO -  at 103.0s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:19] {2258} INFO - iteration 87, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:33:20] {2442} INFO -  at 103.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:20] {2258} INFO - iteration 88, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:33:22] {2442} INFO -  at 105.7s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:22] {2258} INFO - iteration 89, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:33:25] {2442} INFO -  at 108.4s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:25] {2258} INFO - iteration 90, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:33:28] {2442} INFO -  at 111.5s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:28] {2258} INFO - iteration 91, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:33:29] {2442} INFO -  at 112.8s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:29] {2258} INFO - iteration 92, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:33:30] {2442} INFO -  at 114.1s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:30] {2258} INFO - iteration 93, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:33:31] {2442} INFO -  at 114.5s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:31] {2258} INFO - iteration 94, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:33:31] {2442} INFO -  at 114.9s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:31] {2258} INFO - iteration 95, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:33:33] {2442} INFO -  at 116.3s,\testimator lgbm's best error=0.9524,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:33] {2258} INFO - iteration 96, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:33:57] {2442} INFO -  at 140.4s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:57] {2258} INFO - iteration 97, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:33:58] {2442} INFO -  at 141.5s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:58] {2258} INFO - iteration 98, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:33:59] {2442} INFO -  at 142.8s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:33:59] {2258} INFO - iteration 99, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:34:02] {2442} INFO -  at 145.9s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:34:02] {2258} INFO - iteration 100, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:34:05] {2442} INFO -  at 148.9s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:34:05] {2258} INFO - iteration 101, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:34:06] {2442} INFO -  at 149.5s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:34:06] {2258} INFO - iteration 102, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:34:07] {2442} INFO -  at 150.6s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.9524\n",
      "[flaml.automl.logger: 11-01 18:34:07] {2258} INFO - iteration 103, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:34:09] {2442} INFO -  at 152.9s,\testimator lgbm's best error=0.9140,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:09] {2258} INFO - iteration 104, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:34:14] {2442} INFO -  at 157.7s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:14] {2258} INFO - iteration 105, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:34:16] {2442} INFO -  at 159.2s,\testimator lgbm's best error=0.9140,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:16] {2258} INFO - iteration 106, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:34:17] {2442} INFO -  at 161.1s,\testimator lgbm's best error=0.9140,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:17] {2258} INFO - iteration 107, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:34:19] {2442} INFO -  at 162.9s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:19] {2258} INFO - iteration 108, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:34:22] {2442} INFO -  at 165.8s,\testimator lgbm's best error=0.9140,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:22] {2258} INFO - iteration 109, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:34:24] {2442} INFO -  at 167.7s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:24] {2258} INFO - iteration 110, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:34:25] {2442} INFO -  at 169.0s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:25] {2258} INFO - iteration 111, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:34:28] {2442} INFO -  at 171.5s,\testimator lgbm's best error=0.9140,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:28] {2258} INFO - iteration 112, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:34:29] {2442} INFO -  at 172.9s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:29] {2258} INFO - iteration 113, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:34:30] {2442} INFO -  at 174.0s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:30] {2258} INFO - iteration 114, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:34:31] {2442} INFO -  at 174.8s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:31] {2258} INFO - iteration 115, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:34:33] {2442} INFO -  at 177.1s,\testimator lgbm's best error=0.9140,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:33] {2258} INFO - iteration 116, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:34:35] {2442} INFO -  at 179.1s,\testimator lgbm's best error=0.9140,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:35] {2258} INFO - iteration 117, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:34:41] {2442} INFO -  at 184.6s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:41] {2258} INFO - iteration 118, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:34:43] {2442} INFO -  at 186.5s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9140\n",
      "[flaml.automl.logger: 11-01 18:34:43] {2258} INFO - iteration 119, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:34:46] {2442} INFO -  at 189.7s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:34:46] {2258} INFO - iteration 120, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:34:47] {2442} INFO -  at 191.1s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:34:47] {2258} INFO - iteration 121, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:34:49] {2442} INFO -  at 192.2s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:34:49] {2258} INFO - iteration 122, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:34:51] {2442} INFO -  at 194.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:34:51] {2258} INFO - iteration 123, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:34:53] {2442} INFO -  at 196.7s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:34:53] {2258} INFO - iteration 124, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:34:58] {2442} INFO -  at 201.7s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:34:58] {2258} INFO - iteration 125, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:35:00] {2442} INFO -  at 203.4s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:00] {2258} INFO - iteration 126, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:35:02] {2442} INFO -  at 205.8s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:02] {2258} INFO - iteration 127, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:35:05] {2442} INFO -  at 208.3s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:05] {2258} INFO - iteration 128, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:35:06] {2442} INFO -  at 209.9s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:06] {2258} INFO - iteration 129, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:35:08] {2442} INFO -  at 211.5s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:08] {2258} INFO - iteration 130, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:35:09] {2442} INFO -  at 212.5s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:09] {2258} INFO - iteration 131, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:35:10] {2442} INFO -  at 213.6s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:10] {2258} INFO - iteration 132, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:35:11] {2442} INFO -  at 215.1s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:11] {2258} INFO - iteration 133, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:35:16] {2442} INFO -  at 219.7s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:16] {2258} INFO - iteration 134, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:35:18] {2442} INFO -  at 222.1s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:18] {2258} INFO - iteration 135, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:35:20] {2442} INFO -  at 223.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:20] {2258} INFO - iteration 136, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:35:23] {2442} INFO -  at 226.8s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:23] {2258} INFO - iteration 137, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:35:25] {2442} INFO -  at 228.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:25] {2258} INFO - iteration 138, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:35:28] {2442} INFO -  at 232.1s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:28] {2258} INFO - iteration 139, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:35:35] {2442} INFO -  at 239.1s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:35] {2258} INFO - iteration 140, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:35:42] {2442} INFO -  at 245.7s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:42] {2258} INFO - iteration 141, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:35:47] {2442} INFO -  at 250.4s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:47] {2258} INFO - iteration 142, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:35:52] {2442} INFO -  at 255.2s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:52] {2258} INFO - iteration 143, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:35:53] {2442} INFO -  at 256.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:35:53] {2258} INFO - iteration 144, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:36:02] {2442} INFO -  at 265.7s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:02] {2258} INFO - iteration 145, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:36:03] {2442} INFO -  at 266.6s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:03] {2258} INFO - iteration 146, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:36:05] {2442} INFO -  at 268.4s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:05] {2258} INFO - iteration 147, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:36:07] {2442} INFO -  at 270.5s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:07] {2258} INFO - iteration 148, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:36:09] {2442} INFO -  at 273.0s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:09] {2258} INFO - iteration 149, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:36:15] {2442} INFO -  at 278.2s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:15] {2258} INFO - iteration 150, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:36:17] {2442} INFO -  at 281.0s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:17] {2258} INFO - iteration 151, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:36:22] {2442} INFO -  at 285.3s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:22] {2258} INFO - iteration 152, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:36:24] {2442} INFO -  at 287.8s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:24] {2258} INFO - iteration 153, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:36:25] {2442} INFO -  at 288.5s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:25] {2258} INFO - iteration 154, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:36:26] {2442} INFO -  at 289.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:26] {2258} INFO - iteration 155, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:36:29] {2442} INFO -  at 292.9s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:29] {2258} INFO - iteration 156, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:36:36] {2442} INFO -  at 299.9s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:36] {2258} INFO - iteration 157, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:36:41] {2442} INFO -  at 304.5s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:41] {2258} INFO - iteration 158, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:36:43] {2442} INFO -  at 306.5s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:43] {2258} INFO - iteration 159, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:36:47] {2442} INFO -  at 311.1s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:47] {2258} INFO - iteration 160, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:36:51] {2442} INFO -  at 314.7s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:36:51] {2258} INFO - iteration 161, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:37:05] {2442} INFO -  at 328.4s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:37:05] {2258} INFO - iteration 162, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:37:16] {2442} INFO -  at 339.6s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:37:16] {2258} INFO - iteration 163, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:37:17] {2442} INFO -  at 340.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:37:17] {2258} INFO - iteration 164, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:38:04] {2442} INFO -  at 387.5s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:38:04] {2258} INFO - iteration 165, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:38:05] {2442} INFO -  at 388.5s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:38:05] {2258} INFO - iteration 166, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:38:07] {2442} INFO -  at 390.1s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:38:07] {2258} INFO - iteration 167, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:38:08] {2442} INFO -  at 391.6s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:38:08] {2258} INFO - iteration 168, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:38:09] {2442} INFO -  at 392.9s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:38:09] {2258} INFO - iteration 169, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:38:15] {2442} INFO -  at 399.0s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:38:15] {2258} INFO - iteration 170, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:38:17] {2442} INFO -  at 400.5s,\testimator lgbm's best error=0.9065,\tbest estimator lgbm's best error=0.9065\n",
      "[flaml.automl.logger: 11-01 18:38:17] {2258} INFO - iteration 171, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:38:19] {2442} INFO -  at 402.2s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:38:19] {2258} INFO - iteration 172, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:38:27] {2442} INFO -  at 410.5s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:38:27] {2258} INFO - iteration 173, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:38:28] {2442} INFO -  at 411.6s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:38:28] {2258} INFO - iteration 174, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:38:30] {2442} INFO -  at 413.6s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:38:30] {2258} INFO - iteration 175, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:38:31] {2442} INFO -  at 415.0s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:38:31] {2258} INFO - iteration 176, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:38:33] {2442} INFO -  at 416.2s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:38:33] {2258} INFO - iteration 177, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:39:35] {2442} INFO -  at 478.2s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:39:35] {2258} INFO - iteration 178, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:39:36] {2442} INFO -  at 479.2s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:39:36] {2258} INFO - iteration 179, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:39:37] {2442} INFO -  at 480.6s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:39:37] {2258} INFO - iteration 180, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:39:38] {2442} INFO -  at 481.9s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:39:38] {2258} INFO - iteration 181, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:39:40] {2442} INFO -  at 483.3s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:39:40] {2258} INFO - iteration 182, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:39:42] {2442} INFO -  at 485.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:39:42] {2258} INFO - iteration 183, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:39:45] {2442} INFO -  at 489.1s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:39:45] {2258} INFO - iteration 184, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:39:47] {2442} INFO -  at 491.1s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:39:47] {2258} INFO - iteration 185, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:39:49] {2442} INFO -  at 492.2s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:39:49] {2258} INFO - iteration 186, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:39:50] {2442} INFO -  at 493.5s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:39:50] {2258} INFO - iteration 187, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:39:55] {2442} INFO -  at 498.3s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:39:55] {2258} INFO - iteration 188, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:39:56] {2442} INFO -  at 499.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:39:56] {2258} INFO - iteration 189, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:39:58] {2442} INFO -  at 501.3s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:39:58] {2258} INFO - iteration 190, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:40:04] {2442} INFO -  at 508.0s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:40:04] {2258} INFO - iteration 191, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:40:06] {2442} INFO -  at 509.6s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:40:06] {2258} INFO - iteration 192, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:40:08] {2442} INFO -  at 511.7s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:40:08] {2258} INFO - iteration 193, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:40:10] {2442} INFO -  at 513.3s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:40:10] {2258} INFO - iteration 194, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:40:11] {2442} INFO -  at 514.4s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:40:11] {2258} INFO - iteration 195, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:40:38] {2442} INFO -  at 541.9s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:40:38] {2258} INFO - iteration 196, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:40:43] {2442} INFO -  at 546.6s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:40:43] {2258} INFO - iteration 197, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:40:44] {2442} INFO -  at 548.1s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:40:44] {2258} INFO - iteration 198, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:40:46] {2442} INFO -  at 549.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:40:46] {2258} INFO - iteration 199, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:40:48] {2442} INFO -  at 551.9s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:40:48] {2258} INFO - iteration 200, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:40:56] {2442} INFO -  at 559.2s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:40:56] {2258} INFO - iteration 201, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:41:54] {2442} INFO -  at 618.1s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:41:54] {2258} INFO - iteration 202, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:42:00] {2442} INFO -  at 623.8s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:00] {2258} INFO - iteration 203, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:42:04] {2442} INFO -  at 627.4s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:04] {2258} INFO - iteration 204, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:42:05] {2442} INFO -  at 628.4s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:05] {2258} INFO - iteration 205, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:42:43] {2442} INFO -  at 666.7s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:43] {2258} INFO - iteration 206, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:42:44] {2442} INFO -  at 667.3s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:44] {2258} INFO - iteration 207, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:42:45] {2442} INFO -  at 668.9s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:45] {2258} INFO - iteration 208, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:42:47] {2442} INFO -  at 670.3s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:47] {2258} INFO - iteration 209, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:42:48] {2442} INFO -  at 671.7s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:48] {2258} INFO - iteration 210, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:42:49] {2442} INFO -  at 672.7s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:49] {2258} INFO - iteration 211, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:42:50] {2442} INFO -  at 673.9s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:50] {2258} INFO - iteration 212, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:42:51] {2442} INFO -  at 675.0s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:51] {2258} INFO - iteration 213, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:42:53] {2442} INFO -  at 676.1s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:53] {2258} INFO - iteration 214, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:42:54] {2442} INFO -  at 677.2s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:54] {2258} INFO - iteration 215, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:42:54] {2442} INFO -  at 678.1s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:54] {2258} INFO - iteration 216, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:42:57] {2442} INFO -  at 680.2s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:42:57] {2258} INFO - iteration 217, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:43:06] {2442} INFO -  at 689.7s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:43:06] {2258} INFO - iteration 218, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:43:07] {2442} INFO -  at 691.1s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:43:07] {2258} INFO - iteration 219, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:43:12] {2442} INFO -  at 695.9s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:43:12] {2258} INFO - iteration 220, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:43:14] {2442} INFO -  at 697.2s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:43:14] {2258} INFO - iteration 221, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:43:15] {2442} INFO -  at 698.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:43:15] {2258} INFO - iteration 222, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:43:18] {2442} INFO -  at 702.0s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:43:18] {2258} INFO - iteration 223, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:43:21] {2442} INFO -  at 704.7s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:43:21] {2258} INFO - iteration 224, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:43:22] {2442} INFO -  at 706.0s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:43:22] {2258} INFO - iteration 225, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:43:28] {2442} INFO -  at 711.7s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:43:28] {2258} INFO - iteration 226, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:43:30] {2442} INFO -  at 713.5s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:43:30] {2258} INFO - iteration 227, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:43:41] {2442} INFO -  at 725.1s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:43:41] {2258} INFO - iteration 228, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:43:45] {2442} INFO -  at 728.3s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:43:45] {2258} INFO - iteration 229, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:43:47] {2442} INFO -  at 730.4s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:43:47] {2258} INFO - iteration 230, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:44:02] {2442} INFO -  at 745.8s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:44:02] {2258} INFO - iteration 231, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:44:07] {2442} INFO -  at 750.4s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:44:07] {2258} INFO - iteration 232, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:44:08] {2442} INFO -  at 751.7s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:44:08] {2258} INFO - iteration 233, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:44:09] {2442} INFO -  at 752.4s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:44:09] {2258} INFO - iteration 234, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:45:05] {2442} INFO -  at 808.7s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:45:05] {2258} INFO - iteration 235, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:45:06] {2442} INFO -  at 809.4s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:45:06] {2258} INFO - iteration 236, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:45:07] {2442} INFO -  at 810.5s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:45:07] {2258} INFO - iteration 237, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:45:08] {2442} INFO -  at 811.6s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:45:08] {2258} INFO - iteration 238, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:45:10] {2442} INFO -  at 813.4s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:45:10] {2258} INFO - iteration 239, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:45:24] {2442} INFO -  at 827.6s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:45:24] {2258} INFO - iteration 240, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:45:25] {2442} INFO -  at 828.7s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:45:25] {2258} INFO - iteration 241, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:45:26] {2442} INFO -  at 829.8s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:45:26] {2258} INFO - iteration 242, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:45:28] {2442} INFO -  at 831.3s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:45:28] {2258} INFO - iteration 243, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:45:29] {2442} INFO -  at 832.6s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:45:29] {2258} INFO - iteration 244, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:45:30] {2442} INFO -  at 833.8s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:45:30] {2258} INFO - iteration 245, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:45:32] {2442} INFO -  at 835.2s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:45:32] {2258} INFO - iteration 246, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:45:50] {2442} INFO -  at 853.7s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:45:50] {2258} INFO - iteration 247, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:46:04] {2442} INFO -  at 867.4s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:46:04] {2258} INFO - iteration 248, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:46:05] {2442} INFO -  at 868.8s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:46:05] {2258} INFO - iteration 249, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:46:07] {2442} INFO -  at 870.4s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:46:07] {2258} INFO - iteration 250, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:46:08] {2442} INFO -  at 871.9s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:46:08] {2258} INFO - iteration 251, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:46:10] {2442} INFO -  at 874.1s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:46:10] {2258} INFO - iteration 252, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:46:13] {2442} INFO -  at 876.3s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:46:13] {2258} INFO - iteration 253, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:46:14] {2442} INFO -  at 877.6s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:46:14] {2258} INFO - iteration 254, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:46:16] {2442} INFO -  at 879.3s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:46:16] {2258} INFO - iteration 255, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:46:16] {2442} INFO -  at 879.9s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:46:16] {2258} INFO - iteration 256, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:46:18] {2442} INFO -  at 881.4s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:46:18] {2258} INFO - iteration 257, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:46:32] {2442} INFO -  at 895.7s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:46:32] {2258} INFO - iteration 258, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:46:45] {2442} INFO -  at 908.2s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:46:45] {2258} INFO - iteration 259, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:47:03] {2442} INFO -  at 926.2s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:03] {2258} INFO - iteration 260, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:47:04] {2442} INFO -  at 927.2s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:04] {2258} INFO - iteration 261, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:47:05] {2442} INFO -  at 928.9s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:05] {2258} INFO - iteration 262, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:47:06] {2442} INFO -  at 930.1s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:06] {2258} INFO - iteration 263, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:47:08] {2442} INFO -  at 931.5s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:08] {2258} INFO - iteration 264, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:47:10] {2442} INFO -  at 933.2s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:10] {2258} INFO - iteration 265, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:47:11] {2442} INFO -  at 934.4s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:11] {2258} INFO - iteration 266, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:47:12] {2442} INFO -  at 935.9s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:12] {2258} INFO - iteration 267, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:47:14] {2442} INFO -  at 937.5s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:14] {2258} INFO - iteration 268, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:47:16] {2442} INFO -  at 939.2s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:16] {2258} INFO - iteration 269, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:47:17] {2442} INFO -  at 941.1s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:17] {2258} INFO - iteration 270, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:47:18] {2442} INFO -  at 942.0s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:18] {2258} INFO - iteration 271, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:47:27] {2442} INFO -  at 950.9s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:27] {2258} INFO - iteration 272, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:47:29] {2442} INFO -  at 952.6s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:29] {2258} INFO - iteration 273, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:47:30] {2442} INFO -  at 953.7s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:30] {2258} INFO - iteration 274, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:47:55] {2442} INFO -  at 978.9s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:55] {2258} INFO - iteration 275, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:47:56] {2442} INFO -  at 979.9s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:56] {2258} INFO - iteration 276, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:47:58] {2442} INFO -  at 981.2s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:47:58] {2258} INFO - iteration 277, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:48:14] {2442} INFO -  at 997.8s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:48:14] {2258} INFO - iteration 278, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:48:15] {2442} INFO -  at 998.9s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:48:15] {2258} INFO - iteration 279, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:48:16] {2442} INFO -  at 999.9s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:48:16] {2258} INFO - iteration 280, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:48:24] {2442} INFO -  at 1008.1s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:48:24] {2258} INFO - iteration 281, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:48:26] {2442} INFO -  at 1009.5s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:48:26] {2258} INFO - iteration 282, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:48:27] {2442} INFO -  at 1011.0s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:48:27] {2258} INFO - iteration 283, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:48:29] {2442} INFO -  at 1012.2s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:48:29] {2258} INFO - iteration 284, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:48:41] {2442} INFO -  at 1025.0s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:48:41] {2258} INFO - iteration 285, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:48:43] {2442} INFO -  at 1026.7s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:48:43] {2258} INFO - iteration 286, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:49:19] {2442} INFO -  at 1062.2s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:49:19] {2258} INFO - iteration 287, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:49:41] {2442} INFO -  at 1084.4s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:49:41] {2258} INFO - iteration 288, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:49:42] {2442} INFO -  at 1086.0s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:49:42] {2258} INFO - iteration 289, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:49:44] {2442} INFO -  at 1087.7s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:49:44] {2258} INFO - iteration 290, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:49:47] {2442} INFO -  at 1090.6s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:49:47] {2258} INFO - iteration 291, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:49:48] {2442} INFO -  at 1092.1s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:49:48] {2258} INFO - iteration 292, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:49:50] {2442} INFO -  at 1093.4s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:49:50] {2258} INFO - iteration 293, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:49:51] {2442} INFO -  at 1095.0s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:49:51] {2258} INFO - iteration 294, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:49:53] {2442} INFO -  at 1096.2s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:49:53] {2258} INFO - iteration 295, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:49:54] {2442} INFO -  at 1097.6s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:49:54] {2258} INFO - iteration 296, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:49:55] {2442} INFO -  at 1098.8s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:49:55] {2258} INFO - iteration 297, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:49:57] {2442} INFO -  at 1100.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:49:57] {2258} INFO - iteration 298, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:49:58] {2442} INFO -  at 1101.9s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:49:58] {2258} INFO - iteration 299, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:49:59] {2442} INFO -  at 1102.9s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:49:59] {2258} INFO - iteration 300, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:50:20] {2442} INFO -  at 1123.6s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:50:20] {2258} INFO - iteration 301, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:50:21] {2442} INFO -  at 1125.1s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:50:21] {2258} INFO - iteration 302, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:50:33] {2442} INFO -  at 1136.8s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:50:33] {2258} INFO - iteration 303, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:50:36] {2442} INFO -  at 1139.3s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:50:36] {2258} INFO - iteration 304, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:50:53] {2442} INFO -  at 1156.5s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:50:53] {2258} INFO - iteration 305, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:50:54] {2442} INFO -  at 1157.7s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:50:54] {2258} INFO - iteration 306, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:50:56] {2442} INFO -  at 1159.3s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:50:56] {2258} INFO - iteration 307, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:50:56] {2442} INFO -  at 1159.9s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:50:56] {2258} INFO - iteration 308, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:50:58] {2442} INFO -  at 1161.6s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:50:58] {2258} INFO - iteration 309, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:51:00] {2442} INFO -  at 1163.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:00] {2258} INFO - iteration 310, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:51:01] {2442} INFO -  at 1165.0s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:01] {2258} INFO - iteration 311, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:51:03] {2442} INFO -  at 1166.4s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:03] {2258} INFO - iteration 312, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:51:21] {2442} INFO -  at 1185.0s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:21] {2258} INFO - iteration 313, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:51:25] {2442} INFO -  at 1188.9s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:25] {2258} INFO - iteration 314, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:51:26] {2442} INFO -  at 1190.0s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:26] {2258} INFO - iteration 315, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:51:38] {2442} INFO -  at 1201.3s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:38] {2258} INFO - iteration 316, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:51:41] {2442} INFO -  at 1204.8s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:41] {2258} INFO - iteration 317, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:51:42] {2442} INFO -  at 1205.5s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:42] {2258} INFO - iteration 318, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:51:43] {2442} INFO -  at 1206.9s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:43] {2258} INFO - iteration 319, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:51:45] {2442} INFO -  at 1208.7s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:45] {2258} INFO - iteration 320, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:51:48] {2442} INFO -  at 1212.1s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:48] {2258} INFO - iteration 321, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:51:50] {2442} INFO -  at 1213.5s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:50] {2258} INFO - iteration 322, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:51:51] {2442} INFO -  at 1215.0s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:51] {2258} INFO - iteration 323, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:51:53] {2442} INFO -  at 1216.6s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:53] {2258} INFO - iteration 324, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:51:55] {2442} INFO -  at 1218.2s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:55] {2258} INFO - iteration 325, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:51:57] {2442} INFO -  at 1221.1s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:57] {2258} INFO - iteration 326, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:51:59] {2442} INFO -  at 1222.2s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:51:59] {2258} INFO - iteration 327, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:52:00] {2442} INFO -  at 1223.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:00] {2258} INFO - iteration 328, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:52:02] {2442} INFO -  at 1225.4s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:02] {2258} INFO - iteration 329, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:52:03] {2442} INFO -  at 1227.0s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:03] {2258} INFO - iteration 330, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:52:05] {2442} INFO -  at 1228.2s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:05] {2258} INFO - iteration 331, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:52:06] {2442} INFO -  at 1229.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:06] {2258} INFO - iteration 332, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:52:09] {2442} INFO -  at 1232.3s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:09] {2258} INFO - iteration 333, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:52:10] {2442} INFO -  at 1233.6s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:10] {2258} INFO - iteration 334, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:52:11] {2442} INFO -  at 1234.6s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:11] {2258} INFO - iteration 335, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:52:13] {2442} INFO -  at 1236.6s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:13] {2258} INFO - iteration 336, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:52:15] {2442} INFO -  at 1238.6s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:15] {2258} INFO - iteration 337, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:52:17] {2442} INFO -  at 1240.2s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:17] {2258} INFO - iteration 338, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:52:19] {2442} INFO -  at 1242.2s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:19] {2258} INFO - iteration 339, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:52:20] {2442} INFO -  at 1243.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:20] {2258} INFO - iteration 340, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:52:21] {2442} INFO -  at 1244.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:21] {2258} INFO - iteration 341, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:52:23] {2442} INFO -  at 1246.1s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:23] {2258} INFO - iteration 342, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:52:32] {2442} INFO -  at 1255.8s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:32] {2258} INFO - iteration 343, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:52:33] {2442} INFO -  at 1256.6s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:33] {2258} INFO - iteration 344, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:52:34] {2442} INFO -  at 1257.8s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:34] {2258} INFO - iteration 345, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:52:35] {2442} INFO -  at 1259.0s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:35] {2258} INFO - iteration 346, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:52:37] {2442} INFO -  at 1260.2s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:37] {2258} INFO - iteration 347, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:52:54] {2442} INFO -  at 1277.9s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:54] {2258} INFO - iteration 348, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:52:56] {2442} INFO -  at 1279.6s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:56] {2258} INFO - iteration 349, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:52:58] {2442} INFO -  at 1281.9s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:52:58] {2258} INFO - iteration 350, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:53:03] {2442} INFO -  at 1287.0s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:03] {2258} INFO - iteration 351, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:05] {2442} INFO -  at 1288.9s,\testimator xgb_limitdepth's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:05] {2258} INFO - iteration 352, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:07] {2442} INFO -  at 1290.5s,\testimator xgb_limitdepth's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:07] {2258} INFO - iteration 353, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:09] {2442} INFO -  at 1292.4s,\testimator xgb_limitdepth's best error=0.9806,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:09] {2258} INFO - iteration 354, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:10] {2442} INFO -  at 1293.9s,\testimator xgb_limitdepth's best error=0.9412,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:10] {2258} INFO - iteration 355, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:13] {2442} INFO -  at 1296.6s,\testimator xgb_limitdepth's best error=0.9412,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:13] {2258} INFO - iteration 356, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:53:15] {2442} INFO -  at 1299.1s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:15] {2258} INFO - iteration 357, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:17] {2442} INFO -  at 1300.3s,\testimator xgb_limitdepth's best error=0.9412,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:17] {2258} INFO - iteration 358, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:19] {2442} INFO -  at 1302.8s,\testimator xgb_limitdepth's best error=0.8901,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:19] {2258} INFO - iteration 359, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:23] {2442} INFO -  at 1306.2s,\testimator xgb_limitdepth's best error=0.8901,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:23] {2258} INFO - iteration 360, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:24] {2442} INFO -  at 1307.9s,\testimator xgb_limitdepth's best error=0.8901,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:24] {2258} INFO - iteration 361, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:27] {2442} INFO -  at 1310.6s,\testimator xgb_limitdepth's best error=0.8901,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:27] {2258} INFO - iteration 362, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:29] {2442} INFO -  at 1312.7s,\testimator xgb_limitdepth's best error=0.8901,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:29] {2258} INFO - iteration 363, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:31] {2442} INFO -  at 1314.6s,\testimator xgb_limitdepth's best error=0.8901,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:31] {2258} INFO - iteration 364, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:34] {2442} INFO -  at 1317.8s,\testimator xgb_limitdepth's best error=0.8901,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:34] {2258} INFO - iteration 365, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:36] {2442} INFO -  at 1319.3s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:36] {2258} INFO - iteration 366, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:53:49] {2442} INFO -  at 1332.4s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:49] {2258} INFO - iteration 367, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:53:50] {2442} INFO -  at 1333.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:50] {2258} INFO - iteration 368, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:53:51] {2442} INFO -  at 1335.0s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:51] {2258} INFO - iteration 369, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:54] {2442} INFO -  at 1337.3s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:54] {2258} INFO - iteration 370, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:53:55] {2442} INFO -  at 1338.5s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:53:55] {2258} INFO - iteration 371, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:54:12] {2442} INFO -  at 1355.8s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:54:12] {2258} INFO - iteration 372, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:54:15] {2442} INFO -  at 1358.6s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:54:15] {2258} INFO - iteration 373, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:54:25] {2442} INFO -  at 1368.7s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:54:25] {2258} INFO - iteration 374, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:54:26] {2442} INFO -  at 1369.9s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:54:26] {2258} INFO - iteration 375, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:54:29] {2442} INFO -  at 1372.4s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:54:29] {2258} INFO - iteration 376, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:54:53] {2442} INFO -  at 1396.9s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:54:53] {2258} INFO - iteration 377, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:54:54] {2442} INFO -  at 1398.0s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:54:54] {2258} INFO - iteration 378, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:54:56] {2442} INFO -  at 1399.5s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:54:56] {2258} INFO - iteration 379, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:54:57] {2442} INFO -  at 1400.7s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:54:57] {2258} INFO - iteration 380, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:00] {2442} INFO -  at 1403.8s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:00] {2258} INFO - iteration 381, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:55:04] {2442} INFO -  at 1407.2s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:04] {2258} INFO - iteration 382, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:05] {2442} INFO -  at 1408.6s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:05] {2258} INFO - iteration 383, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:07] {2442} INFO -  at 1410.5s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:07] {2258} INFO - iteration 384, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:08] {2442} INFO -  at 1411.9s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:08] {2258} INFO - iteration 385, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:10] {2442} INFO -  at 1414.0s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:10] {2258} INFO - iteration 386, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:55:11] {2442} INFO -  at 1415.1s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:11] {2258} INFO - iteration 387, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:13] {2442} INFO -  at 1416.3s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:13] {2258} INFO - iteration 388, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:15] {2442} INFO -  at 1419.1s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:15] {2258} INFO - iteration 389, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:55:26] {2442} INFO -  at 1429.2s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:26] {2258} INFO - iteration 390, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:55:27] {2442} INFO -  at 1430.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:27] {2258} INFO - iteration 391, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:55:28] {2442} INFO -  at 1431.4s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:28] {2258} INFO - iteration 392, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:55:29] {2442} INFO -  at 1432.5s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:29] {2258} INFO - iteration 393, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:30] {2442} INFO -  at 1433.8s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:30] {2258} INFO - iteration 394, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:32] {2442} INFO -  at 1435.6s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:32] {2258} INFO - iteration 395, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:55:33] {2442} INFO -  at 1437.1s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:33] {2258} INFO - iteration 396, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:35] {2442} INFO -  at 1438.3s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:35] {2258} INFO - iteration 397, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:55:36] {2442} INFO -  at 1439.5s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:36] {2258} INFO - iteration 398, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:39] {2442} INFO -  at 1442.9s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:39] {2258} INFO - iteration 399, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:55:40] {2442} INFO -  at 1444.0s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:40] {2258} INFO - iteration 400, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:55:43] {2442} INFO -  at 1446.5s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:43] {2258} INFO - iteration 401, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:45] {2442} INFO -  at 1448.2s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:45] {2258} INFO - iteration 402, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:46] {2442} INFO -  at 1449.6s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:46] {2258} INFO - iteration 403, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:55:48] {2442} INFO -  at 1451.2s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:48] {2258} INFO - iteration 404, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:49] {2442} INFO -  at 1453.0s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:49] {2258} INFO - iteration 405, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:51] {2442} INFO -  at 1454.4s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:51] {2258} INFO - iteration 406, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:55:56] {2442} INFO -  at 1459.7s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:55:56] {2258} INFO - iteration 407, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:56:09] {2442} INFO -  at 1472.7s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:09] {2258} INFO - iteration 408, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:56:10] {2442} INFO -  at 1474.0s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:10] {2258} INFO - iteration 409, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:56:12] {2442} INFO -  at 1475.3s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:12] {2258} INFO - iteration 410, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:56:14] {2442} INFO -  at 1477.2s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:14] {2258} INFO - iteration 411, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:56:15] {2442} INFO -  at 1478.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:15] {2258} INFO - iteration 412, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:56:16] {2442} INFO -  at 1479.5s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:16] {2258} INFO - iteration 413, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:56:17] {2442} INFO -  at 1480.7s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:17] {2258} INFO - iteration 414, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:56:18] {2442} INFO -  at 1481.8s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:18] {2258} INFO - iteration 415, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:56:20] {2442} INFO -  at 1483.2s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:20] {2258} INFO - iteration 416, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:56:21] {2442} INFO -  at 1484.7s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:21] {2258} INFO - iteration 417, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:56:23] {2442} INFO -  at 1486.8s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:23] {2258} INFO - iteration 418, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:56:25] {2442} INFO -  at 1488.4s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:25] {2258} INFO - iteration 419, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:56:28] {2442} INFO -  at 1491.3s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:28] {2258} INFO - iteration 420, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:56:29] {2442} INFO -  at 1492.7s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:29] {2258} INFO - iteration 421, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:56:30] {2442} INFO -  at 1493.9s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:30] {2258} INFO - iteration 422, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:56:31] {2442} INFO -  at 1495.1s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:31] {2258} INFO - iteration 423, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:56:34] {2442} INFO -  at 1497.7s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:34] {2258} INFO - iteration 424, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:56:36] {2442} INFO -  at 1499.5s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:36] {2258} INFO - iteration 425, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:56:37] {2442} INFO -  at 1501.1s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:37] {2258} INFO - iteration 426, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:56:39] {2442} INFO -  at 1502.5s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:39] {2258} INFO - iteration 427, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:56:41] {2442} INFO -  at 1504.8s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:41] {2258} INFO - iteration 428, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:56:43] {2442} INFO -  at 1506.3s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:43] {2258} INFO - iteration 429, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:56:44] {2442} INFO -  at 1507.6s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:44] {2258} INFO - iteration 430, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:56:47] {2442} INFO -  at 1510.1s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:47] {2258} INFO - iteration 431, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:56:48] {2442} INFO -  at 1512.0s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:48] {2258} INFO - iteration 432, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:56:50] {2442} INFO -  at 1513.9s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:50] {2258} INFO - iteration 433, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:56:52] {2442} INFO -  at 1515.8s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:52] {2258} INFO - iteration 434, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:56:54] {2442} INFO -  at 1517.4s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:54] {2258} INFO - iteration 435, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:56:55] {2442} INFO -  at 1518.6s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:55] {2258} INFO - iteration 436, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:56:56] {2442} INFO -  at 1519.8s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:56] {2258} INFO - iteration 437, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:56:58] {2442} INFO -  at 1521.4s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:56:58] {2258} INFO - iteration 438, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:57:00] {2442} INFO -  at 1523.5s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:00] {2258} INFO - iteration 439, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:57:01] {2442} INFO -  at 1524.9s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:01] {2258} INFO - iteration 440, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:57:03] {2442} INFO -  at 1526.7s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:03] {2258} INFO - iteration 441, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:57:04] {2442} INFO -  at 1528.1s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:04] {2258} INFO - iteration 442, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:57:06] {2442} INFO -  at 1529.2s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:06] {2258} INFO - iteration 443, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:57:07] {2442} INFO -  at 1530.2s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:07] {2258} INFO - iteration 444, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:57:08] {2442} INFO -  at 1531.7s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:08] {2258} INFO - iteration 445, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:57:10] {2442} INFO -  at 1533.6s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:10] {2258} INFO - iteration 446, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:57:13] {2442} INFO -  at 1536.9s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:13] {2258} INFO - iteration 447, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:57:16] {2442} INFO -  at 1539.2s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:16] {2258} INFO - iteration 448, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:57:18] {2442} INFO -  at 1541.2s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:18] {2258} INFO - iteration 449, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 18:57:19] {2442} INFO -  at 1542.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:19] {2258} INFO - iteration 450, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:57:21] {2442} INFO -  at 1544.3s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:21] {2258} INFO - iteration 451, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:57:45] {2442} INFO -  at 1568.4s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:45] {2258} INFO - iteration 452, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:57:46] {2442} INFO -  at 1569.8s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:46] {2258} INFO - iteration 453, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:57:48] {2442} INFO -  at 1571.3s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:48] {2258} INFO - iteration 454, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:57:49] {2442} INFO -  at 1572.5s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:49] {2258} INFO - iteration 455, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 18:57:51] {2442} INFO -  at 1574.2s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:57:51] {2258} INFO - iteration 456, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:58:03] {2442} INFO -  at 1586.9s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:58:03] {2258} INFO - iteration 457, current learner rf\n",
      "[flaml.automl.logger: 11-01 18:58:20] {2442} INFO -  at 1603.9s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:58:20] {2258} INFO - iteration 458, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:58:23] {2442} INFO -  at 1606.3s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:58:23] {2258} INFO - iteration 459, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:58:24] {2442} INFO -  at 1607.2s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:58:24] {2258} INFO - iteration 460, current learner catboost\n",
      "[flaml.automl.logger: 11-01 18:58:37] {2442} INFO -  at 1620.3s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:58:37] {2258} INFO - iteration 461, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:58:38] {2442} INFO -  at 1621.6s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:58:38] {2258} INFO - iteration 462, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 18:58:40] {2442} INFO -  at 1623.3s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:58:40] {2258} INFO - iteration 463, current learner sgd\n",
      "[flaml.automl.logger: 11-01 18:59:48] {2442} INFO -  at 1691.7s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:59:48] {2258} INFO - iteration 464, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:59:49] {2442} INFO -  at 1693.0s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:59:49] {2258} INFO - iteration 465, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 18:59:50] {2442} INFO -  at 1694.0s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 18:59:50] {2258} INFO - iteration 466, current learner rf\n",
      "[flaml.automl.logger: 11-01 19:00:13] {2442} INFO -  at 1716.5s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:13] {2258} INFO - iteration 467, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 19:00:15] {2442} INFO -  at 1718.2s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:15] {2258} INFO - iteration 468, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 19:00:16] {2442} INFO -  at 1719.8s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:16] {2258} INFO - iteration 469, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 19:00:18] {2442} INFO -  at 1721.5s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:18] {2258} INFO - iteration 470, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 19:00:19] {2442} INFO -  at 1722.7s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:19] {2258} INFO - iteration 471, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 19:00:21] {2442} INFO -  at 1724.3s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:21] {2258} INFO - iteration 472, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 19:00:22] {2442} INFO -  at 1725.4s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:22] {2258} INFO - iteration 473, current learner catboost\n",
      "[flaml.automl.logger: 11-01 19:00:40] {2442} INFO -  at 1743.5s,\testimator catboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:40] {2258} INFO - iteration 474, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 19:00:41] {2442} INFO -  at 1744.6s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:41] {2258} INFO - iteration 475, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 19:00:43] {2442} INFO -  at 1746.8s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:43] {2258} INFO - iteration 476, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 19:00:45] {2442} INFO -  at 1748.7s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:45] {2258} INFO - iteration 477, current learner sgd\n",
      "[flaml.automl.logger: 11-01 19:00:46] {2442} INFO -  at 1749.4s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:46] {2258} INFO - iteration 478, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 19:00:47] {2442} INFO -  at 1750.8s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:47] {2258} INFO - iteration 479, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 19:00:49] {2442} INFO -  at 1752.3s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:49] {2258} INFO - iteration 480, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 19:00:50] {2442} INFO -  at 1754.1s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:50] {2258} INFO - iteration 481, current learner sgd\n",
      "[flaml.automl.logger: 11-01 19:00:51] {2442} INFO -  at 1754.9s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:51] {2258} INFO - iteration 482, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 19:00:53] {2442} INFO -  at 1756.5s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:53] {2258} INFO - iteration 483, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 19:00:54] {2442} INFO -  at 1758.1s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:54] {2258} INFO - iteration 484, current learner sgd\n",
      "[flaml.automl.logger: 11-01 19:00:56] {2442} INFO -  at 1760.0s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:56] {2258} INFO - iteration 485, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 19:00:58] {2442} INFO -  at 1762.0s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:00:58] {2258} INFO - iteration 486, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 19:01:00] {2442} INFO -  at 1763.6s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:00] {2258} INFO - iteration 487, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 19:01:01] {2442} INFO -  at 1764.7s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:01] {2258} INFO - iteration 488, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 19:01:05] {2442} INFO -  at 1768.2s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:05] {2258} INFO - iteration 489, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 19:01:06] {2442} INFO -  at 1770.1s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:06] {2258} INFO - iteration 490, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 19:01:08] {2442} INFO -  at 1771.3s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:08] {2258} INFO - iteration 491, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 19:01:09] {2442} INFO -  at 1772.7s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:09] {2258} INFO - iteration 492, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 19:01:10] {2442} INFO -  at 1774.0s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:10] {2258} INFO - iteration 493, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 19:01:11] {2442} INFO -  at 1775.1s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:11] {2258} INFO - iteration 494, current learner lgbm\n",
      "[flaml.automl.logger: 11-01 19:01:14] {2442} INFO -  at 1777.2s,\testimator lgbm's best error=0.8824,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:14] {2258} INFO - iteration 495, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 19:01:15] {2442} INFO -  at 1779.0s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:15] {2258} INFO - iteration 496, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 19:01:17] {2442} INFO -  at 1780.9s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:17] {2258} INFO - iteration 497, current learner extra_tree\n",
      "[flaml.automl.logger: 11-01 19:01:19] {2442} INFO -  at 1782.4s,\testimator extra_tree's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:19] {2258} INFO - iteration 498, current learner sgd\n",
      "[flaml.automl.logger: 11-01 19:01:22] {2442} INFO -  at 1785.8s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:22] {2258} INFO - iteration 499, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 19:01:24] {2442} INFO -  at 1787.3s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:24] {2258} INFO - iteration 500, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 19:01:25] {2442} INFO -  at 1788.5s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:25] {2258} INFO - iteration 501, current learner xgboost\n",
      "[flaml.automl.logger: 11-01 19:01:26] {2442} INFO -  at 1789.8s,\testimator xgboost's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:26] {2258} INFO - iteration 502, current learner sgd\n",
      "[flaml.automl.logger: 11-01 19:01:27] {2442} INFO -  at 1790.6s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:27] {2258} INFO - iteration 503, current learner sgd\n",
      "[flaml.automl.logger: 11-01 19:01:30] {2442} INFO -  at 1793.7s,\testimator sgd's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:30] {2258} INFO - iteration 504, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 19:01:32] {2442} INFO -  at 1795.9s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:32] {2258} INFO - iteration 505, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 11-01 19:01:34] {2442} INFO -  at 1797.9s,\testimator xgb_limitdepth's best error=0.8837,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:34] {2258} INFO - iteration 506, current learner rf\n",
      "[flaml.automl.logger: 11-01 19:01:37] {2442} INFO -  at 1800.6s,\testimator rf's best error=1.0000,\tbest estimator lgbm's best error=0.8824\n",
      "[flaml.automl.logger: 11-01 19:01:39] {2685} INFO - retrain lgbm for 1.8s\n",
      "[flaml.automl.logger: 11-01 19:01:39] {2688} INFO - retrained model: LGBMClassifier(colsample_bytree=0.7741399962513769, learning_rate=1.0,\n",
      "               max_bin=255, min_child_samples=11, n_estimators=4, n_jobs=-1,\n",
      "               num_leaves=132, reg_alpha=0.0009765625,\n",
      "               reg_lambda=0.7537234734338761, verbose=-1)\n",
      "[flaml.automl.logger: 11-01 19:01:39] {1985} INFO - fit succeeded\n",
      "[flaml.automl.logger: 11-01 19:01:39] {1986} INFO - Time taken to find the best model: 402.21666717529297\n"
     ]
    }
   ],
   "source": [
    "automl.fit(X_train, y_train, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca36bd57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T19:01:39.558990Z",
     "iopub.status.busy": "2024-11-01T19:01:39.558106Z",
     "iopub.status.idle": "2024-11-01T19:01:39.562565Z",
     "shell.execute_reply": "2024-11-01T19:01:39.561555Z"
    },
    "papermill": {
     "duration": 0.117194,
     "end_time": "2024-11-01T19:01:39.564937",
     "exception": false,
     "start_time": "2024-11-01T19:01:39.447743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = CatBoostClassifier(iterations=1000,\n",
    "#                           task_type=\"GPU\",\n",
    "#                           devices='0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8338bb87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T19:01:39.759864Z",
     "iopub.status.busy": "2024-11-01T19:01:39.759469Z",
     "iopub.status.idle": "2024-11-01T19:01:39.764031Z",
     "shell.execute_reply": "2024-11-01T19:01:39.763087Z"
    },
    "papermill": {
     "duration": 0.105074,
     "end_time": "2024-11-01T19:01:39.766343",
     "exception": false,
     "start_time": "2024-11-01T19:01:39.661269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d4ed6ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T19:01:39.964232Z",
     "iopub.status.busy": "2024-11-01T19:01:39.963820Z",
     "iopub.status.idle": "2024-11-01T19:01:40.164187Z",
     "shell.execute_reply": "2024-11-01T19:01:40.163218Z"
    },
    "papermill": {
     "duration": 0.301174,
     "end_time": "2024-11-01T19:01:40.166827",
     "exception": false,
     "start_time": "2024-11-01T19:01:39.865653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_predictions = automl.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd2aae43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T19:01:40.402115Z",
     "iopub.status.busy": "2024-11-01T19:01:40.401345Z",
     "iopub.status.idle": "2024-11-01T19:01:40.409910Z",
     "shell.execute_reply": "2024-11-01T19:01:40.409118Z"
    },
    "papermill": {
     "duration": 0.121398,
     "end_time": "2024-11-01T19:01:40.411857",
     "exception": false,
     "start_time": "2024-11-01T19:01:40.290459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_val, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34a29a7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T19:01:40.609608Z",
     "iopub.status.busy": "2024-11-01T19:01:40.608941Z",
     "iopub.status.idle": "2024-11-01T19:01:41.508081Z",
     "shell.execute_reply": "2024-11-01T19:01:41.507146Z"
    },
    "papermill": {
     "duration": 0.999947,
     "end_time": "2024-11-01T19:01:41.510145",
     "exception": false,
     "start_time": "2024-11-01T19:01:40.510198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', xticklabels=y.unique(), yticklabels=y.unique())\n",
    "plt.xlabel('Предсказанные')\n",
    "plt.ylabel('Истинные')\n",
    "plt.title('Матрица ошибок')\n",
    "plt.savefig(\"conf.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2223edf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T19:01:41.704589Z",
     "iopub.status.busy": "2024-11-01T19:01:41.703800Z",
     "iopub.status.idle": "2024-11-01T19:01:41.722333Z",
     "shell.execute_reply": "2024-11-01T19:01:41.721542Z"
    },
    "papermill": {
     "duration": 0.117738,
     "end_time": "2024-11-01T19:01:41.724550",
     "exception": false,
     "start_time": "2024-11-01T19:01:41.606812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "roc = roc_auc_score(y_val, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2f0dcbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T19:01:41.934662Z",
     "iopub.status.busy": "2024-11-01T19:01:41.934243Z",
     "iopub.status.idle": "2024-11-01T19:01:41.940181Z",
     "shell.execute_reply": "2024-11-01T19:01:41.939319Z"
    },
    "papermill": {
     "duration": 0.111197,
     "end_time": "2024-11-01T19:01:41.942222",
     "exception": false,
     "start_time": "2024-11-01T19:01:41.831025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5299107667018903"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35b25f",
   "metadata": {
    "papermill": {
     "duration": 0.097472,
     "end_time": "2024-11-01T19:01:42.138851",
     "exception": false,
     "start_time": "2024-11-01T19:01:42.041379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5992963,
     "sourceId": 9782069,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1859.066164,
   "end_time": "2024-11-01T19:01:45.811749",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-01T18:30:46.745585",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
